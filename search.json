[{"path":"https://teazrq.github.io/RLT/articles/Confidence-Interval.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Confidence Interval Estimation","text":"Confidence interval estimation essential understanding uncertainty random forest predictions. RLT package provides comprehensive tools estimating confidence intervals across different types models, including regression, classification, survival analysis.","code":""},{"path":"https://teazrq.github.io/RLT/articles/Confidence-Interval.html","id":"variance-estimation-for-regression","dir":"Articles","previous_headings":"","what":"Variance Estimation for Regression","title":"Confidence Interval Estimation","text":"regression models, RLT can estimate prediction variance construct confidence intervals using --bag samples.","code":"library(RLT) #> RLT and Random Forests v4.2.6 #> pre-release at github.com/teazrq/RLT  # Generate regression data set.seed(1) trainn = 1000 testn = 100 n = trainn + testn p = 20  # Generate features X1 = matrix(rnorm(n*p/2), n, p/2) X2 = matrix(as.integer(runif(n*p/2)*3), n, p/2)  X = data.frame(X1, X2) for (j in (p/2 + 1):p) X[,j] = as.factor(X[,j])  # Generate response y = 1 + X[, 1] + rnorm(n)  # Split data trainX = X[1:trainn, ] trainY = y[1:trainn] testX = X[(trainn+1):n, ] testY = y[(trainn+1):n]  # Order test data for visualization xorder = order(testX[, 1]) testX = testX[xorder, ] testY = testY[xorder]"},{"path":"https://teazrq.github.io/RLT/articles/Confidence-Interval.html","id":"regression-confidence-intervals","dir":"Articles","previous_headings":"Variance Estimation for Regression","what":"Regression Confidence Intervals","title":"Confidence Interval Estimation","text":"","code":"# Fit RLT model with variance estimation RLTfit <- RLT(trainX, trainY, model = \"regression\",                ntrees = 2000, mtry = p, nmin = 40,                split.gen = \"best\", resample.prob = 0.5,               param.control = list(\"var.ready\" = TRUE,                                     \"resample.track\" = TRUE),               verbose = TRUE) #> Warning in RLT(trainX, trainY, model = \"regression\", ntrees = 2000, mtry = p, : resample.replace is set to FALSE due to var.ready #> Regression Random Forest ...  #> ---------- Parameters Summary ---------- #>               (N, P) = (1000, 20) #>           # of trees = 2000 #>         (mtry, nmin) = (20, 40) #>       split generate = Best #>             sampling = 0.5 w/o replace #>   (Obs, Var) weights = (No, No) #>           importance = none #>        reinforcement = No #> ----------------------------------------  # Predict with variance estimation RLTPred <- predict(RLTfit, testX, var.est = TRUE, keep.all = TRUE)  # Calculate confidence intervals upper = RLTPred$Prediction + 1.96*sqrt(RLTPred$Variance) lower = RLTPred$Prediction - 1.96*sqrt(RLTPred$Variance) cover = (1 + testX$X1 > lower) & (1 + testX$X1 < upper)  # Plot results plot(1 + testX$X1, RLTPred$Prediction, pch = 19,       col = ifelse(is.na(cover), \"red\", ifelse(cover, \"green\", \"black\")),      xlab = \"Truth\", ylab = \"Predicted\",       xlim = c(min(y)+1, max(y)-1), ylim = c(min(y)+1, max(y)-1),      main = \"Regression Confidence Intervals\")  abline(0, 1, col = \"darkorange\", lwd = 2)  # Add confidence intervals for (i in 1:testn)   segments(1+testX$X1[i], lower[i], 1+testX$X1[i], upper[i],             col = ifelse(is.na(cover[i]), \"red\", ifelse(cover[i], \"green\", \"black\")))  legend(\"topleft\", c(\"covered\", \"not covered\"), col = c(\"green\", \"black\"),         lty = 1, pch = 19, cex = 1.2)"},{"path":"https://teazrq.github.io/RLT/articles/Confidence-Interval.html","id":"variance-estimation-for-classification","dir":"Articles","previous_headings":"","what":"Variance Estimation for Classification","title":"Confidence Interval Estimation","text":"classification models, RLT can estimate variance class probabilities construct confidence intervals.","code":"# Generate classification data set.seed(1) trainn <- 1000 testn <- 100 n <- trainn + testn p <- 20  # Generate features X1 <- matrix(rnorm(n * p / 2), n, p / 2) X2 <- matrix(as.integer(runif(n * p / 2) * 4), n, p / 2)  X <- data.frame(X1, X2) X[, (p / 2 + 1):p] <- lapply(X[, (p / 2 + 1):p], as.factor)  # Generate outcomes logit <- function(x) exp(x) / (1 + exp(x)) y <- as.factor(rbinom(n, 1, prob = logit(-0.5 + 2*X[, 1])))  # Split data trainX = X[1:trainn, ] trainY = y[1:trainn] testX = X[(trainn+1):n, ] testY = y[(trainn+1):n]  # Order test data xorder = order(testX[, 1]) testX = testX[xorder, ] testY = testY[xorder] testprob = logit(-0.5 + 2*testX$X1)"},{"path":"https://teazrq.github.io/RLT/articles/Confidence-Interval.html","id":"classification-confidence-intervals","dir":"Articles","previous_headings":"Variance Estimation for Classification","what":"Classification Confidence Intervals","title":"Confidence Interval Estimation","text":"","code":"# Fit classification model with variance estimation RLTfit <- RLT(trainX, trainY, model = \"classification\",                ntrees = 2000, mtry = p, nmin = 20,                split.gen = \"random\", resample.prob = 0.5,               param.control = list(\"var.ready\" = TRUE, \"resample.track\" = TRUE),               verbose = TRUE) #> Warning in RLT(trainX, trainY, model = \"classification\", ntrees = 2000, : resample.replace is set to FALSE due to var.ready #> Classification Random Forest ...  #> ---------- Parameters Summary ---------- #>               (N, P) = (1000, 20) #>           # of trees = 2000 #>         (mtry, nmin) = (20, 20) #>       split generate = Random, 1 #>             sampling = 0.5 w/o replace #>   (Obs, Var) weights = (No, No) #>           importance = none #>        reinforcement = No #> ----------------------------------------  # Predict with variance estimation RLTPred <- predict(RLTfit, testX, var.est = TRUE, keep.all = TRUE)  # Calculate confidence intervals for P(Y = 1) upper = RLTPred$Prob[, 2] + 1.96*sqrt(RLTPred$Variance[,2]) #> Warning in sqrt(RLTPred$Variance[, 2]): NaNs produced lower = RLTPred$Prob[, 2] - 1.96*sqrt(RLTPred$Variance[,2]) #> Warning in sqrt(RLTPred$Variance[, 2]): NaNs produced cover = (testprob > lower) & (testprob < upper)  # Plot results plot(testX$X1, RLTPred$Prob[,2], pch = 19,       col = ifelse(is.na(cover), \"red\", ifelse(cover, \"green\", \"black\")),      xlab = \"X1\", ylab = \"P(Y = 1)\",       xlim = c(min(testX$X1)-0.1, max(testX$X1)+0.1),       ylim = c(min(RLTPred$Prob[,1])-0.1, max(RLTPred$Prob[,1])+0.1),      main = \"Classification Confidence Intervals\")  lines(testX$X1, testprob, col = \"darkorange\", lwd = 2)  # Add confidence intervals for (i in 1:testn)   segments(testX$X1[i], lower[i], testX$X1[i], upper[i],             col = ifelse(is.na(cover[i]), \"red\", ifelse(cover[i], \"green\", \"black\")))  legend(\"topleft\", c(\"covered\", \"not covered\"), col = c(\"green\", \"black\"),         lty = 1, pch = 19, cex = 1.2)"},{"path":"https://teazrq.github.io/RLT/articles/Confidence-Interval.html","id":"survival-analysis-confidence-bands","dir":"Articles","previous_headings":"","what":"Survival Analysis Confidence Bands","title":"Confidence Interval Estimation","text":"survival analysis, RLT provides sophisticated methods constructing confidence bands around survival curves.","code":"# Generate survival data set.seed(2) n = 600 p = 20 X = matrix(rnorm(n*p), n, p)  # Create survival times xlink <- function(x) exp(x[, 1] + x[, 3]/2) FT = rexp(n, rate = xlink(X)) CT = pmin(6, rexp(n, rate = 0.25))  Y = pmin(FT, CT) Censor = as.numeric(FT <= CT)  # Generate test data ntest = 15 testx = matrix(rnorm(ntest*p), ntest, p)  # Calculate true survival function timepoints = sort(unique(Y[Censor==1])) SurvMat = matrix(NA, nrow(testx), length(timepoints)) exprate = xlink(testx)  for (j in 1:length(timepoints)) {   SurvMat[, j] = 1 - pexp(timepoints[j], rate = exprate) }"},{"path":"https://teazrq.github.io/RLT/articles/Confidence-Interval.html","id":"survival-model-fitting","dir":"Articles","previous_headings":"Survival Analysis Confidence Bands","what":"Survival Model Fitting","title":"Confidence Interval Estimation","text":"","code":"# Fit survival model RLTfit <- RLT(X, Y, Censor, model = \"survival\",                ntrees = 2000, nmin = 20, mtry = 20, split.gen = \"random\",               resample.prob = 0.5, resample.replace = FALSE, nsplit = 5,               param.control = list(split.rule = \"logrank\", \"var.ready\" = TRUE),                importance = FALSE, verbose = TRUE, ncores = 1) #> Fitting Survival Forest...  #> ---------- Parameters Summary ---------- #>               (N, P) = (600, 20) #>           # of trees = 2000 #>         (mtry, nmin) = (20, 20) #>       split generate = Random, 5 #>             sampling = 0.5 w/o replace #>   (Obs, Var) weights = (No, No) #>           importance = none #>        reinforcement = No #> ----------------------------------------  # Predict with variance estimation RLTPred <- predict(RLTfit, testx, ncores = 1, var.est = TRUE)"},{"path":"https://teazrq.github.io/RLT/articles/Confidence-Interval.html","id":"confidence-band-methods","dir":"Articles","previous_headings":"Survival Analysis Confidence Bands","what":"Confidence Band Methods","title":"Confidence Interval Estimation","text":"RLT provides several approaches constructing confidence bands:","code":""},{"path":"https://teazrq.github.io/RLT/articles/Confidence-Interval.html","id":"naive-monte-carlo-approach","dir":"Articles","previous_headings":"Survival Analysis Confidence Bands > Confidence Band Methods","what":"1. Naive Monte Carlo Approach","title":"Confidence Interval Estimation","text":"","code":"alpha = 0.05  # Original Monte Carlo approach without smoothing SurvBand = get.surv.band(RLTPred, alpha = alpha, approach = \"eigen-th-mc\")  # Plot results for first few subjects par(mfrow = c(2, 3)) logt = log(1 + timepoints)  for (i in 1:min(6, ntest)) {   # Truth   plot(logt, SurvMat[i, ], type = \"l\", lwd = 2, col = \"red\",         ylab = paste(\"Subject\", i), main = paste(\"Subject\", i))      # Estimated survival   lines(logt, RLTPred$Survival[i,], lwd = 2, col = \"black\")      # Naive confidence bands   lines(logt, SurvBand[[i]]$lower[,1], lty = 2, lwd = 2, col = \"deepskyblue\")   lines(logt, SurvBand[[i]]$upper[,1], lty = 2, lwd = 2, col = \"deepskyblue\") }"},{"path":"https://teazrq.github.io/RLT/articles/Confidence-Interval.html","id":"smoothed-monte-carlo-approach","dir":"Articles","previous_headings":"Survival Analysis Confidence Bands > Confidence Band Methods","what":"2. Smoothed Monte Carlo Approach","title":"Confidence Interval Estimation","text":"","code":"# Monte Carlo approach using smoothed rescaled covariance matrix SurvBand = get.surv.band(RLTPred, alpha = alpha, approach = \"smoothed-mc\", nsim = 1000) #> Warning: no DISPLAY variable so Tk is not available #> Warning in rgl.init(initValue, onlyNULL): RGL: unable to open X11 display #> Warning: 'rgl.init' failed, will use the null device. #> See '?rgl.useNULL' for ways to avoid this warning.  # Plot results par(mfrow = c(2, 3)) logt = log(1 + timepoints)  for (i in 1:min(6, ntest)) {   # Truth   plot(logt, SurvMat[i, ], type = \"l\", lwd = 2, col = \"red\",         ylab = paste(\"Subject\", i), main = paste(\"Subject\", i))      # Estimated survival   lines(logt, RLTPred$Survival[i,], lwd = 2, col = \"black\")      # Smoothed confidence bands   lines(logt, SurvBand[[i]]$lower[,1], lty = 2, lwd = 2, col = \"deepskyblue\")   lines(logt, SurvBand[[i]]$upper[,1], lty = 2, lwd = 2, col = \"deepskyblue\") }"},{"path":"https://teazrq.github.io/RLT/articles/Confidence-Interval.html","id":"smoothed-low-rank-monte-carlo","dir":"Articles","previous_headings":"Survival Analysis Confidence Bands > Confidence Band Methods","what":"3. Smoothed Low-rank Monte Carlo","title":"Confidence Interval Estimation","text":"","code":"# Low rank smoothed approach with adaptive Bonferroni critical value SurvBand = get.surv.band(RLTPred, alpha = alpha, approach = \"smoothed-lr\", r = 5)  # Plot results par(mfrow = c(2, 3)) logt = log(1 + timepoints)  for (i in 1:min(6, ntest)) {   # Truth   plot(logt, SurvMat[i, ], type = \"l\", lwd = 2, col = \"red\",         ylab = paste(\"Subject\", i), main = paste(\"Subject\", i))      # Estimated survival   lines(logt, RLTPred$Survival[i,], lwd = 2, col = \"black\")      # Low-rank confidence bands   lines(logt, SurvBand[[i]]$lower[,1], lty = 2, lwd = 2, col = \"deepskyblue\")   lines(logt, SurvBand[[i]]$upper[,1], lty = 2, lwd = 2, col = \"deepskyblue\") } # Report average approximation error cat(paste(\"Average approximation error:\", mean(sapply(SurvBand, \"[[\", 3)))) #> Average approximation error: 0.0906328215138488"},{"path":"https://teazrq.github.io/RLT/articles/Confidence-Interval.html","id":"covariance-matrix-visualization","dir":"Articles","previous_headings":"","what":"Covariance Matrix Visualization","title":"Confidence Interval Estimation","text":"","code":"# Visualize covariance matrices i = 5  # Choose a subject ccov = RLTPred$Cov[,,i]  # Original covariance matrix heatmap(ccov[rev(1:nrow(ccov)),], Rowv = NA, Colv = NA, symm = TRUE,         main = \"Original Covariance Matrix\") # Note: The smoothed covariance approximation code is used internally # in the get.surv.band function for better numerical stability"},{"path":"https://teazrq.github.io/RLT/articles/Confidence-Interval.html","id":"key-points","dir":"Articles","previous_headings":"","what":"Key Points","title":"Confidence Interval Estimation","text":"Regression: Variance estimation provides confidence intervals continuous predictions Classification: Variance estimation provides confidence intervals class probabilities Survival: Multiple approaches confidence bands around survival curves Naive Monte Carlo: Simple may computationally intensive Smoothed Monte Carlo: Uses smoothed covariance better numerical stability Low-rank Monte Carlo: Efficient approach adaptive critical values Coverage: Confidence intervals cover true values specified confidence level","code":""},{"path":"https://teazrq.github.io/RLT/articles/Confidence-Interval.html","id":"summary","dir":"Articles","previous_headings":"","what":"Summary","title":"Confidence Interval Estimation","text":"RLT package provides comprehensive tools confidence interval estimation across different types models. choice method depends model type computational requirements. survival analysis, smoothed approaches generally provide better numerical stability computational efficiency.","code":""},{"path":"https://teazrq.github.io/RLT/articles/Test-Cla.html","id":"install-and-load-package","dir":"Articles","previous_headings":"","what":"Install and Load Package","title":"RLT Package Testing Classification Functions and Features","text":"Install load GitHub version RLT package. use CRAN version. Load packages used guide.","code":"# install.packages(\"devtools\")   # devtools::install_github(\"teazrq/RLT\")   library(RLT) ## RLT and Random Forests v4.2.6 ## pre-release at github.com/teazrq/RLT library(randomForest)   library(randomForestSRC)   library(ranger)   library(parallel)"},{"path":"https://teazrq.github.io/RLT/articles/Test-Cla.html","id":"benchmark-against-existing-packages","dir":"Articles","previous_headings":"","what":"Benchmark Against Existing Packages","title":"RLT Package Testing Classification Functions and Features","text":"generate dataset 1000 observations 400 variables, 200 continuous variables 200 categorical ones three categories.","code":"# Set seed for reproducibility   set.seed(1)    # Define data size   trainn <- 800   testn <- 1000   n <- trainn + testn   p <- 30    # Generate continuous variables (X1) and categorical variables (X2)   X1 <- matrix(rnorm(n * p / 2), n, p / 2)   #X2 <- matrix(rnorm(n * p / 2), n, p / 2)   X2 <- matrix(as.integer(runif(n * p / 2) * 10), n, p / 2)    # Combine continuous and categorical variables into a data frame (X)   X <- data.frame(X1, X2)    # Convert the second half of the columns in X to factors   X[, (p / 2 + 1):p] <- lapply(X[, (p / 2 + 1):p], as.factor)    # Generate outcomes (y)   logit <- function(x) exp(x) / (1 + exp(x)) #  y <- as.factor(rbinom(n, 1, prob = logit(1 + rowSums(X[, 1:5]) + 2 * (X[, p / 2 + 1] %in% c(1, 3)) + rnorm(n))) + 2)      y <- as.factor(rbinom(n, 1, prob = logit(1 + 1*X[, 2] + 3*(X[, p] %in% c(1, 3, 5, 7)))) + 2)      # Set tuning parameters   ntrees <- 1000   ncores <- 10   nmin <- 20   mtry <- p/2   samplereplace <- TRUE   sampleprob <- 0.75   rule <- \"best\"   nsplit <- ifelse(rule == \"best\", 0, 3)   importance <- TRUE    # Split data into training and testing sets   trainX <- X[1:trainn, ]   trainY <- y[1:trainn]   testX <- X[(trainn + 1):(trainn + testn), ]   testY <- y[(trainn + 1):(trainn + testn)] # recording results   metric = data.frame(matrix(NA, 5, 6))   rownames(metric) = c(\"RLT\", \"randomForestSRC\", \"randomForest\", \"ranger\", \"ranger fast\")   colnames(metric) = c(\"fit.time\", \"pred.time\", \"oob.error\",                        \"pred.error\", \"obj.size\", \"ave.tree.size\")    # using RLT package    start_time <- Sys.time()   RLTfit <- RLT(trainX, trainY, model = \"classification\",                 ntrees = ntrees, mtry = mtry, nmin = nmin,                 resample.prob = sampleprob, split.gen = rule,                 resample.replace = samplereplace,                  nsplit = nsplit, importance = importance,                 param.control = list(\"alpha\" = 0),                 ncores = ncores, verbose = TRUE) ## Classification Random Forest ...  ## ---------- Parameters Summary ---------- ##               (N, P) = (800, 30) ##           # of trees = 1000 ##         (mtry, nmin) = (15, 20) ##       split generate = Best ##             sampling = 0.75 w/ replace ##   (Obs, Var) weights = (No, No) ##           importance = permute ##        reinforcement = No ## ---------------------------------------- ## Do not have 10 cores, use maximum 4 cores.   metric[1, 1] = difftime(Sys.time(), start_time, units = \"secs\")   start_time <- Sys.time()   RLTPred <- predict(RLTfit, testX, ncores = ncores)   metric[1, 2] = difftime(Sys.time(), start_time, units = \"secs\")   metric[1, 3] = mean(RLTfit$Prediction != trainY)   metric[1, 4] = mean(RLTPred$Prediction != testY)   metric[1, 5] = object.size(RLTfit)   metric[1, 6] = mean(unlist(lapply(RLTfit$FittedForest$SplitVar, length)))    # use randomForestSRC   options(rf.cores = ncores)   start_time <- Sys.time()   rsffit <- rfsrc(y ~ ., data = data.frame(trainX, \"y\"= trainY),                    ntree = ntrees, nodesize = nmin/2, mtry = mtry,                    samptype = ifelse(samplereplace == TRUE, \"swor\", \"swr\"),                   nsplit = nsplit, sampsize = trainn*sampleprob,                    importance = ifelse(importance, \"permute\", \"none\"))   metric[2, 1] = difftime(Sys.time(), start_time, units = \"secs\")   start_time <- Sys.time()   rsfpred = predict(rsffit, data.frame(testX))   metric[2, 2] = difftime(Sys.time(), start_time, units = \"secs\")   metric[2, 3] = mean(apply(rsffit$predicted.oob, 1, which.max) + 1 != trainY)   metric[2, 4] = mean(rsfpred$class != testY)   metric[2, 5] = object.size(rsffit)   metric[2, 6] = rsffit$forest$totalNodeCount / rsffit$ntree      # use randomForest   start_time <- Sys.time()   rf.fit <- randomForest(trainX, trainY, ntree = ntrees,                           mtry = mtry, nodesize = nmin,                           replace = samplereplace,                          sampsize = trainn*sampleprob,                           importance = importance)   metric[3, 1] = difftime(Sys.time(), start_time, units = \"secs\")   start_time <- Sys.time()   rf.pred <- predict(rf.fit, testX)   metric[3, 2] = difftime(Sys.time(), start_time, units = \"secs\")   metric[3, 3] = mean(rf.fit$predicted != trainY)   metric[3, 4] = mean(rf.pred != testY)   metric[3, 5] = object.size(rf.fit)   metric[3, 6] = mean(colSums(rf.fit$forest$nodestatus != 0))      # use ranger     start_time <- Sys.time()   rangerfit <- ranger(y ~ ., data = data.frame(trainX, \"y\"= trainY),                        num.trees = ntrees, min.node.size = nmin,                        mtry = mtry, num.threads = ncores,                        replace = samplereplace,                       sample.fraction = sampleprob,                        importance = \"permutation\",                       respect.unordered.factors = \"partition\") ## Growing trees.. Progress: 60%. Estimated remaining time: 21 seconds.   metric[4, 1] = difftime(Sys.time(), start_time, units = \"secs\")   start_time <- Sys.time()   rangerpred = predict(rangerfit, data.frame(testX))   metric[4, 2] = difftime(Sys.time(), start_time, units = \"secs\")   metric[4, 3] = mean(rangerfit$predictions != trainY)   metric[4, 4] = mean(rangerpred$predictions != testY)   metric[4, 5] = object.size(rangerfit)   metric[4, 6] = mean(unlist(lapply(rangerfit$forest$split.varIDs, length)))      # use ranger without partitioning   start_time <- Sys.time()   rangerfast <- ranger(y ~ ., data = data.frame(trainX, \"y\"= trainY),                         num.trees = ntrees, min.node.size = nmin,                         mtry = mtry, num.threads = ncores,                         replace = samplereplace,                        sample.fraction = sampleprob,                         importance = \"permutation\")   metric[5, 1] = difftime(Sys.time(), start_time, units = \"secs\")   start_time <- Sys.time()   rangerpred = predict(rangerfast, data.frame(testX))   metric[5, 2] = difftime(Sys.time(), start_time, units = \"secs\")   metric[5, 3] = mean(rangerfast$predictions != trainY)   metric[5, 4] = mean(rangerpred$predictions != testY)   metric[5, 5] = object.size(rangerfast)   metric[5, 6] = mean(unlist(lapply(rangerfast$forest$split.varIDs, length)))      # performance summary   metric ##                   fit.time  pred.time oob.error pred.error obj.size ave.tree.size ## RLT              3.8877819 0.04142427   0.18375      0.182  4432112        58.618 ## randomForestSRC 42.5205061 0.17455602   0.18250      0.171 12907088        71.766 ## randomForest     5.0521028 0.04990530   0.18000      0.176  2122056        50.650 ## ranger          52.2684577 0.09444880   0.19250      0.183  2182560        58.514 ## ranger fast      0.6899104 0.08232188   0.20125      0.201  2754232        76.386"},{"path":"https://teazrq.github.io/RLT/articles/Test-Cla.html","id":"print-a-single-tree","dir":"Articles","previous_headings":"","what":"Print a Single Tree","title":"RLT Package Testing Classification Functions and Features","text":"can use get.one.tree() function peek single tree.","code":"get.one.tree(RLTfit, 1) ## Tree #1 in the fitted classification forest: ##     SplitVar   SplitValue LeftNode RightNode NodeWeight  Prob.of..2 Prob.of..3 ## 1        X2    -0.6926111        2         3        600 0.193333333  0.8066667 ## 2   X4.1 (F)  448.0000000        4         5        146 0.390410959  0.6095890 ## 3  X15.1 (F)  852.0000000       26        27        454 0.129955947  0.8700441 ## 4   X8.1 (F)  306.0000000        6         7         98 0.510204082  0.4897959 ## 5        X3     0.6296932       20        21         48 0.145833333  0.8541667 ## 6  X15.1 (F)  682.0000000        8         9         57 0.666666667  0.3333333 ## 7  X10.1 (F)  442.0000000       14        15         41 0.292682927  0.7073171 ## 8       <NA>           NA       NA        NA         12 0.166666667  0.8333333 ## 9        X3     1.0694546       10        11         45 0.800000000  0.2000000 ## 10 X14.1 (F)  528.0000000       12        13         40 0.875000000  0.1250000 ## 11      <NA>           NA       NA        NA          5 0.200000000  0.8000000 ## 12      <NA>           NA       NA        NA         34 1.000000000  0.0000000 ## 13      <NA>           NA       NA        NA          6 0.166666667  0.8333333 ## 14      <NA>           NA       NA        NA         12 0.833333333  0.1666667 ## 15       X9     0.9688454       16        17         29 0.068965517  0.9310345 ## 16  X3.1 (F)  512.0000000       18        19         28 0.035714286  0.9642857 ## 17      <NA>           NA       NA        NA          1 1.000000000  0.0000000 ## 18      <NA>           NA       NA        NA         27 0.000000000  1.0000000 ## 19      <NA>           NA       NA        NA          1 1.000000000  0.0000000 ## 20  X3.1 (F)   32.0000000       22        23         40 0.050000000  0.9500000 ## 21      <NA>           NA       NA        NA          8 0.625000000  0.3750000 ## 22 X13.1 (F)  256.0000000       24        25         39 0.025641026  0.9743590 ## 23      <NA>           NA       NA        NA          1 1.000000000  0.0000000 ## 24      <NA>           NA       NA        NA         38 0.000000000  1.0000000 ## 25      <NA>           NA       NA        NA          1 1.000000000  0.0000000 ## 26  X4.1 (F)  778.0000000       28        29        227 0.215859031  0.7841410 ## 27       X3     2.0473906       52        53        227 0.044052863  0.9559471 ## 28  X1.1 (F)  892.0000000       30        31        145 0.296551724  0.7034483 ## 29 X11.1 (F)    2.0000000       46        47         82 0.073170732  0.9268293 ## 30       X4     0.3217051       32        33         38 0.552631579  0.4473684 ## 31 X13.1 (F)  256.0000000       36        37        107 0.205607477  0.7943925 ## 32  X7.1 (F)  902.0000000       34        35         27 0.777777778  0.2222222 ## 33      <NA>           NA       NA        NA         11 0.000000000  1.0000000 ## 34      <NA>           NA       NA        NA          7 0.285714286  0.7142857 ## 35      <NA>           NA       NA        NA         20 0.950000000  0.0500000 ## 36       X5     0.2664197       38        39         96 0.145833333  0.8541667 ## 37      <NA>           NA       NA        NA         11 0.727272727  0.2727273 ## 38  X7.1 (F) 1018.0000000       40        41         67 0.059701493  0.9402985 ## 39       X5     0.9116686       44        45         29 0.344827586  0.6551724 ## 40      <NA>           NA       NA        NA         10 0.300000000  0.7000000 ## 41      X12     1.1907859       42        43         57 0.017543860  0.9824561 ## 42      <NA>           NA       NA        NA         54 0.000000000  1.0000000 ## 43      <NA>           NA       NA        NA          3 0.333333333  0.6666667 ## 44      <NA>           NA       NA        NA         13 0.692307692  0.3076923 ## 45      <NA>           NA       NA        NA         16 0.062500000  0.9375000 ## 46      X11    -2.4272374       48        49         72 0.027777778  0.9722222 ## 47      <NA>           NA       NA        NA         10 0.400000000  0.6000000 ## 48      <NA>           NA       NA        NA          1 1.000000000  0.0000000 ## 49  X7.1 (F)  512.0000000       50        51         71 0.014084507  0.9859155 ## 50      <NA>           NA       NA        NA         66 0.000000000  1.0000000 ## 51      <NA>           NA       NA        NA          5 0.200000000  0.8000000 ## 52       X7     1.2252040       54        55        222 0.031531532  0.9684685 ## 53      <NA>           NA       NA        NA          5 0.600000000  0.4000000 ## 54      X11     2.3314162       56        57        192 0.010416667  0.9895833 ## 55       X7     1.4388455       60        61         30 0.166666667  0.8333333 ## 56      X11    -2.0074093       58        59        190 0.005263158  0.9947368 ## 57      <NA>           NA       NA        NA          2 0.500000000  0.5000000 ## 58      <NA>           NA       NA        NA          9 0.111111111  0.8888889 ## 59      <NA>           NA       NA        NA        181 0.000000000  1.0000000 ## 60      <NA>           NA       NA        NA          7 0.714285714  0.2857143 ## 61      <NA>           NA       NA        NA         23 0.000000000  1.0000000"},{"path":"https://teazrq.github.io/RLT/articles/Test-Debug.html","id":"one-dimensional-errors","dir":"Articles","previous_headings":"","what":"One-dimensional errors","title":"RLT Package Debug and Issues","text":"","code":"library(RLT) ## RLT and Random Forests v4.2.6 ## pre-release at github.com/teazrq/RLT   # Generate synthetic data   set.seed(1)   x <- runif(40, 0, 2*pi)   y <- 2*sin(x) + rnorm(length(x))   testx <- seq(0, 2*pi, 0.01)   x0 = 2      # fit single tree model   library(RLT)   tree.fit <- RLT(x = data.frame(x), y = y, model = \"regression\",                    ntrees = 1, nmin = 5, mtry = 1,                    resample.prob = 1, resample.replace = FALSE, # use all data                   split.gen = \"best\", verbose = TRUE) ## Regression Random Forest ...  ## ---------- Parameters Summary ---------- ##               (N, P) = (40, 1) ##           # of trees = 1 ##         (mtry, nmin) = (1, 5) ##       split generate = Best ##             sampling = 1 w/o replace ##   (Obs, Var) weights = (No, No) ##           importance = none ##        reinforcement = No ## ----------------------------------------"},{"path":"https://teazrq.github.io/RLT/articles/Test-RLT-Cla.html","id":"install-and-load-package","dir":"Articles","previous_headings":"","what":"Install and Load Package","title":"RLT Package: Classification With Reinforcement Learning and Linear Combination Splits","text":"Install load GitHub version RLT package. use CRAN version. Load packages used guide.","code":"# install.packages(\"devtools\")   # devtools::install_github(\"teazrq/RLT\")   library(RLT) ## RLT and Random Forests v4.2.6 ## pre-release at github.com/teazrq/RLT library(randomForest)   library(randomForestSRC)   library(ranger)   library(parallel)"},{"path":"https://teazrq.github.io/RLT/articles/Test-RLT-Cla.html","id":"single-variable-embedded-splitting","dir":"Articles","previous_headings":"","what":"Single Variable Embedded Splitting","title":"RLT Package: Classification With Reinforcement Learning and Linear Combination Splits","text":"reinforcement enabled, embedded random forest model corresponding variable importance measure used search best splitting rule. default setting parameters embedded model, however can still tune individually.","code":"# Set seed for reproducibility   set.seed(1)    # Define data size   trainn <- 800   testn <- 1000   n <- trainn + testn   p <- 30    # Generate continuous variables (X1) and categorical variables (X2)   X1 <- matrix(rnorm(n * p / 2), n, p / 2)   #X2 <- matrix(rnorm(n * p / 2), n, p / 2)   X2 <- matrix(as.integer(runif(n * p / 2) * 10), n, p / 2)    # Combine continuous and categorical variables into a data frame (X)   X <- data.frame(X1, X2)    # Convert the second half of the columns in X to factors   X[, (p / 2 + 1):p] <- lapply(X[, (p / 2 + 1):p], as.factor)    # Generate outcomes (y)   logit <- function(x) exp(x) / (1 + exp(x)) #  y <- as.factor(rbinom(n, 1, prob = logit(1 + rowSums(X[, 1:5]) + 2 * (X[, p / 2 + 1] %in% c(1, 3)) + rnorm(n))) + 2)      y <- as.factor(rbinom(n, 1, prob = logit(1 + 1*X[, 2] + 1*X[, 5] + 3*(X[, p] %in% c(1, 3, 5, 7)))) + 2)      # Set tuning parameters   ntrees <- 1000   ncores <- 10   nmin <- 20   mtry <- p/2   samplereplace <- TRUE   sampleprob <- 0.75   rule <- \"best\"   nsplit <- ifelse(rule == \"best\", 0, 3)   importance <- TRUE    # Split data into training and testing sets   trainX <- X[1:trainn, ]   trainY <- y[1:trainn]   testX <- X[(trainn + 1):(trainn + testn), ]   testY <- y[(trainn + 1):(trainn + testn)] start_time <- Sys.time()      RLTfit <- RLT(trainX, trainY,                 ntrees = 500, ncores = 10, nmin = 10,                 split.gen = \"random\", nsplit = 2,                 resample.prob = 0.8, resample.replace = FALSE,                 reinforcement = TRUE, importance = \"distribute\",                 param.control = list(\"embed.ntrees\" = 50,                                      \"embed.mtry\" = 2/3,                                      \"embed.nmin\" = 5,                                      \"alpha\" = 0.1),                 verbose = TRUE) ## Classification Random Forest ...  ## ---------- Parameters Summary ---------- ##               (N, P) = (800, 30) ##           # of trees = 500 ##         (mtry, nmin) = (10, 10) ##       split generate = Random, 2 ##             sampling = 0.8 w/o replace ##   (Obs, Var) weights = (No, No) ##                alpha = 0.1 ##           importance = distribute ##        reinforcement = Yes ## ---------------------------------------- ##  embed.ntrees            = 50 ##  embed.mtry              = 66.7% ##  embed.nmin              = 5 ##  embed.split.gen         = Random, 1 ##  embed.resample.replace  = TRUE ##  embed.resample.prob     = 0.9 ##  embed.mute              = 0 ##  embed.protect           = 14 ##  embed.threshold         = 0.25 ## ---------------------------------------- ## Do not have 10 cores, use maximum 4 cores.      difftime(Sys.time(), start_time, units = \"secs\") ## Time difference of 173.4728 secs      # prediction   RLTPred <- predict(RLTfit, testX, ncores = ncores)    # inbag and oobag errors   mean(RLTfit$Prediction != trainY) ## [1] 0.17875   mean(RLTPred$Prediction != testY) ## [1] 0.169      # VI   barplot(as.vector(RLTfit$VarImp), main = \"RLT\")"},{"path":"https://teazrq.github.io/RLT/articles/Test-RLT-Reg.html","id":"install-and-load-package","dir":"Articles","previous_headings":"","what":"Install and Load Package","title":"RLT Package: Regression With Reinforcement Learning and Linear Combination Splits","text":"Install load GitHub version RLT package. use CRAN version.","code":"# install.packages(\"devtools\")   # devtools::install_github(\"teazrq/RLT\")   library(RLT) ## RLT and Random Forests v4.2.6 ## pre-release at github.com/teazrq/RLT"},{"path":"https://teazrq.github.io/RLT/articles/Test-RLT-Reg.html","id":"single-variable-embedded-splitting","dir":"Articles","previous_headings":"","what":"Single Variable Embedded Splitting","title":"RLT Package: Regression With Reinforcement Learning and Linear Combination Splits","text":"reinforcement enabled, embedded random forest model corresponding variable importance measure used search best splitting rule. default setting parameters embedded model, however can still tune individually.  Check seed match","code":"set.seed(2)      n = 1000   p = 10   X = matrix(rnorm(n*p), n, p)   y = 1 + X[, 1] + X[, 3] + X[, 9] + rnorm(n)      testX = matrix(rnorm(n*p), n, p)   testy = 1 + testX[, 1] + testX[, 3] + testX[, 9] + rnorm(n)      start_time <- Sys.time()      RLTfit <- RLT(X, y, model = \"regression\",                  ntrees = 100, ncores = 1, nmin = 10,                 split.gen = \"random\", nsplit = 1,                 resample.prob = 0.85, resample.replace = FALSE,                 reinforcement = TRUE, importance = \"distribute\",                 param.control = list(\"embed.ntrees\" = 50,                                      \"embed.mtry\" = 1/2,                                      \"embed.nmin\" = 5),                 verbose = TRUE) ## Regression Random Forest ...  ## ---------- Parameters Summary ---------- ##               (N, P) = (1000, 10) ##           # of trees = 100 ##         (mtry, nmin) = (3, 10) ##       split generate = Random, 1 ##             sampling = 0.85 w/o replace ##   (Obs, Var) weights = (No, No) ##           importance = distribute ##        reinforcement = Yes ## ---------------------------------------- ##  embed.ntrees            = 50 ##  embed.mtry              = 50% ##  embed.nmin              = 5 ##  embed.split.gen         = Random, 1 ##  embed.resample.replace  = TRUE ##  embed.resample.prob     = 0.9 ##  embed.mute              = 0 ##  embed.protect           = 14 ##  embed.threshold         = 0.25 ## ----------------------------------------      difftime(Sys.time(), start_time, units = \"secs\") ## Time difference of 56.29952 secs        # oob error   mean((RLTfit$Prediction - y)^2, na.rm = TRUE) ## [1] 1.332247      # prediction error   pred = predict(RLTfit, testX)   mean((pred$Prediction - testy)^2) ## [1] 1.279387 # sparse variable importance   barplot(as.vector(RLTfit$VarImp), main = \"RLT\") # check one tree   get.one.tree(RLTfit, 1) ## Tree #1 in the fitted regression forest: ##     SplitVar   SplitValue LeftNode RightNode NodeWeight      NodeAve ## 1       V 1  -0.980294208        2         3        850  0.000000000 ## 2       V 3   1.081845086        4         5        128  0.000000000 ## 3       V 9  -1.975008396       42        43        722  0.000000000 ## 4       V 9  -0.567707078        6         7        109  0.000000000 ## 5       V 9  -3.314706284       38        39         19  0.000000000 ## 6       V 3  -0.166537182        8         9         40  0.000000000 ## 7       V 3  -0.227713509       20        21         69  0.000000000 ## 8       V 9  -1.057140123       10        11         21  0.000000000 ## 9       V 4   0.129346882       14        15         19  0.000000000 ## 10      <NA>           NA       NA        NA          9 -2.946581367 ## 11     V 10  -0.965233503       12        13         12  0.000000000 ## 12      <NA>           NA       NA        NA          3 -2.357358172 ## 13      <NA>           NA       NA        NA          9 -1.413073684 ## 14      V 4  -1.475830364       16        17         13  0.000000000 ## 15      <NA>           NA       NA        NA          6 -2.569854211 ## 16      <NA>           NA       NA        NA          1 -0.404507502 ## 17      V 4  -1.041379040       18        19         12  0.000000000 ## 18      <NA>           NA       NA        NA          4 -0.795853226 ## 19      <NA>           NA       NA        NA          8 -1.597350858 ## 20      V 3  -1.068702106       22        23         33  0.000000000 ## 21      V 9  -0.432333301       26        27         36  0.000000000 ## 22      V 1  -2.099225632       24        25         11  0.000000000 ## 23      <NA>           NA       NA        NA         22 -0.307904267 ## 24      <NA>           NA       NA        NA          2 -2.892655219 ## 25      <NA>           NA       NA        NA          9 -2.312824540 ## 26      <NA>           NA       NA        NA          2  0.347616707 ## 27      V 9   0.063996427       28        29         34  0.000000000 ## 28      <NA>           NA       NA        NA         10 -0.311221632 ## 29      V 5   0.888988473       30        31         24  0.000000000 ## 30      V 5  -1.209662195       32        33         21  0.000000000 ## 31      <NA>           NA       NA        NA          3  1.352770278 ## 32      <NA>           NA       NA        NA          2 -0.989977059 ## 33      V 3  -0.059455969       34        35         19  0.000000000 ## 34      <NA>           NA       NA        NA          5 -0.128353622 ## 35      V 5  -0.229565449       36        37         14  0.000000000 ## 36      <NA>           NA       NA        NA          8  1.171801487 ## 37      <NA>           NA       NA        NA          6  0.667392199 ## 38      <NA>           NA       NA        NA          1 -3.210977199 ## 39      V 5  -0.747214107       40        41         18  0.000000000 ## 40      <NA>           NA       NA        NA          9  1.278504984 ## 41      <NA>           NA       NA        NA          9  0.790146990 ## 42      V 1  -0.012259387       44        45         22  0.000000000 ## 43      V 9  -0.342721271       48        49        700  0.000000000 ## 44      <NA>           NA       NA        NA         10 -2.144898360 ## 45      V 1   0.372123840       46        47         12  0.000000000 ## 46      <NA>           NA       NA        NA          3 -1.609794044 ## 47      <NA>           NA       NA        NA          9 -0.491322769 ## 48      V 3   0.943714876       50        51        249  0.000000000 ## 49      V 3  -0.390820753      130       131        451  0.000000000 ## 50      V 1  -0.948187756       52        53        209  0.000000000 ## 51      V 1   0.364553558      124       125         40  0.000000000 ## 52      <NA>           NA       NA        NA          2 -2.086041019 ## 53      V 1  -0.381395987       54        55        207  0.000000000 ## 54      V 3   0.840826107       56        57         48  0.000000000 ## 55      V 1   0.069347145       72        73        159  0.000000000 ## 56      V 3   0.494422039       58        59         47  0.000000000 ## 57      <NA>           NA       NA        NA          1 -0.045884843 ## 58      V 3  -0.592980770       60        61         39  0.000000000 ## 59      <NA>           NA       NA        NA          8 -0.131433235 ## 60      V 3  -0.693980785       62        63         16  0.000000000 ## 61      V 8   0.366139663       66        67         23  0.000000000 ## 62      V 3  -1.249552132       64        65         15  0.000000000 ## 63      <NA>           NA       NA        NA          1 -0.379246393 ## 64      <NA>           NA       NA        NA          9 -1.656711905 ## 65      <NA>           NA       NA        NA          6 -0.418339252 ## 66      V 7   0.844452684       68        69         11  0.000000000 ## 67      V 5  -0.038589072       70        71         12  0.000000000 ## 68      <NA>           NA       NA        NA         10 -0.722036381 ## 69      <NA>           NA       NA        NA          1 -0.433014076 ## 70      <NA>           NA       NA        NA          7 -0.631405423 ## 71      <NA>           NA       NA        NA          5 -1.106582171 ## 72      V 3   0.253750270       74        75         46  0.000000000 ## 73      V 1   1.176608935       88        89        113  0.000000000 ## 74      V 3  -1.402474593       76        77         30  0.000000000 ## 75      V 9  -0.870533884       86        87         16  0.000000000 ## 76      <NA>           NA       NA        NA          5 -2.292160534 ## 77      V 9  -1.248944399       78        79         25  0.000000000 ## 78      <NA>           NA       NA        NA          9 -0.821608795 ## 79      V 4  -0.707155481       80        81         16  0.000000000 ## 80      <NA>           NA       NA        NA          2 -1.410490863 ## 81      V 4   0.541512747       82        83         14  0.000000000 ## 82      V 4  -0.360714757       84        85         11  0.000000000 ## 83      <NA>           NA       NA        NA          3 -1.016217441 ## 84      <NA>           NA       NA        NA          3  0.956852740 ## 85      <NA>           NA       NA        NA          8 -0.177579922 ## 86      <NA>           NA       NA        NA         10 -0.445508047 ## 87      <NA>           NA       NA        NA          6  1.362056300 ## 88      V 9  -0.414155432       90        91         77  0.000000000 ## 89      V 7  -1.310902523      112       113         36  0.000000000 ## 90      V 9  -0.704789443       92        93         74  0.000000000 ## 91      <NA>           NA       NA        NA          3  0.822624630 ## 92      V 9  -1.334623845       94        95         49  0.000000000 ## 93      V 3   0.502624752      106       107         25  0.000000000 ## 94      V 3  -0.587525038       96        97         20  0.000000000 ## 95      V 5  -0.588941176      100       101         29  0.000000000 ## 96      <NA>           NA       NA        NA          3 -2.101852726 ## 97      V 9  -1.546749861       98        99         17  0.000000000 ## 98      <NA>           NA       NA        NA          9 -0.377257958 ## 99      <NA>           NA       NA        NA          8  0.502438622 ## 100     <NA>           NA       NA        NA         12 -0.152758117 ## 101     V 6   0.188828891      102       103         17  0.000000000 ## 102     V 3   0.371449427      104       105         11  0.000000000 ## 103     <NA>           NA       NA        NA          6  1.351209989 ## 104     <NA>           NA       NA        NA          7  0.124928479 ## 105     <NA>           NA       NA        NA          4  1.406889173 ## 106     V 3  -0.254496482      108       109         22  0.000000000 ## 107     <NA>           NA       NA        NA          3  1.600456857 ## 108     V 8  -0.608467484      110       111         14  0.000000000 ## 109     <NA>           NA       NA        NA          8  1.240444565 ## 110     <NA>           NA       NA        NA          6  0.830417982 ## 111     <NA>           NA       NA        NA          8 -0.208581743 ## 112     <NA>           NA       NA        NA          4  1.602902041 ## 113     V 3   0.740537784      114       115         32  0.000000000 ## 114     V 9  -0.432452981      116       117         28  0.000000000 ## 115     <NA>           NA       NA        NA          4  3.109474634 ## 116     V 9  -1.529638071      118       119         27  0.000000000 ## 117     <NA>           NA       NA        NA          1 -0.588311255 ## 118     <NA>           NA       NA        NA          3  1.059922330 ## 119     V 9  -0.758692636      120       121         24  0.000000000 ## 120     V 3  -1.072888333      122       123         14  0.000000000 ## 121     <NA>           NA       NA        NA         10  2.265950232 ## 122     <NA>           NA       NA        NA          2 -0.159231460 ## 123     <NA>           NA       NA        NA         12  1.259372165 ## 124     <NA>           NA       NA        NA         25  1.274163571 ## 125     V 7  -1.156705580      126       127         15  0.000000000 ## 126     <NA>           NA       NA        NA          1  5.203009807 ## 127     V 6   0.448026134      128       129         14  0.000000000 ## 128     <NA>           NA       NA        NA         11  2.494537473 ## 129     <NA>           NA       NA        NA          3  2.466676960 ## 130     V 9  -0.164446376      132       133        141  0.000000000 ## 131     V 1   1.421779808      168       169        310  0.000000000 ## 132     V 1   1.092478657      134       135         22  0.000000000 ## 133     V 1   0.667522687      138       139        119  0.000000000 ## 134     V 1   0.290622192      136       137         20  0.000000000 ## 135     <NA>           NA       NA        NA          2  0.748583516 ## 136     <NA>           NA       NA        NA         13 -1.068779685 ## 137     <NA>           NA       NA        NA          7  0.316972118 ## 138     V 9   2.145745584      140       141         74  0.000000000 ## 139     V 3  -0.968510334      162       163         45  0.000000000 ## 140     V 9   0.378459341      142       143         72  0.000000000 ## 141     <NA>           NA       NA        NA          2  2.861149330 ## 142     V 3  -1.026005198      144       145         24  0.000000000 ## 143     V 1  -0.601160105      148       149         48  0.000000000 ## 144     V 3  -1.784254983      146       147         14  0.000000000 ## 145     <NA>           NA       NA        NA         10  0.352463908 ## 146     <NA>           NA       NA        NA          7 -1.342753577 ## 147     <NA>           NA       NA        NA          7 -0.478125604 ## 148     <NA>           NA       NA        NA         10  0.452243129 ## 149     V 1  -0.269269376      150       151         38  0.000000000 ## 150     V 3  -2.031660362      152       153         12  0.000000000 ## 151     V 1  -0.249391973      156       157         26  0.000000000 ## 152     <NA>           NA       NA        NA          1 -0.161411369 ## 153     V 3  -0.694417185      154       155         11  0.000000000 ## 154     <NA>           NA       NA        NA         10  0.918789555 ## 155     <NA>           NA       NA        NA          1  0.373254813 ## 156     <NA>           NA       NA        NA          1  0.304052770 ## 157     V 1   0.302551135      158       159         25  0.000000000 ## 158     V 4  -0.768414248      160       161         15  0.000000000 ## 159     <NA>           NA       NA        NA         10  2.090603844 ## 160     <NA>           NA       NA        NA          8  0.257445331 ## 161     <NA>           NA       NA        NA          7  1.727717773 ## 162     V 5   0.950067905      164       165         28  0.000000000 ## 163     V 3  -0.769979488      166       167         17  0.000000000 ## 164     <NA>           NA       NA        NA         21  1.560635190 ## 165     <NA>           NA       NA        NA          7  1.387853122 ## 166     <NA>           NA       NA        NA          7  1.241767160 ## 167     <NA>           NA       NA        NA         10  3.155629542 ## 168     V 1   0.790544664      170       171        275  0.000000000 ## 169     V 9   0.623554849      258       259         35  0.000000000 ## 170     V 3  -0.084940836      172       173        223  0.000000000 ## 171     V 3   0.763312927      244       245         52  0.000000000 ## 172     V 9   1.081387426      174       175         36  0.000000000 ## 173     V 3   2.051815273      186       187        187  0.000000000 ## 174     V 9   0.270557457      176       177         28  0.000000000 ## 175     <NA>           NA       NA        NA          8  2.646220066 ## 176     V 8  -0.750501199      178       179         15  0.000000000 ## 177     V 1  -0.471508188      184       185         13  0.000000000 ## 178     <NA>           NA       NA        NA          1 -1.433253766 ## 179     V 7  -2.138510389      180       181         14  0.000000000 ## 180     <NA>           NA       NA        NA          1 -0.079023630 ## 181     V 8   0.943044327      182       183         13  0.000000000 ## 182     <NA>           NA       NA        NA         10  0.272021119 ## 183     <NA>           NA       NA        NA          3  2.611826807 ## 184     <NA>           NA       NA        NA          3 -0.052476521 ## 185     <NA>           NA       NA        NA         10  2.277160016 ## 186     V 9   1.343866134      188       189        178  0.000000000 ## 187     <NA>           NA       NA        NA          9  4.222427173 ## 188     V 3   1.332984816      190       191        167  0.000000000 ## 189    V 10  -1.350376528      242       243         11  0.000000000 ## 190     V 3   1.136046868      192       193        143  0.000000000 ## 191     V 1   0.150069231      236       237         24  0.000000000 ## 192     V 3   0.276076882      194       195        134  0.000000000 ## 193     <NA>           NA       NA        NA          9  2.842131069 ## 194     V 9   0.924490900      196       197         48  0.000000000 ## 195     V 1  -0.126527401      210       211         86  0.000000000 ## 196     V 1  -0.562247053      198       199         44  0.000000000 ## 197     <NA>           NA       NA        NA          4  2.557928653 ## 198     V 2   0.921662358      200       201         15  0.000000000 ## 199     V 4   0.063461054      204       205         29  0.000000000 ## 200     V 2  -0.073666563      202       203         13  0.000000000 ## 201     <NA>           NA       NA        NA          2 -0.001233008 ## 202     <NA>           NA       NA        NA          7  0.381035910 ## 203     <NA>           NA       NA        NA          6  1.281298336 ## 204     V 7  -0.543694073      206       207         17  0.000000000 ## 205     V 3   0.006409995      208       209         12  0.000000000 ## 206     <NA>           NA       NA        NA          7  2.082875104 ## 207     <NA>           NA       NA        NA         10  1.414596639 ## 208     <NA>           NA       NA        NA          5  0.517814545 ## 209     <NA>           NA       NA        NA          7  1.425930355 ## 210     V 4   0.835895726      212       213         37  0.000000000 ## 211     V 3   0.709453728      224       225         49  0.000000000 ## 212     V 4  -0.269537878      214       215         32  0.000000000 ## 213     <NA>           NA       NA        NA          5  1.008647127 ## 214     V 9   0.677534260      216       217         15  0.000000000 ## 215     V 4   0.624935172      220       221         17  0.000000000 ## 216     V 9   0.166190376      218       219         11  0.000000000 ## 217     <NA>           NA       NA        NA          4  2.762488633 ## 218     <NA>           NA       NA        NA          7  1.473904804 ## 219     <NA>           NA       NA        NA          4  2.333795962 ## 220     V 7   0.285653119      222       223         13  0.000000000 ## 221     <NA>           NA       NA        NA          4  0.678882941 ## 222     <NA>           NA       NA        NA          8  2.106691243 ## 223     <NA>           NA       NA        NA          5  0.496236762 ## 224     <NA>           NA       NA        NA         28  1.894484219 ## 225     V 6  -1.105478996      226       227         21  0.000000000 ## 226     <NA>           NA       NA        NA          3  3.246087325 ## 227     V 4   1.059238428      228       229         18  0.000000000 ## 228     V 4  -0.480899694      230       231         17  0.000000000 ## 229     <NA>           NA       NA        NA          1  1.733109373 ## 230     <NA>           NA       NA        NA          4  1.723979096 ## 231     V 4  -0.418572196      232       233         13  0.000000000 ## 232     <NA>           NA       NA        NA          1  1.265400523 ## 233     V 6  -0.076132076      234       235         12  0.000000000 ## 234     <NA>           NA       NA        NA          3  3.949907056 ## 235     <NA>           NA       NA        NA          9  2.929014449 ## 236     V 9   0.713825962      238       239         16  0.000000000 ## 237     <NA>           NA       NA        NA          8  2.837167998 ## 238     V 9  -0.132071548      240       241         13  0.000000000 ## 239     <NA>           NA       NA        NA          3  3.887067036 ## 240     <NA>           NA       NA        NA          5  1.755728835 ## 241     <NA>           NA       NA        NA          8  2.824642864 ## 242     <NA>           NA       NA        NA          4  4.181286753 ## 243     <NA>           NA       NA        NA          7  2.775822127 ## 244     V 9   1.238645228      246       247         29  0.000000000 ## 245     V 9   0.404696084      254       255         23  0.000000000 ## 246     V 9   0.063649950      248       249         25  0.000000000 ## 247     <NA>           NA       NA        NA          4  3.925580300 ## 248     <NA>           NA       NA        NA          8  2.162254340 ## 249     V 7  -0.900734272      250       251         17  0.000000000 ## 250     <NA>           NA       NA        NA          3  2.495057397 ## 251     V 4  -0.527972240      252       253         14  0.000000000 ## 252     <NA>           NA       NA        NA          4  3.024567839 ## 253     <NA>           NA       NA        NA         10  3.830827884 ## 254     <NA>           NA       NA        NA         10  3.632664827 ## 255     V 5  -0.112974489      256       257         13  0.000000000 ## 256     <NA>           NA       NA        NA          6  5.679580559 ## 257     <NA>           NA       NA        NA          7  4.553183634 ## 258     V 3   1.530286592      260       261         14  0.000000000 ## 259     V 5   1.043822482      264       265         21  0.000000000 ## 260     V 9  -0.110898622      262       263         13  0.000000000 ## 261     <NA>           NA       NA        NA          1  4.090908056 ## 262     <NA>           NA       NA        NA          3  2.442667057 ## 263     <NA>           NA       NA        NA         10  4.097267466 ## 264     V 5  -0.772476544      266       267         19  0.000000000 ## 265     <NA>           NA       NA        NA          2  5.976713414 ## 266     <NA>           NA       NA        NA          7  4.880034990 ## 267     V 4   0.402821807      268       269         12  0.000000000 ## 268     <NA>           NA       NA        NA          9  4.793675377 ## 269     <NA>           NA       NA        NA          3  6.759036121 RLTrep <- RLT(X, y, model = \"regression\",                  ntrees = 100, ncores = 1, nmin = 10,                 split.gen = \"random\", nsplit = 1,                 resample.prob = 0.85, resample.replace = FALSE,                 reinforcement = TRUE, importance = \"distribute\",                 param.control = list(\"embed.ntrees\" = 50,                                      \"embed.mtry\" = 1/2,                                      \"embed.nmin\" = 5),                 verbose = TRUE, seed = RLTfit$parameters$seed) ## Regression Random Forest ...  ## ---------- Parameters Summary ---------- ##               (N, P) = (1000, 10) ##           # of trees = 100 ##         (mtry, nmin) = (3, 10) ##       split generate = Random, 1 ##             sampling = 0.85 w/o replace ##   (Obs, Var) weights = (No, No) ##           importance = distribute ##        reinforcement = Yes ## ---------------------------------------- ##  embed.ntrees            = 50 ##  embed.mtry              = 50% ##  embed.nmin              = 5 ##  embed.split.gen         = Random, 1 ##  embed.resample.replace  = TRUE ##  embed.resample.prob     = 0.9 ##  embed.mute              = 0 ##  embed.protect           = 14 ##  embed.threshold         = 0.25 ## ----------------------------------------    all(RLTfit$VarImp == RLTrep$VarImp) ## [1] TRUE"},{"path":"https://teazrq.github.io/RLT/articles/Test-RLT-Reg.html","id":"linear-combination-split","dir":"Articles","previous_headings":"","what":"Linear Combination Split","title":"RLT Package: Regression With Reinforcement Learning and Linear Combination Splits","text":"can also use linear combination variables splitting rule, .e., 𝟏(𝛃T𝐱>c) \\mathbf{1}( \\boldsymbol \\beta^T \\mathbf{x} > c )  search top variables embedded random forest, however, 𝛃\\boldsymbol \\beta determined using criteria \"naive\" appraoch proposed original paper (Zhu, et al. 2015), PCA (\"pca\"), linear regression (\"lm\") sliced inverse regression (\"sir\"). categorical variable encountered (random best internal node), algorithm switch default single variable split.","code":"# set.seed(1)   library(MASS)   ntrain = 300   ntest = 500   n = ntrain + ntest   p = 10   S = matrix(0.3, p, p)   diag(S) = 1    X1 = mvrnorm(n, mu = rep(0, p), Sigma = S)   X2 <- matrix(as.integer(runif(n * p) * 5), n, p)    # Combine continuous and categorical variables into a data frame (X)   X <- data.frame(X1, X2)    # Convert the second half of the columns in X to factors   X[, (p + 1):(2*p)] <- lapply(X[, (p+1):(2*p)], as.factor)      xlink <- function(x) 1 + x[, 1] + x[, 3] + x[, ncol(x)] %in% c(0,2,4)      # outcome   y = xlink(X) + rnorm(n)   w = runif(ntrain)      xtrain = X[1:ntrain, ]   ytrain = y[1:ntrain]   xtest = X[-(1:ntrain), ]   ytest = y[-(1:ntrain)]    start_time <- Sys.time()   RLTfit <- RLT(xtrain, ytrain, model = \"regression\", obs.w = w,                 ntrees = 100, ncores = 1, nmin = 10, mtry = 10,                 split.gen = \"random\", nsplit = 2,                 resample.prob = 0.9, resample.replace = FALSE,                  param.control = list(\"linear.comb\" = 3,                                      \"split.rule\" = \"naive\",                                      \"embed.ntrees\" = 50,                                      \"embed.mtry\" = 0.5,                                      \"embed.nmin\" = 5,                                      \"embed.split.gen\" = \"random\",                                      \"embed.nsplit\" = 3,                                      \"embed.resample.replace\" = FALSE,                                      \"embed.resample.prob\" = 0.9,                                      \"embed.mute\" = 1/3,                                      \"embed.protect\" = 3,                                      \"embed.threshold\" = 0.25),                 importance = \"permute\",                  verbose = TRUE) ## Regression Forest with Linear Combination Splits ...  ## ---------- Parameters Summary ---------- ##               (N, P) = (300, 20) ##           # of trees = 100 ##         (mtry, nmin) = (10, 10) ##       split generate = Random, 2 ##             sampling = 0.9 w/o replace ##   (Obs, Var) weights = (Yes, No) ##   linear combination = 3 ##           split rule = naive ##           importance = permute ##        reinforcement = No ## ----------------------------------------   difftime(Sys.time(), start_time, units = \"secs\") ## Time difference of 6.851847 secs      # oob prediction and error   plot(RLTfit$Prediction, ytrain) mean( (RLTfit$Prediction - ytrain)^2 , na.rm = TRUE) ## [1] 1.235547      # testing data error   mean((predict(RLTfit, xtest)$Prediction - ytest)^2) ## [1] 1.149594 start_time <- Sys.time()   RLTvi2 <- RLT(xtrain, ytrain, model = \"regression\", obs.w = w,                 ntrees = 100, ncores = 1, nmin = 10, mtry = 10,                 split.gen = \"random\", nsplit = 2,                 resample.prob = 0.9, resample.replace = FALSE,                  param.control = list(\"linear.comb\" = 3,                                      \"split.rule\" = \"sir\",                                      \"embed.ntrees\" = 50,                                      \"embed.mtry\" = 0.5,                                      \"embed.nmin\" = 5,                                      \"embed.split.gen\" = \"random\",                                      \"embed.nsplit\" = 3,                                      \"embed.resample.replace\" = FALSE,                                      \"embed.resample.prob\" = 0.9,                                      \"embed.mute\" = 1/3,                                      \"embed.protect\" = 3,                                      \"embed.threshold\" = 0.25),                 importance = \"distribute\",                  verbose = TRUE) ## Regression Forest with Linear Combination Splits ...  ## ---------- Parameters Summary ---------- ##               (N, P) = (300, 20) ##           # of trees = 100 ##         (mtry, nmin) = (10, 10) ##       split generate = Random, 2 ##             sampling = 0.9 w/o replace ##   (Obs, Var) weights = (Yes, No) ##   linear combination = 3 ##           split rule = sir ##           importance = distribute ##        reinforcement = No ## ----------------------------------------   difftime(Sys.time(), start_time, units = \"secs\") ## Time difference of 6.842688 secs par(mfrow=c(1,2))   par(mar = c(1, 2, 2, 2))      # sparse variable importance   barplot(as.vector(RLTfit$VarImp), main = \"RLT Permutation VI\")   barplot(as.vector(RLTvi2$VarImp), main = \"RLT Distributed VI\") # check one tree   get.one.tree(RLTfit, 1) ## Tree #1 in the fitted linear combination regression forest: ##    SplitVar.1 SplitVar.2 SplitVar.3 SplitLoad.1 SplitLoad.2 SplitLoad.3  SplitValue LeftNode RightNode  NodeWeight    NodeAve ## 1         X3         X1               1.3231691  1.23174687           0 -0.07706315        2         3 0.908612391  0.0000000 ## 2         X1         X3               0.8274917  0.74400121           0 -0.18481930        4         5 0.422003759  0.0000000 ## 3         X3         X1               0.9085061  0.77844733           0  0.61676199       26        27 0.486608632  0.0000000 ## 4         X3         X1               0.7920686  0.74643704           0 -1.26713524        6         7 0.380656316  0.0000000 ## 5        <NA>       <NA>       <NA>          NA          NA          NA          NA       NA        NA 0.041347443  1.5303985 ## 6   X10.1 (F)                         1.0000000  0.00000000           0 34.00000000        8         9 0.109248402  0.0000000 ## 7   X10.1 (F)                         1.0000000  0.00000000           0 40.00000000       16        17 0.271407914  0.0000000 ## 8         X1                          1.0000000  0.00000000           0 -1.57700855       10        11 0.062214882  0.0000000 ## 9         X1                          1.0000000  0.00000000           0 -1.35618526       14        15 0.047033519  0.0000000 ## 10       <NA>       <NA>       <NA>          NA          NA          NA          NA       NA        NA 0.032011465 -2.6569265 ## 11        X3                          1.0000000  0.00000000           0 -2.34377950       12        13 0.030203418  0.0000000 ## 12       <NA>       <NA>       <NA>          NA          NA          NA          NA       NA        NA 0.006914019 -2.7335368 ## 13       <NA>       <NA>       <NA>          NA          NA          NA          NA       NA        NA 0.023289398 -0.8729562 ## 14       <NA>       <NA>       <NA>          NA          NA          NA          NA       NA        NA 0.021295620 -0.4331007 ## 15       <NA>       <NA>       <NA>          NA          NA          NA          NA       NA        NA 0.025737899  0.1901192 ## 16  X10.1 (F)                         1.0000000  0.00000000           0  2.00000000       18        19 0.191915479  0.0000000 ## 17        X3        X10               0.4931212  0.43142204           0 -0.37759703       20        21 0.079492435  0.0000000 ## 18       <NA>       <NA>       <NA>          NA          NA          NA          NA       NA        NA 0.141408082  0.1102366 ## 19       <NA>       <NA>       <NA>          NA          NA          NA          NA       NA        NA 0.050507398  0.6981400 ## 20       <NA>       <NA>       <NA>          NA          NA          NA          NA       NA        NA 0.022320250  1.4395327 ## 21        X1                          1.0000000  0.00000000           0 -0.54953057       22        23 0.057172185  0.0000000 ## 22        X1                          1.0000000  0.00000000           0 -1.63211666       24        25 0.047952749  0.0000000 ## 23       <NA>       <NA>       <NA>          NA          NA          NA          NA       NA        NA 0.009219436  1.5860325 ## 24       <NA>       <NA>       <NA>          NA          NA          NA          NA       NA        NA 0.011230709  1.7119574 ## 25       <NA>       <NA>       <NA>          NA          NA          NA          NA       NA        NA 0.036722040  0.7172546 ## 26        X7                          1.0000000  0.00000000           0 -1.67131191       28        29 0.213692879  0.0000000 ## 27        X1         X3               0.9747714  0.84941119           0  2.66452790       50        51 0.272915754  0.0000000 ## 28       <NA>       <NA>       <NA>          NA          NA          NA          NA       NA        NA 0.014994611  0.9554701 ## 29        X7         X2              -0.4209997  0.22710658           0  0.31062849       30        31 0.198698267  0.0000000 ## 30        X7                          1.0000000  0.00000000           0  1.87526053       32        33 0.153665240  0.0000000 ## 31       <NA>       <NA>       <NA>          NA          NA          NA          NA       NA        NA 0.045033028  2.4412080 ## 32        X7                          1.0000000  0.00000000           0  0.79648732       34        35 0.147998682  0.0000000 ## 33       <NA>       <NA>       <NA>          NA          NA          NA          NA       NA        NA 0.005666558  2.9048770 ## 34   X5.1 (F)                         1.0000000  0.00000000           0 60.00000000       36        37 0.101844543  0.0000000 ## 35        X2                          1.0000000  0.00000000           0  0.14897519       48        49 0.046154139  0.0000000 ## 36       <NA>       <NA>       <NA>          NA          NA          NA          NA       NA        NA 0.015924004  1.4467864 ## 37        X3         X1               0.4007989 -0.34891053           0  0.51704594       38        39 0.085920539  0.0000000 ## 38        X7         X1              -0.2657772  0.25425640           0  0.14424181       40        41 0.079711804  0.0000000 ## 39       <NA>       <NA>       <NA>          NA          NA          NA          NA       NA        NA 0.006208735  3.4318513 ## 40        X3         X7               0.1431964 -0.13490213           0  0.04575013       42        43 0.054982336  0.0000000 ## 41       <NA>       <NA>       <NA>          NA          NA          NA          NA       NA        NA 0.024729468  2.4318951 ## 42        X1                          1.0000000  0.00000000           0  0.60092053       44        45 0.044591402  0.0000000 ## 43       <NA>       <NA>       <NA>          NA          NA          NA          NA       NA        NA 0.010390934  2.6567664 ## 44        X7         X3              -0.1037545 -0.06234505           0 -0.05734202       46        47 0.032246716  0.0000000 ## 45       <NA>       <NA>       <NA>          NA          NA          NA          NA       NA        NA 0.012344686  2.7439300 ## 46       <NA>       <NA>       <NA>          NA          NA          NA          NA       NA        NA 0.004994244  1.7678419 ## 47       <NA>       <NA>       <NA>          NA          NA          NA          NA       NA        NA 0.027252472  1.5191137 ## 48       <NA>       <NA>       <NA>          NA          NA          NA          NA       NA        NA 0.023854892  0.7492995 ## 49       <NA>       <NA>       <NA>          NA          NA          NA          NA       NA        NA 0.022299248  1.4490659 ## 50        X3         X1               0.4206006  0.21512389           0  0.31635158       52        53 0.240125913  0.0000000 ## 51        X1         X3               1.1773737 -1.09770251           0 -1.18651743       66        67 0.032789840  0.0000000 ## 52       <NA>       <NA>       <NA>          NA          NA          NA          NA       NA        NA 0.038752381  2.1558582 ## 53        X3         X1               0.7759470  0.50676296           0  1.32511136       54        55 0.201373532  0.0000000 ## 54        X3                          1.0000000  0.00000000           0  0.57031210       56        57 0.150660578  0.0000000 ## 55        X3                          1.0000000  0.00000000           0  2.45611914       62        63 0.050712954  0.0000000 ## 56        X1                          1.0000000  0.00000000           0  0.67884452       58        59 0.042563050  0.0000000 ## 57       <NA>       <NA>       <NA>          NA          NA          NA          NA       NA        NA 0.108097528  2.7121510 ## 58       <NA>       <NA>       <NA>          NA          NA          NA          NA       NA        NA 0.004856537  1.3274791 ## 59        X1         X3               0.4855952  0.40446088           0  0.75671826       60        61 0.037706514  0.0000000 ## 60       <NA>       <NA>       <NA>          NA          NA          NA          NA       NA        NA 0.023900269  2.3772008 ## 61       <NA>       <NA>       <NA>          NA          NA          NA          NA       NA        NA 0.013806244  3.3803396 ## 62        X3         X1               0.4581321  0.38550016           0  1.05865315       64        65 0.047123176  0.0000000 ## 63       <NA>       <NA>       <NA>          NA          NA          NA          NA       NA        NA 0.003589778  6.6364296 ## 64       <NA>       <NA>       <NA>          NA          NA          NA          NA       NA        NA 0.028595474  3.9662215 ## 65       <NA>       <NA>       <NA>          NA          NA          NA          NA       NA        NA 0.018527702  4.3113683 ## 66       <NA>       <NA>       <NA>          NA          NA          NA          NA       NA        NA 0.013614761  4.1598683 ## 67       <NA>       <NA>       <NA>          NA          NA          NA          NA       NA        NA 0.019175079  7.1428770"},{"path":"https://teazrq.github.io/RLT/articles/Test-RLT-Reg.html","id":"linear-combination-kernel","dir":"Articles","previous_headings":"","what":"Linear combination kernel","title":"RLT Package: Regression With Reinforcement Learning and Linear Combination Splits","text":"linear combination split leads non-rectangular kernels.","code":"# generate data   set.seed(1)   n = 500; p = 5   X = matrix(runif(n*p), n, p)   y = X[, 1] + X[, 3] + 0.3*rnorm(n)    # fit model   RLTfit <- RLT(X, y, model = \"regression\",                 ntrees = 300, ncores = 10, nmin = 15, mtry = 5,                 split.gen = \"random\", nsplit = 3,                  resample.prob = 0.9, resample.replace = FALSE,                 param.control = list(\"embed.ntrees\" = 50,                                      \"linear.comb\" = 3,                                      \"embed.nmin\" = 10,                                      \"split.rule\" = \"naive\",                                      \"alpha\" = 0.25),                 verbose = TRUE) ## Regression Forest with Linear Combination Splits ...  ## ---------- Parameters Summary ---------- ##               (N, P) = (500, 5) ##           # of trees = 300 ##         (mtry, nmin) = (5, 15) ##       split generate = Random, 3 ##             sampling = 0.9 w/o replace ##   (Obs, Var) weights = (No, No) ##                alpha = 0.25 ##   linear combination = 3 ##           split rule = naive ##           importance = none ##        reinforcement = No ## ---------------------------------------- ## Do not have 10 cores, use maximum 4 cores.    # target point   newX = matrix(c(0.5, 0.5, 0.5, 0.5, 0.5),                  1, 5)      # get kernel weights defined by the kernel function   KernelW = forest.kernel(RLTfit, X1 = newX, X2 = X)$Kernel      par(mar = c(2, 2, 2, 2))   plot(X[, 1], X[, 3], col = \"deepskyblue\", pch = 19, cex = 0.5)   points(X[, 1], X[, 3], col = \"darkorange\",           cex = 10*sqrt(KernelW/sqrt(sum(KernelW^2))), lwd = 2)   points(newX[1], newX[3], col = \"black\", pch = 4, cex = 4, lwd = 5)   legend(\"topright\", \"Target Point\", pch = 4, col = \"black\",           lwd = 5, lty = NA, cex = 1.5)"},{"path":[]},{"path":"https://teazrq.github.io/RLT/articles/Test-RLT.html","id":"for-windows-users","dir":"Articles","previous_headings":"Install the RLT Package","what":"For Windows Users","title":"RLT Package Reinforcement Learning Test","text":"Windows, RLT can installed using install_github. compilation package requires GCC. Hence, may also need install Rtools .","code":"# you may need to install the devtools package first   # install.packages(\"devtools\")   library(devtools)    # install the RLT package from GitHub   # Rtools is required in Windows    install_github(\"teazrq/RLT\")"},{"path":"https://teazrq.github.io/RLT/articles/Test-RLT.html","id":"for-macos-users","dir":"Articles","previous_headings":"Install the RLT Package","what":"For macOS Users","title":"RLT Package Reinforcement Learning Test","text":"installing RLT macOS, process can slightly trickier. mainly consists two steps:","code":""},{"path":"https://teazrq.github.io/RLT/articles/Test-RLT.html","id":"step-1-install-compilers","dir":"Articles","previous_headings":"Install the RLT Package > For macOS Users","what":"Step 1: Install Compilers","title":"RLT Package Reinforcement Learning Test","text":"Follow guide install GNU Fortran compiler: gfortran-12.2-universal.pkg already Xcode (systems come Xcode installed already), can use line install :","code":"sudo xcode-select --install"},{"path":"https://teazrq.github.io/RLT/articles/Test-RLT.html","id":"step-2-set-makevars-to-point-to-the-compiler","dir":"Articles","previous_headings":"Install the RLT Package > For macOS Users","what":"Step 2: Set Makevars to Point to the Compiler","title":"RLT Package Reinforcement Learning Test","text":"Create Makevars file using: Check folder gfortran. compiler installed /opt/gfortran folder default. Add lines point correct folder compiler. may use open -TextEdit ~/.R/Makevars open text editor type lines: completing steps, able directly install RLT package using install_github(\"teazrq/RLT\") RStudio way Windows machine. compilation may require minutes.","code":"mkdir ~/.R touch ~/.R/Makevars FC = /opt/gfortran/bin/gfortran F77 = /opt/gfortran/bin/gfortran FLIBS = -L/opt/gfortran/lib"},{"path":"https://teazrq.github.io/RLT/articles/Test-RLT.html","id":"openmp-in-macos","dir":"Articles","previous_headings":"Install the RLT Package","what":"OpenMP in macOS","title":"RLT Package Reinforcement Learning Test","text":"previous steps activate OpenMP parallel computing. enable OpenMP compiling RLT package, follow steps:","code":""},{"path":"https://teazrq.github.io/RLT/articles/Test-RLT.html","id":"step-1-download-and-install-llvm","dir":"Articles","previous_headings":"Install the RLT Package > OpenMP in macOS","what":"Step 1: Download and Install LLVM","title":"RLT Package Reinforcement Learning Test","text":"example, install 14.0.6 version (check ones compatible Xcode version), use: see following message:","code":"curl -O https://mac.r-project.org/openmp/openmp-14.0.6-darwin20-Release.tar.gz sudo tar fvxz openmp-14.0.6-darwin20-Release.tar.gz -C / usr/local/lib/libomp.dylib usr/local/include/ompt.h usr/local/include/omp.h usr/local/include/omp-tools.h"},{"path":"https://teazrq.github.io/RLT/articles/Test-RLT.html","id":"step-2-add-flags-into-makevars","dir":"Articles","previous_headings":"Install the RLT Package > OpenMP in macOS","what":"Step 2: Add Flags into Makevars","title":"RLT Package Reinforcement Learning Test","text":"procedure explained previously. add two lines Makevars file: two steps, can use install_github(\"teazrq/RLT\", force = TRUE) re-install RLT package.","code":"CPPFLAGS += -Xclang -fopenmp LDFLAGS += -lomp"},{"path":"https://teazrq.github.io/RLT/articles/Test-RLT.html","id":"load-the-rlt-package","dir":"Articles","previous_headings":"Install the RLT Package","what":"Load the RLT Package","title":"RLT Package Reinforcement Learning Test","text":"","code":"library(RLT) ## RLT and Random Forests v4.2.6 ## pre-release at github.com/teazrq/RLT"},{"path":"https://teazrq.github.io/RLT/articles/Test-RLT.html","id":"benchmarking","dir":"Articles","previous_headings":"","what":"Benchmarking","title":"RLT Package Reinforcement Learning Test","text":"following code tests performance RLT package regression problem:","code":"library(parallel)   # Set seed for reproducibility   set.seed(1)    # Define data size   trainn <- 800   testn <- 1000   n <- trainn + testn   p <- 30    # Generate continuous variables (X1) and categorical variables (X2)   X1 <- matrix(rnorm(n * p / 2), n, p / 2)   X2 <- matrix(as.integer(runif(n * p / 2) * 3), n, p / 2)    # Combine continuous and categorical variables into a data frame (X)   X <- data.frame(X1, X2)    # Convert the second half of the columns in X to factors   X[, (p / 2 + 1):p] <- lapply(X[, (p / 2 + 1):p], as.factor)    # Generate outcomes (y)   y <- 1 + rowSums(X[, 2:6]) + 2 * (X[, p / 2 + 1] %in% c(1, 3)) + rnorm(n)    # Set tuning parameters   ntrees <- 1000   ncores <- detectCores() - 1 # Benchmark RLT   start_time <- Sys.time()      RLTfit <- RLT(X[1:trainn, ], y[1:trainn], model = \"regression\",                 ntrees = ntrees, ncores = ncores, nmin = 10,                 split.gen = \"random\", nsplit = 1,                 resample.prob = 0.85, resample.replace = FALSE,                 reinforcement = TRUE, importance = TRUE,                 param.control = list(\"embed.ntrees\" = 50,                                      \"embed.mtry\" = 1/2,                                      \"embed.nmin\" = 10,                                      \"embed.split.gen\" = \"random\",                                      \"embed.nsplit\" = 1,                                      \"embed.resample.replace\" = TRUE,                                      \"embed.resample.prob\" = 0.9,                                      \"embed.mute\" = 0.5,                                      \"embed.protect\" = 5),                 verbose = TRUE) ## Regression Random Forest ...  ## ---------- Parameters Summary ---------- ##               (N, P) = (800, 30) ##           # of trees = 1000 ##         (mtry, nmin) = (10, 10) ##       split generate = Random, 1 ##             sampling = 0.85 w/o replace ##   (Obs, Var) weights = (No, No) ##           importance = permute ##        reinforcement = Yes ## ---------------------------------------- ##  embed.ntrees            = 50 ##  embed.mtry              = 50% ##  embed.nmin              = 10 ##  embed.split.gen         = Random, 1 ##  embed.resample.replace  = TRUE ##  embed.resample.prob     = 0.9 ##  embed.mute              = 0.5 ##  embed.protect           = 5 ##  embed.threshold         = 0.25 ## ----------------------------------------      RLT_time <- difftime(Sys.time(), start_time, units = \"secs\")      # Make predictions   RLT_pred <- predict(RLTfit, X[(trainn + 1):n, ])   RLT_error <- mean((RLT_pred$Prediction - y[(trainn + 1):n])^2)      cat(\"RLT Results:\\n\") ## RLT Results:   cat(\"Training time:\", round(as.numeric(RLT_time), 2), \"seconds\\n\") ## Training time: 141.87 seconds   cat(\"Test MSE:\", round(RLT_error, 4), \"\\n\") ## Test MSE: 2.0946   cat(\"Variable importance (top 5):\\n\") ## Variable importance (top 5):   print(head(sort(RLTfit$VarImp, decreasing = TRUE), 5)) ## [1] 1.940552 1.594786 1.419061 1.205183 1.172316 # Plot variable importance   par(mar = c(5, 8, 4, 2))   barplot(sort(RLTfit$VarImp, decreasing = TRUE)[1:10],            horiz = TRUE, las = 1,            main = \"RLT Variable Importance (Top 10)\",           xlab = \"Importance Score\")"},{"path":"https://teazrq.github.io/RLT/articles/Test-RLT.html","id":"single-variable-embedded-splitting","dir":"Articles","previous_headings":"","what":"Single Variable Embedded Splitting","title":"RLT Package Reinforcement Learning Test","text":"Check seed match","code":"# set.seed(2)      n = 1000   p = 10   X = matrix(rnorm(n*p), n, p)   y = 1 + X[, 1] + X[, 3] + X[, 9] + rnorm(n)      testX = matrix(rnorm(n*p), n, p)   testy = 1 + testX[, 1] + testX[, 3] + testX[, 9] + rnorm(n)      start_time <- Sys.time()      RLTfit <- RLT(X, y, model = \"regression\",                  ntrees = 100, ncores = 1, nmin = 10,                 split.gen = \"random\", nsplit = 1,                 resample.prob = 0.85, resample.replace = FALSE,                 reinforcement = TRUE, importance = \"distribute\",                 param.control = list(\"embed.ntrees\" = 50,                                      \"embed.mtry\" = 1/2,                                      \"embed.nmin\" = 10,                                      \"embed.split.gen\" = \"random\",                                      \"embed.nsplit\" = 1,                                      \"embed.resample.replace\" = TRUE,                                      \"embed.resample.prob\" = 0.9,                                      \"embed.mute\" = 0.5,                                      \"embed.protect\" = 5),                 verbose = TRUE) ## Regression Random Forest ...  ## ---------- Parameters Summary ---------- ##               (N, P) = (1000, 10) ##           # of trees = 100 ##         (mtry, nmin) = (3, 10) ##       split generate = Random, 1 ##             sampling = 0.85 w/o replace ##   (Obs, Var) weights = (No, No) ##           importance = distribute ##        reinforcement = Yes ## ---------------------------------------- ##  embed.ntrees            = 50 ##  embed.mtry              = 50% ##  embed.nmin              = 10 ##  embed.split.gen         = Random, 1 ##  embed.resample.replace  = TRUE ##  embed.resample.prob     = 0.9 ##  embed.mute              = 0.5 ##  embed.protect           = 5 ##  embed.threshold         = 0.25 ## ----------------------------------------      difftime(Sys.time(), start_time, units = \"secs\") ## Time difference of 22.83125 secs        # oob error   mean((RLTfit$Prediction - y)^2, na.rm = TRUE) ## [1] 1.135828      # prediction error   pred = predict(RLTfit, testX)   mean((pred$Prediction - testy)^2) ## [1] 1.127407 # sparse variable importance   barplot(as.vector(RLTfit$VarImp), main = \"RLT\") # check one tree   get.one.tree(RLTfit, 1) ## Tree #1 in the fitted regression forest: ##     SplitVar   SplitValue LeftNode RightNode NodeWeight     NodeAve ## 1       V 1  -0.271622734        2         3        850  0.00000000 ## 2       V 3   1.374945871        4         5        336  0.00000000 ## 3       V 9  -0.727417719       90        91        514  0.00000000 ## 4       V 3   1.234704337        6         7        309  0.00000000 ## 5       V 9  -0.948021158       88        89         27  0.00000000 ## 6       V 9  -1.693387017        8         9        302  0.00000000 ## 7       <NA>           NA       NA        NA          7  0.71192780 ## 8       V 3  -0.883435226       10        11         14  0.00000000 ## 9       V 3  -0.089922191       12        13        288  0.00000000 ## 10      <NA>           NA       NA        NA          3 -4.26542670 ## 11      <NA>           NA       NA        NA         11 -1.57313849 ## 12      V 9   0.912644306       14        15        152  0.00000000 ## 13      V 9   1.249173258       54        55        136  0.00000000 ## 14      V 9  -0.097220388       16        17        125  0.00000000 ## 15      V 1  -0.510435508       46        47         27  0.00000000 ## 16      V 3  -0.754646362       18        19         64  0.00000000 ## 17      V 1  -1.872329678       36        37         61  0.00000000 ## 18      V 3  -2.541912118       20        21         38  0.00000000 ## 19      V 1  -0.409637646       32        33         26  0.00000000 ## 20      <NA>           NA       NA        NA          2 -3.47944364 ## 21      V 3  -1.127445885       22        23         36  0.00000000 ## 22      V 3  -2.151212576       24        25         21  0.00000000 ## 23      V 9  -1.021296406       30        31         15  0.00000000 ## 24      <NA>           NA       NA        NA          3 -3.27609845 ## 25      V 3  -1.200141685       26        27         18  0.00000000 ## 26      V 9  -1.125341544       28        29         15  0.00000000 ## 27      <NA>           NA       NA        NA          3 -1.19882299 ## 28      <NA>           NA       NA        NA          3 -3.12106506 ## 29      <NA>           NA       NA        NA         12 -2.35400875 ## 30      <NA>           NA       NA        NA          6 -2.97086231 ## 31      <NA>           NA       NA        NA          9 -0.73618046 ## 32      V 1  -1.080571631       34        35         23  0.00000000 ## 33      <NA>           NA       NA        NA          3 -1.48260215 ## 34      <NA>           NA       NA        NA         11 -1.91594192 ## 35      <NA>           NA       NA        NA         12 -0.72815582 ## 36      <NA>           NA       NA        NA          4 -2.72532811 ## 37      V 1  -0.605717476       38        39         57  0.00000000 ## 38      V 1  -1.529468330       40        41         42  0.00000000 ## 39      V 1  -0.432920316       44        45         15  0.00000000 ## 40      <NA>           NA       NA        NA          8 -1.02858638 ## 41      V 3  -1.426741255       42        43         34  0.00000000 ## 42      <NA>           NA       NA        NA          7 -1.03788729 ## 43      <NA>           NA       NA        NA         27 -0.23195290 ## 44      <NA>           NA       NA        NA         11 -0.05656630 ## 45      <NA>           NA       NA        NA          4  1.35331932 ## 46      V 1  -0.538605815       48        49         21  0.00000000 ## 47      <NA>           NA       NA        NA          6  1.77815785 ## 48      V 1  -0.558162770       50        51         20  0.00000000 ## 49      <NA>           NA       NA        NA          1 -0.44745212 ## 50      V 1  -0.680710723       52        53         18  0.00000000 ## 51      <NA>           NA       NA        NA          2  0.68823768 ## 52      <NA>           NA       NA        NA         12 -0.04243928 ## 53      <NA>           NA       NA        NA          6  0.47270611 ## 54      V 9   0.799874127       56        57        115  0.00000000 ## 55      V 1  -1.046523344       86        87         21  0.00000000 ## 56      V 9  -0.375221396       58        59        100  0.00000000 ## 57      V 3   0.253907444       84        85         15  0.00000000 ## 58      V 1  -0.705645768       60        61         42  0.00000000 ## 59      V 1  -1.020032372       68        69         58  0.00000000 ## 60      V 1  -1.731467755       62        63         21  0.00000000 ## 61      V 3   0.761169870       66        67         21  0.00000000 ## 62      <NA>           NA       NA        NA          3 -1.25252916 ## 63      V 1  -0.873759411       64        65         18  0.00000000 ## 64      <NA>           NA       NA        NA         12 -0.66336370 ## 65      <NA>           NA       NA        NA          6 -0.07782485 ## 66      <NA>           NA       NA        NA         17  0.03622966 ## 67      <NA>           NA       NA        NA          4  0.97543688 ## 68      V 1  -1.741806080       70        71         25  0.00000000 ## 69      V 1  -1.005773900       74        75         33  0.00000000 ## 70      <NA>           NA       NA        NA          8 -0.66962756 ## 71      V 9   0.187767144       72        73         17  0.00000000 ## 72      <NA>           NA       NA        NA          9  0.14252798 ## 73      <NA>           NA       NA        NA          8  1.16742055 ## 74      <NA>           NA       NA        NA          1 -1.66153363 ## 75      V 1  -0.729474746       76        77         32  0.00000000 ## 76      V 9   0.044841245       78        79         13  0.00000000 ## 77      V 3   0.095847049       80        81         19  0.00000000 ## 78      <NA>           NA       NA        NA          9  0.95727738 ## 79      <NA>           NA       NA        NA          4  1.15035663 ## 80      <NA>           NA       NA        NA          5  0.75039618 ## 81      V 3   0.945358635       82        83         14  0.00000000 ## 82      <NA>           NA       NA        NA         11  1.33935375 ## 83      <NA>           NA       NA        NA          3  3.01374415 ## 84      <NA>           NA       NA        NA          7  0.79048424 ## 85      <NA>           NA       NA        NA          8  2.26542007 ## 86      <NA>           NA       NA        NA         12  1.46900280 ## 87      <NA>           NA       NA        NA          9  2.09600159 ## 88      <NA>           NA       NA        NA          7  0.12661862 ## 89      <NA>           NA       NA        NA         20  2.78737361 ## 90      V 3   1.267165218       92        93        126  0.00000000 ## 91      V 3  -0.330760953      124       125        388  0.00000000 ## 92      V 1   0.405914400       94        95        119  0.00000000 ## 93      <NA>           NA       NA        NA          7  2.38802331 ## 94      V 3   0.747528070       96        97         54  0.00000000 ## 95      V 3  -0.359616043      108       109         65  0.00000000 ## 96      V 3   0.027194774       98        99         48  0.00000000 ## 97      <NA>           NA       NA        NA          6  0.72838854 ## 98      V 9  -1.107816947      100       101         38  0.00000000 ## 99      <NA>           NA       NA        NA         10 -0.27799643 ## 100     V 9  -2.447337622      102       103         24  0.00000000 ## 101     V 3  -1.328138262      106       107         14  0.00000000 ## 102     <NA>           NA       NA        NA          2 -2.37760595 ## 103     V 3  -1.376264711      104       105         22  0.00000000 ## 104     <NA>           NA       NA        NA          4 -1.53795574 ## 105     <NA>           NA       NA        NA         18 -0.78354380 ## 106     <NA>           NA       NA        NA          2 -1.56711502 ## 107     <NA>           NA       NA        NA         12 -0.40202238 ## 108     V 9  -1.901469017      110       111         25  0.00000000 ## 109     V 1   1.013605229      114       115         40  0.00000000 ## 110     <NA>           NA       NA        NA          5 -1.59273315 ## 111     V 3  -0.570041103      112       113         20  0.00000000 ## 112     <NA>           NA       NA        NA         14 -0.29737049 ## 113     <NA>           NA       NA        NA          6  1.16632057 ## 114     V 9  -2.306502866      116       117         16  0.00000000 ## 115     V 9  -0.995887076      120       121         24  0.00000000 ## 116     <NA>           NA       NA        NA          1 -0.35776804 ## 117     V 9  -1.139062166      118       119         15  0.00000000 ## 118     <NA>           NA       NA        NA          7  0.44933019 ## 119     <NA>           NA       NA        NA          8  1.09751244 ## 120     V 3   0.141065954      122       123         16  0.00000000 ## 121     <NA>           NA       NA        NA          8  2.86883252 ## 122     <NA>           NA       NA        NA          9  0.65278175 ## 123     <NA>           NA       NA        NA          7  1.69992724 ## 124     V 9  -0.691983530      126       127        132  0.00000000 ## 125     V 9  -0.610407411      164       165        256  0.00000000 ## 126     <NA>           NA       NA        NA          1 -1.77870752 ## 127     V 9   1.622401036      128       129        131  0.00000000 ## 128     V 1   0.316377929      130       131        121  0.00000000 ## 129     <NA>           NA       NA        NA         10  3.20109493 ## 130     V 3  -0.426647066      132       133         45  0.00000000 ## 131     V 9  -0.009049132      144       145         76  0.00000000 ## 132     V 3  -0.944307385      134       135         43  0.00000000 ## 133     <NA>           NA       NA        NA          2  1.58596402 ## 134     V 3  -1.120979231      136       137         21  0.00000000 ## 135     V 9  -0.185182700      140       141         22  0.00000000 ## 136     V 3  -1.449634214      138       139         16  0.00000000 ## 137     <NA>           NA       NA        NA          5  0.52970559 ## 138     <NA>           NA       NA        NA          8 -1.76438808 ## 139     <NA>           NA       NA        NA          8 -0.11908409 ## 140     <NA>           NA       NA        NA          6  0.01946929 ## 141     V 9   0.648977291      142       143         16  0.00000000 ## 142     <NA>           NA       NA        NA          9  0.47790599 ## 143     <NA>           NA       NA        NA          7  1.52264332 ## 144     V 3  -1.470568801      146       147         29  0.00000000 ## 145     V 1   0.707096946      152       153         47  0.00000000 ## 146     <NA>           NA       NA        NA          9 -0.55405067 ## 147     V 1   0.564986642      148       149         20  0.00000000 ## 148     <NA>           NA       NA        NA          6  0.14311401 ## 149     V 9  -0.268171284      150       151         14  0.00000000 ## 150     <NA>           NA       NA        NA         11  1.38670442 ## 151     <NA>           NA       NA        NA          3 -0.01070608 ## 152     V 9   0.532634901      154       155         15  0.00000000 ## 153     V 1   0.926551310      156       157         32  0.00000000 ## 154     <NA>           NA       NA        NA          8  0.90469300 ## 155     <NA>           NA       NA        NA          7  1.93628824 ## 156     <NA>           NA       NA        NA          7  1.07095999 ## 157     V 1   1.173078632      158       159         25  0.00000000 ## 158     <NA>           NA       NA        NA          7  1.29151153 ## 159     V 1   1.487007326      160       161         18  0.00000000 ## 160     <NA>           NA       NA        NA          5  2.36713456 ## 161     V 1   2.158366473      162       163         13  0.00000000 ## 162     <NA>           NA       NA        NA         12  2.58789063 ## 163     <NA>           NA       NA        NA          1  3.33842500 ## 164     <NA>           NA       NA        NA         12  1.25050247 ## 165     V 9  -0.435209955      166       167        244  0.00000000 ## 166     V 1   0.559515085      168       169         21  0.00000000 ## 167     V 1  -0.255512201      170       171        223  0.00000000 ## 168     <NA>           NA       NA        NA          9  0.38547909 ## 169     <NA>           NA       NA        NA         12  2.45272840 ## 170     <NA>           NA       NA        NA          3  2.13500092 ## 171     V 3  -0.180156580      172       173        220  0.00000000 ## 172     V 9   0.697650062      174       175         20  0.00000000 ## 173     V 1   0.113212358      176       177        200  0.00000000 ## 174     <NA>           NA       NA        NA         16  1.69905716 ## 175     <NA>           NA       NA        NA          4  3.43957073 ## 176     V 9   0.326819991      178       179         49  0.00000000 ## 177     V 3   0.760558440      192       193        151  0.00000000 ## 178     V 3   0.523460727      180       181         22  0.00000000 ## 179     V 3   0.054738808      184       185         27  0.00000000 ## 180     <NA>           NA       NA        NA          5  1.46637156 ## 181     V 1  -0.042233537      182       183         17  0.00000000 ## 182     <NA>           NA       NA        NA          7  2.45547937 ## 183     <NA>           NA       NA        NA         10  1.90938888 ## 184     <NA>           NA       NA        NA          3  2.81635615 ## 185     V 3   0.428669325      186       187         24  0.00000000 ## 186     <NA>           NA       NA        NA          7  2.27998056 ## 187     V 3   1.143515760      188       189         17  0.00000000 ## 188     V 9   0.511508204      190       191         14  0.00000000 ## 189     <NA>           NA       NA        NA          3  4.10278294 ## 190     <NA>           NA       NA        NA          2  1.68255475 ## 191     <NA>           NA       NA        NA         12  2.87219379 ## 192     V 9  -0.007922848      194       195         97  0.00000000 ## 193     V 9  -0.135311880      220       221         54  0.00000000 ## 194     V 1   0.682559821      196       197         19  0.00000000 ## 195     V 1   0.488762966      198       199         78  0.00000000 ## 196     <NA>           NA       NA        NA          7  1.24641637 ## 197     <NA>           NA       NA        NA         12  2.70591957 ## 198     V 3   0.681793763      200       201         27  0.00000000 ## 199     V 9   0.138165461      208       209         51  0.00000000 ## 200     V 3  -0.042053145      202       203         26  0.00000000 ## 201     <NA>           NA       NA        NA          1  4.29893352 ## 202     <NA>           NA       NA        NA          4  2.73221045 ## 203     V 3   0.137527511      204       205         22  0.00000000 ## 204     <NA>           NA       NA        NA          6  1.84536639 ## 205     V 3   0.464864082      206       207         16  0.00000000 ## 206     <NA>           NA       NA        NA         11  2.41765627 ## 207     <NA>           NA       NA        NA          5  3.13310186 ## 208     <NA>           NA       NA        NA          9  2.58187150 ## 209     V 1   1.270314868      210       211         42  0.00000000 ## 210     V 9   1.164587398      212       213         31  0.00000000 ## 211     <NA>           NA       NA        NA         11  4.21605610 ## 212     V 3  -0.131521293      214       215         25  0.00000000 ## 213     <NA>           NA       NA        NA          6  3.90452572 ## 214     <NA>           NA       NA        NA          1  1.86358877 ## 215     V 3   0.756225023      216       217         24  0.00000000 ## 216     V 3   0.275884173      218       219         23  0.00000000 ## 217     <NA>           NA       NA        NA          1  3.38284974 ## 218     <NA>           NA       NA        NA         12  2.76257479 ## 219     <NA>           NA       NA        NA         11  3.09088634 ## 220     V 9  -0.166624080      222       223         13  0.00000000 ## 221     V 3   1.327673831      224       225         41  0.00000000 ## 222     <NA>           NA       NA        NA          9  3.33907208 ## 223     <NA>           NA       NA        NA          4  3.67201726 ## 224     V 9   0.016279962      226       227         23  0.00000000 ## 225     V 9   0.770508072      230       231         18  0.00000000 ## 226     <NA>           NA       NA        NA          4  3.37756123 ## 227     V 9   0.544482457      228       229         19  0.00000000 ## 228     <NA>           NA       NA        NA          8  2.81274534 ## 229     <NA>           NA       NA        NA         11  4.16942669 ## 230     <NA>           NA       NA        NA          9  4.34731857 ## 231     <NA>           NA       NA        NA          9  4.99769295 RLTfit2 <- RLT(X, y, model = \"regression\",                   ntrees = 20, ncores = 1, nmin = 10,                  split.gen = \"random\", nsplit = 1,                  resample.prob = 0.85, resample.replace = FALSE,                  reinforcement = TRUE, importance = TRUE,                  param.control = list(\"embed.ntrees\" = 50,                                       \"embed.mtry\" = 1/3,                                       \"embed.nmin\" = 10,                                       \"embed.split.gen\" = \"random\",                                       \"embed.nsplit\" = 1,                                       \"embed.resample.prob\" = 0.75,                                       \"embed.mute\" = 0.5,                                       \"embed.protect\" = 5),                  verbose = TRUE, seed = RLTfit$parameters$seed) ## Regression Random Forest ...  ## ---------- Parameters Summary ---------- ##               (N, P) = (1000, 10) ##           # of trees = 20 ##         (mtry, nmin) = (3, 10) ##       split generate = Random, 1 ##             sampling = 0.85 w/o replace ##   (Obs, Var) weights = (No, No) ##           importance = permute ##        reinforcement = Yes ## ---------------------------------------- ##  embed.ntrees            = 50 ##  embed.mtry              = 33.3% ##  embed.nmin              = 10 ##  embed.split.gen         = Random, 1 ##  embed.resample.replace  = TRUE ##  embed.resample.prob     = 0.75 ##  embed.mute              = 0.5 ##  embed.protect           = 5 ##  embed.threshold         = 0.25 ## ----------------------------------------    all(RLTfit$VarImp == RLTfit2$VarImp) ## [1] FALSE"},{"path":"https://teazrq.github.io/RLT/articles/Test-RLT.html","id":"linear-combination-split","dir":"Articles","previous_headings":"","what":"Linear Combination Split","title":"RLT Package Reinforcement Learning Test","text":"model without fitting embedded random forest search linear combinations. variables ranked marginal screening.","code":"# set.seed(1)   library(MASS)   n = 1000   p = 5   S = matrix(0.3, p, p)   diag(S) = 1   S[1, 5] = S[5, 1] = 0.9   S[1, 3] = S[3, 1] = S[5, 3] = S[3, 5] = -0.3      X1 = mvrnorm(n, mu = rep(0, p), Sigma = S)   X2 = as.factor(sample(c(-1,1), size = n, replace = TRUE))   X = data.frame(X1, X2)   y = 1 + 2*(X[, 6] == 1) + X[, 1] + X[, 3] + rnorm(n)   w = runif(n)      start_time <- Sys.time()      RLTfit <- RLT(X, y, model = \"regression\", obs.w = w,                 ntrees = 100, ncores = 1, nmin = 50, mtry = 3,                 split.gen = \"random\", nsplit = 3,                  resample.prob = 0.8, resample.replace = FALSE,                  split.rule = \"sir\",                 param.control = list(\"linear.comb\" = 3), #ZRL                 importance = TRUE,                  verbose = TRUE) ## Regression Forest with Linear Combination Splits ...  ## ---------- Parameters Summary ---------- ##               (N, P) = (1000, 6) ##           # of trees = 100 ##         (mtry, nmin) = (3, 50) ##       split generate = Random, 3 ##             sampling = 0.8 w/o replace ##   (Obs, Var) weights = (Yes, No) ##   linear combination = 3 ##           split rule = sir ##           importance = permute ##        reinforcement = No ## ----------------------------------------      difftime(Sys.time(), start_time, units = \"secs\") ## Time difference of 45.19931 secs      plot(RLTfit$Prediction, y) mean( (RLTfit$Prediction - y)^2 , na.rm = TRUE) ## [1] 1.132771"},{"path":"https://teazrq.github.io/RLT/articles/Test-Reg.html","id":"install-and-load-package","dir":"Articles","previous_headings":"","what":"Install and Load Package","title":"RLT Package Testing Regression Functions and Features","text":"Install load GitHub version RLT package. use CRAN version. Load packages used guide.","code":"# install.packages(\"devtools\")   # devtools::install_github(\"teazrq/RLT\")   library(RLT) ## RLT and Random Forests v4.2.6 ## pre-release at github.com/teazrq/RLT library(randomForest)   library(randomForestSRC)   library(ranger)"},{"path":"https://teazrq.github.io/RLT/articles/Test-Reg.html","id":"benchmark-against-existing-packages","dir":"Articles","previous_headings":"","what":"Benchmark Against Existing Packages","title":"RLT Package Testing Regression Functions and Features","text":"generate dataset 1000 observations 400 variables, 200 continuous variables 200 categorical ones three categories.","code":"library(parallel)   # Set seed for reproducibility   set.seed(1)    # Define data size   trainn <- 800   testn <- 1000   n <- trainn + testn   p <- 30    # Generate continuous variables (X1) and categorical variables (X2)   X1 <- matrix(rnorm(n * p / 2), n, p / 2)   X2 <- matrix(as.integer(runif(n * p / 2) * 3), n, p / 2)    # Combine continuous and categorical variables into a data frame (X)   X <- data.frame(X1, X2)    # Convert the second half of the columns in X to factors   X[, (p / 2 + 1):p] <- lapply(X[, (p / 2 + 1):p], as.factor)    # Generate outcomes (y)   y <- 1 + rowSums(X[, 2:6]) + 2 * (X[, p / 2 + 1] %in% c(1, 3)) + rnorm(n)    # Set tuning parameters   ntrees <- 1000   ncores <- detectCores() - 1   nmin <- 30   mtry <- p / 2   samplereplace <- TRUE   sampleprob <- 0.80   rule <- \"best\"   nsplit <- ifelse(rule == \"best\", 0, 3)   importance <- TRUE    # Split data into training and testing sets   trainX <- X[1:trainn, ]   trainY <- y[1:trainn]   testX <- X[(trainn + 1):(trainn + testn), ]   testY <- y[(trainn + 1):(trainn + testn)] # recording results   metric = data.frame(matrix(NA, 4, 6))   rownames(metric) = c(\"RLT\", \"randomForestSRC\", \"randomForest\", \"ranger\")   colnames(metric) = c(\"fit.time\", \"pred.time\", \"oob.error\", \"pred.error\",                        \"obj.size\", \"tree.size\")      # using RLT package    start_time <- Sys.time()   RLTfit <- RLT(trainX, trainY, model = \"regression\",                  ntrees = ntrees, mtry = mtry, nmin = nmin,                  resample.prob = sampleprob, split.gen = rule,                  resample.replace = samplereplace,                  nsplit = nsplit, importance = importance,                  param.control = list(\"alpha\" = 0),                 ncores = ncores, verbose = TRUE) ## Regression Random Forest ...  ## ---------- Parameters Summary ---------- ##               (N, P) = (800, 30) ##           # of trees = 1000 ##         (mtry, nmin) = (15, 30) ##       split generate = Best ##             sampling = 0.8 w/ replace ##   (Obs, Var) weights = (No, No) ##           importance = permute ##        reinforcement = No ## ----------------------------------------   metric[1, 1] = difftime(Sys.time(), start_time, units = \"secs\")   start_time <- Sys.time()   RLTPred <- predict(RLTfit, testX, ncores = ncores)   metric[1, 2] = difftime(Sys.time(), start_time, units = \"secs\")   metric[1, 3] = mean((RLTfit$Prediction - trainY)^2)   metric[1, 4] = mean((RLTPred$Prediction - testY)^2)   metric[1, 5] = object.size(RLTfit)   metric[1, 6] = mean(unlist(lapply(RLTfit$FittedForest$SplitVar, length)))      # use randomForestSRC   options(rf.cores = ncores)   start_time <- Sys.time()   rsffit <- rfsrc(y ~ ., data = data.frame(trainX, \"y\"= trainY),                    ntree = ntrees, nodesize = nmin/2, mtry = mtry,                    samptype = ifelse(samplereplace == TRUE, \"swor\", \"swr\"),                   nsplit = nsplit, sampsize = trainn*sampleprob,                    importance = ifelse(importance, \"permute\", \"none\"))   metric[2, 1] = difftime(Sys.time(), start_time, units = \"secs\")   start_time <- Sys.time()   rsfpred = predict(rsffit, data.frame(testX))   metric[2, 2] = difftime(Sys.time(), start_time, units = \"secs\")   metric[2, 3] = mean((rsffit$predicted.oob - trainY)^2)   metric[2, 4] = mean((rsfpred$predicted - testY)^2)   metric[2, 5] = object.size(rsffit)   metric[2, 6] = rsffit$forest$totalNodeCount / rsffit$ntree      # use randomForest   start_time <- Sys.time()   rf.fit <- randomForest(trainX, trainY, ntree = ntrees,                           mtry = mtry, nodesize = nmin,                           replace = samplereplace,                          sampsize = trainn*sampleprob,                           importance = importance)   metric[3, 1] = difftime(Sys.time(), start_time, units = \"secs\")   start_time <- Sys.time()   rf.pred <- predict(rf.fit, testX)   metric[3, 2] = difftime(Sys.time(), start_time, units = \"secs\")   metric[3, 3] = mean((rf.fit$predicted - trainY)^2)   metric[3, 4] = mean((rf.pred - testY)^2)   metric[3, 5] = object.size(rf.fit)   metric[3, 6] = mean(colSums(rf.fit$forest$nodestatus != 0))         # use ranger     start_time <- Sys.time()   rangerfit <- ranger(trainY ~ ., data = data.frame(trainX),                        num.trees = ntrees, min.node.size = nmin,                        mtry = mtry, num.threads = ncores,                        replace = samplereplace,                       sample.fraction = sampleprob,                        importance = \"permutation\",                       respect.unordered.factors = \"partition\")   metric[4, 1] = difftime(Sys.time(), start_time, units = \"secs\")   start_time <- Sys.time()   rangerpred = predict(rangerfit, data.frame(testX))   metric[4, 2] = difftime(Sys.time(), start_time, units = \"secs\")   metric[4, 3] = mean((rangerfit$predictions - trainY)^2)   metric[4, 4] = mean((rangerpred$predictions - testY)^2)   metric[4, 5] = object.size(rangerfit)   metric[4, 6] = mean(unlist(lapply(rangerfit$forest$split.varIDs, length)))      # performance summary   metric ##                  fit.time  pred.time oob.error pred.error obj.size tree.size ## RLT             0.7899787 0.04829931  2.326833   2.348519  4855800    79.080 ## randomForestSRC 2.3519471 0.15868258  2.329721   2.417986 13419416    81.296 ## randomForest    3.5314641 0.05414748  2.327304   2.372427  3212440    79.312 ## ranger          1.3993149 0.07059932  2.318683   2.358706  2842856    79.352"},{"path":"https://teazrq.github.io/RLT/articles/Test-Reg.html","id":"print-a-single-tree","dir":"Articles","previous_headings":"","what":"Print a Single Tree","title":"RLT Package Testing Regression Functions and Features","text":"can use get.one.tree() function peek single tree.","code":"get.one.tree(RLTfit, 1) ## Tree #1 in the fitted regression forest: ##     SplitVar SplitValue LeftNode RightNode NodeWeight     NodeAve ## 1        X2   0.5130879        2         3        640  0.00000000 ## 2        X5   0.1245613        4         5        461  0.00000000 ## 3        X4   0.1278372       68        69        179  0.00000000 ## 4        X6  -0.9315267        6         7        270  0.00000000 ## 5        X3  -0.3566060       36        37        191  0.00000000 ## 6        X3  -0.6818253        8         9         52  0.00000000 ## 7        X2  -1.1202194       12        13        218  0.00000000 ## 8       <NA>         NA       NA        NA         20 -4.05831582 ## 9       X13   0.6969620       10        11         32  0.00000000 ## 10      <NA>         NA       NA        NA         25 -1.56610269 ## 11      <NA>         NA       NA        NA          7  0.33467263 ## 12      X11   0.8086734       14        15         49  0.00000000 ## 13       X5  -1.9740546       18        19        169  0.00000000 ## 14       X4  -1.2378102       16        17         32  0.00000000 ## 15      <NA>         NA       NA        NA         17  0.83807065 ## 16      <NA>         NA       NA        NA          4 -5.07730369 ## 17      <NA>         NA       NA        NA         28 -1.16601538 ## 18      <NA>         NA       NA        NA         15 -1.75654440 ## 19  X1.1 (F)  4.0000000       20        21        154  0.00000000 ## 20       X6   0.2068412       22        23         94  0.00000000 ## 21       X5  -0.8397821       30        31         60  0.00000000 ## 22       X3  -0.2591860       24        25         54  0.00000000 ## 23       X7  -0.9688096       28        29         40  0.00000000 ## 24      <NA>         NA       NA        NA         17 -1.33291805 ## 25 X14.1 (F)  4.0000000       26        27         37  0.00000000 ## 26      <NA>         NA       NA        NA         18 -0.43713880 ## 27      <NA>         NA       NA        NA         19  0.94011843 ## 28      <NA>         NA       NA        NA         17  2.53305108 ## 29      <NA>         NA       NA        NA         23  0.66413456 ## 30      <NA>         NA       NA        NA         22  0.85873903 ## 31      X12  -1.8288810       32        33         38  0.00000000 ## 32      <NA>         NA       NA        NA          3  4.32370542 ## 33      X13   0.7978382       34        35         35  0.00000000 ## 34      <NA>         NA       NA        NA         26  2.58142104 ## 35      <NA>         NA       NA        NA          9  1.49224825 ## 36       X3  -2.0974592       38        39         70  0.00000000 ## 37       X4  -0.1241238       52        53        121  0.00000000 ## 38      <NA>         NA       NA        NA          4 -2.32649240 ## 39       X6  -0.7842442       40        41         66  0.00000000 ## 40      <NA>         NA       NA        NA          9 -1.03000167 ## 41       X6  -0.7207343       42        43         57  0.00000000 ## 42      <NA>         NA       NA        NA          3  4.06230418 ## 43      X12  -0.3962172       44        45         54  0.00000000 ## 44      <NA>         NA       NA        NA         13  2.51658857 ## 45       X3  -0.4572172       46        47         41  0.00000000 ## 46       X4   0.4462492       48        49         39  0.00000000 ## 47      <NA>         NA       NA        NA          2  3.21089666 ## 48      X13  -0.9595361       50        51         32  0.00000000 ## 49      <NA>         NA       NA        NA          7  1.74014623 ## 50      <NA>         NA       NA        NA          4  2.01719301 ## 51      <NA>         NA       NA        NA         28  0.43829619 ## 52       X3   1.5642136       54        55         65  0.00000000 ## 53       X6  -1.1783072       64        65         56  0.00000000 ## 54       X9  -1.2829842       56        57         59  0.00000000 ## 55      <NA>         NA       NA        NA          6  4.35667510 ## 56      <NA>         NA       NA        NA          6 -0.81379470 ## 57       X5   2.4673092       58        59         53  0.00000000 ## 58       X4  -0.4766599       60        61         52  0.00000000 ## 59      <NA>         NA       NA        NA          1  6.50481360 ## 60      X14  -1.0536246       62        63         35  0.00000000 ## 61      <NA>         NA       NA        NA         17  2.30415578 ## 62      <NA>         NA       NA        NA          6  2.05429613 ## 63      <NA>         NA       NA        NA         29  0.80208708 ## 64      <NA>         NA       NA        NA          6  1.02489544 ## 65       X3   0.3655888       66        67         50  0.00000000 ## 66      <NA>         NA       NA        NA         23  3.17356164 ## 67      <NA>         NA       NA        NA         27  5.19880987 ## 68  X1.1 (F)  4.0000000       70        71        104  0.00000000 ## 69      X15   1.6156622       84        85         75  0.00000000 ## 70       X4  -2.5436361       72        73         71  0.00000000 ## 71       X3  -0.1189881       82        83         33  0.00000000 ## 72      <NA>         NA       NA        NA          1 -5.09138993 ## 73       X3   0.4572242       74        75         70  0.00000000 ## 74      X10  -1.4332881       76        77         50  0.00000000 ## 75      <NA>         NA       NA        NA         20  2.67380588 ## 76      <NA>         NA       NA        NA          6 -0.77604144 ## 77      X14   1.7117741       78        79         44  0.00000000 ## 78      X14   0.7914825       80        81         39  0.00000000 ## 79      <NA>         NA       NA        NA          5  3.02254894 ## 80      <NA>         NA       NA        NA         29  1.56733301 ## 81      <NA>         NA       NA        NA         10  0.07304804 ## 82      <NA>         NA       NA        NA         12  2.58827777 ## 83      <NA>         NA       NA        NA         21  4.89771881 ## 84       X6   0.6855359       86        87         73  0.00000000 ## 85      <NA>         NA       NA        NA          2  0.41755174 ## 86       X7   0.9193260       88        89         54  0.00000000 ## 87      <NA>         NA       NA        NA         19  6.38522344 ## 88       X5   0.7407007       90        91         42  0.00000000 ## 89      <NA>         NA       NA        NA         12  6.16322549 ## 90      <NA>         NA       NA        NA         30  3.36388822 ## 91      <NA>         NA       NA        NA         12  5.19780288"},{"path":"https://teazrq.github.io/RLT/articles/Test-Reg.html","id":"random-forest-kernel","dir":"Articles","previous_headings":"","what":"Random Forest Kernel","title":"RLT Package Testing Regression Functions and Features","text":"Let’s generate new dataset 5 continuous variables. true model depends just first two variables.","code":"# generate data   n = 1000; p = 5   X = matrix(runif(n*p), n, p)   y = X[, 1] + X[, 2] + rnorm(n)    # fit model   RLTfit <- RLT(X, y, model = \"regression\",                  ntrees = 50, nmin = 4, mtry = 5,                 split.gen = \"best\", resample.prob = 0.8,                 resample.replace = FALSE,                 importance = TRUE, param.control = list(\"resample.track\" = TRUE))    par(mfrow=c(2, 2))    # target point   newX = matrix(c(0.5, 0.3, 0.5, 0.5, 0.5),                  1, 5)      # get kernel weights defined by the kernel function   KernelW = forest.kernel(RLTfit, X1 = newX, X2 = X)$Kernel      par(mar = c(2, 2, 2, 2))   plot(X[, 1], X[, 2], col = \"deepskyblue\", pch = 19, cex = 0.5)   points(X[, 1], X[, 2], col = \"darkorange\", cex = 10*sqrt(KernelW/sqrt(sum(KernelW^2))), lwd = 2)   points(newX[1], newX[2], col = \"black\", pch = 4, cex = 4, lwd = 5)   legend(\"bottomright\", \"Target Point\", pch = 4, col = \"black\",           lwd = 5, lty = NA, cex = 1.5)        # check against X3   plot(X[, 1], X[, 3], col = \"deepskyblue\", pch = 19, cex = 0.5)   points(X[, 1], X[, 3], col = \"darkorange\", cex = 10*sqrt(KernelW/sqrt(sum(KernelW^2))), lwd = 2)   points(newX[1], newX[3], col = \"black\", pch = 4, cex = 4, lwd = 5)   legend(\"bottomright\", \"Target Point\", pch = 4, col = \"black\",           lwd = 5, lty = NA, cex = 1.5)        # get kernel weights in the original forest   # this is slightly different since the original samples may or may not appear in each tree   KernelW = forest.kernel(RLTfit, X1 = newX, X2 = X, vs.train = TRUE)$Kernel      par(mar = c(2, 2, 2, 2))   plot(X[, 1], X[, 2], col = \"deepskyblue\", pch = 19, cex = 0.5)   points(X[, 1], X[, 2], col = \"darkorange\", cex = 10*sqrt(KernelW/sqrt(sum(KernelW^2))), lwd = 2)   points(newX[1], newX[2], col = \"black\", pch = 4, cex = 4, lwd = 5)   legend(\"bottomright\", \"Target Point\", pch = 4, col = \"black\",           lwd = 5, lty = NA, cex = 1.5)        # check against X3   plot(X[, 1], X[, 3], col = \"deepskyblue\", pch = 19, cex = 0.5)   points(X[, 1], X[, 3], col = \"darkorange\", cex = 10*sqrt(KernelW/sqrt(sum(KernelW^2))), lwd = 2)   points(newX[1], newX[3], col = \"black\", pch = 4, cex = 4, lwd = 5)   legend(\"bottomright\", \"Target Point\", pch = 4, col = \"black\",           lwd = 5, lty = NA, cex = 1.5)"},{"path":"https://teazrq.github.io/RLT/articles/Test-Reg.html","id":"setting-random-seed","dir":"Articles","previous_headings":"","what":"Setting random seed","title":"RLT Package Testing Regression Functions and Features","text":"","code":"## Fitting a forest   RLTfit1 <- RLT(trainX, trainY, model = \"regression\",                   ntrees = 100, importance = TRUE, nmin = 1)    RLTfit2 <- RLT(trainX, trainY, model = \"regression\",                   ntrees = 100, importance = TRUE, nmin = 1,                  seed = RLTfit1$parameters$seed)      # check if importance are identical   all(RLTfit1$VarImp == RLTfit2$VarImp) ## [1] TRUE      # prediction   RLTPred1 <- predict(RLTfit1, testX, keep.all = TRUE)   RLTPred2 <- predict(RLTfit2, testX, keep.all = TRUE)    # check predictions are identical   all(RLTPred1$Prediction == RLTPred2$Prediction) ## [1] TRUE"},{"path":"https://teazrq.github.io/RLT/articles/Test-Surv-Band.html","id":"install-and-load-package","dir":"Articles","previous_headings":"","what":"Install and Load Package","title":"RLT Package Testing Survival Functions and Features","text":"Install load GitHub version RLT package. use CRAN version.","code":"# install.packages(\"devtools\")   # devtools::install_github(\"teazrq/RLT\")   library(RLT) ## RLT and Random Forests v4.2.6 ## pre-release at github.com/teazrq/RLT"},{"path":"https://teazrq.github.io/RLT/articles/Test-Surv-Band.html","id":"confidence-band-estimation","dir":"Articles","previous_headings":"","what":"Confidence Band Estimation","title":"RLT Package Testing Survival Functions and Features","text":"","code":"# generate a simpler data   library(RLT)   set.seed(2)   n = 600   p = 20   X = matrix(rnorm(n*p), n, p)      xlink <- function(x) exp(x[, 1] + x[, 3]/2)   FT = rexp(n, rate = xlink(X))   CT = pmin(6, rexp(n, rate = 0.25))      Y = pmin(FT, CT)   Censor = as.numeric(FT <= CT)   mean(Censor) ## [1] 0.765      ntest = 15   testx = matrix(rnorm(ntest*p), ntest, p)   #testx[1, 1:3] = c(0.5, 0, 0.5)   #testx[2, 1:3] = c(-0.5, 0, -0.5)      # get true survival function    timepoints = sort(unique(Y[Censor==1]))   SurvMat = matrix(NA, nrow(testx), length(timepoints))   exprate = xlink(testx)      for (j in 1:length(timepoints))   {     SurvMat[, j] = 1 - pexp(timepoints[j], rate = exprate )   } RLTfit <- RLT(X, Y, Censor, model = \"survival\",                  ntrees = 20000, nmin = 20, mtry = 20, split.gen = \"random\",                 resample.prob = 0.5, resample.replace = FALSE, nsplit = 5,                 param.control = list(split.rule = \"logrank\", \"var.ready\" = TRUE),                  importance = FALSE, verbose = TRUE, ncores = 10) ## Fitting Survival Forest...  ## ---------- Parameters Summary ---------- ##               (N, P) = (600, 20) ##           # of trees = 20000 ##         (mtry, nmin) = (20, 20) ##       split generate = Random, 5 ##             sampling = 0.5 w/o replace ##   (Obs, Var) weights = (No, No) ##           importance = none ##        reinforcement = No ## ---------------------------------------- ## Do not have 10 cores, use maximum 4 cores.    start_time <- Sys.time()   RLTPred <- predict(RLTfit, testx, ncores = 10, var.est = TRUE)   difftime(Sys.time(), start_time, units = \"secs\") ## Time difference of 12.57355 secs      paste(\"Prediction object size:\", format(object.size(RLTPred), units = \"MB\")) ## [1] \"Prediction object size: 24.3 Mb\""},{"path":"https://teazrq.github.io/RLT/articles/Test-Surv-Band.html","id":"naive-monte-carlo","dir":"Articles","previous_headings":"Confidence Band Estimation","what":"Naive Monte Carlo","title":"RLT Package Testing Survival Functions and Features","text":"Red line truth, black-solid estimated survival, black-dotted +/- 1.96 diagonal. blue-dotted naive Monte Carlo approach.","code":"alpha = 0.05    # original Monte Carlo approach without smoothing   start_time <- Sys.time()   SurvBand = get.surv.band(RLTPred, alpha = alpha, approach = \"eigen-th-mc\")   difftime(Sys.time(), start_time, units = \"secs\") ## Time difference of 2.407021 secs    par(mfrow = c(ceiling(ntest/3), 3))   logt = log(1 + timepoints)      for (i in 1:ntest)   {     # truth     plot(logt, SurvMat[i, ], type = \"l\", lwd = 2, col = \"red\", ylab = paste(\"Subject\",  i))          # estimated and 1.96     lines(logt, RLTPred$Survival[i,], lwd = 2, col = \"black\")     lines(logt, RLTPred$Survival[i, ] - qnorm(1-alpha/2)*sqrt(diag(RLTPred$Cov[,,i])),            lty = 2, col = \"black\")     lines(logt, RLTPred$Survival[i, ] + qnorm(1-alpha/2)*sqrt(diag(RLTPred$Cov[,,i])),            lty = 2, col = \"black\")          # naive     lines(logt, SurvBand[[i]]$lower[,1], lty = 2, lwd = 2, col = \"deepskyblue\")     lines(logt, SurvBand[[i]]$upper[,1], lty = 2, lwd = 2, col = \"deepskyblue\")   }"},{"path":"https://teazrq.github.io/RLT/articles/Test-Surv-Band.html","id":"smoothed-rescaling-mc-approach","dir":"Articles","previous_headings":"Confidence Band Estimation","what":"Smoothed Rescaling MC Approach","title":"RLT Package Testing Survival Functions and Features","text":"","code":"# Monte Carlo approach using smoothed rescaled cov matrix   start_time <- Sys.time()   SurvBand = get.surv.band(RLTPred, alpha = alpha, approach = \"smoothed-mc\", nsim = 1000) ## Warning: no DISPLAY variable so Tk is not available ## Warning in rgl.init(initValue, onlyNULL): RGL: unable to open X11 display ## Warning: 'rgl.init' failed, will use the null device. ## See '?rgl.useNULL' for ways to avoid this warning.   difftime(Sys.time(), start_time, units = \"secs\") ## Time difference of 8.21561 secs    par(mfrow = c(ceiling(ntest/3), 3))   logt = log(1 + timepoints)      for (i in 1:ntest)   {     # truth     plot(logt, SurvMat[i, ], type = \"l\", lwd = 2, col = \"red\", ylab = paste(\"Subject\",  i))          # estimated and 1.96     lines(logt, RLTPred$Survival[i,], lwd = 2, col = \"black\")     lines(logt, RLTPred$Survival[i, ] - qnorm(1-alpha/2)*sqrt(diag(RLTPred$Cov[,,i])),            lty = 2, col = \"black\")     lines(logt, RLTPred$Survival[i, ] + qnorm(1-alpha/2)*sqrt(diag(RLTPred$Cov[,,i])),            lty = 2, col = \"black\")          # naive     lines(logt, SurvBand[[i]]$lower[,1], lty = 2, lwd = 2, col = \"deepskyblue\")     lines(logt, SurvBand[[i]]$upper[,1], lty = 2, lwd = 2, col = \"deepskyblue\")   }"},{"path":"https://teazrq.github.io/RLT/articles/Test-Surv-Band.html","id":"smoothed-low-rank-monte-carlo","dir":"Articles","previous_headings":"Confidence Band Estimation","what":"Smoothed low-rank Monte Carlo","title":"RLT Package Testing Survival Functions and Features","text":"demonstration smoothed covariance approximation code used internally get.surv.band function. following comparison two covariance matrices","code":"# a low rank smoothed approach with adaptive bonferoni critical value   start_time <- Sys.time()   SurvBand = get.surv.band(RLTPred, alpha = alpha, approach = \"smoothed-lr\", r = 5)   difftime(Sys.time(), start_time, units = \"secs\") ## Time difference of 0.5070095 secs    par(mfrow = c(ceiling(ntest/3), 3))   logt = log(1 + timepoints)      for (i in 1:ntest)   {     # truth     plot(logt, SurvMat[i, ], type = \"l\", lwd = 2, col = \"red\", ylab = paste(\"Subject\",  i))          # estimated and 1.96     lines(logt, RLTPred$Survival[i,], lwd = 2, col = \"black\")     lines(logt, RLTPred$Survival[i, ] - qnorm(1-alpha/2)*sqrt(diag(RLTPred$Cov[,,i])),            lty = 2, col = \"black\")     lines(logt, RLTPred$Survival[i, ] + qnorm(1-alpha/2)*sqrt(diag(RLTPred$Cov[,,i])),            lty = 2, col = \"black\")          # naive     lines(logt, SurvBand[[i]]$lower[,1], lty = 2, lwd = 2, col = \"deepskyblue\")     lines(logt, SurvBand[[i]]$upper[,1], lty = 2, lwd = 2, col = \"deepskyblue\")   } cat(paste(\"average approximation error:\", mean(sapply(SurvBand, \"[[\", 3)))) ## average approximation error: 0.0653082651713332 i = 10   ccov = RLTPred$Cov[,,i]   heatmap(ccov[rev(1:nrow(ccov)),], Rowv = NA, Colv = NA, symm = TRUE) # generate grid points for kernel   p = dim(ccov)[1]     alltime = 0:(p+1)   alltime = alltime / stats::sd(alltime)   nknots = ceiling( max(alltime)/orthoDr::silverman(1, p) ) + 2      basis = orthoDr::kernel_weight(matrix(alltime),                                   matrix(seq(0, max(alltime), length.out = nknots)))      # raw marginal variance   mar_var = diag(ccov)      fit <- glmnet::glmnet(basis, c(mar_var[1], mar_var, utils::tail(mar_var, 1)),                         alpha = 0, intercept = FALSE,                         lower.limits = 0, lambda = 1e-5)   smarvar = stats::predict(fit, basis)   smarvar = smarvar[2:(p+1)]      newmat = ccov   diag(newmat) = smarvar      # find positive definite projection   pdmat = suppressWarnings(Matrix::nearPD(newmat,                             keepDiag = TRUE, # keep the diagonal                            base.matrix = TRUE,                             conv.norm.type = \"F\",                            trace = FALSE, maxit = 200))          sum((pdmat$mat - ccov)^2) / sum(ccov^2) ## [1] 0.002275218   heatmap(pdmat$mat[rev(1:nrow(pdmat$mat)),], Rowv = NA, Colv = NA, symm = TRUE)"},{"path":"https://teazrq.github.io/RLT/articles/Test-Surv.html","id":"install-and-load-package","dir":"Articles","previous_headings":"","what":"Install and Load Package","title":"RLT Package Testing Survival Functions and Features","text":"Install load GitHub version RLT package. use CRAN version. Load packages used guide.","code":"# install.packages(\"devtools\")   # devtools::install_github(\"teazrq/RLT\")   library(RLT) ## RLT and Random Forests v4.2.6 ## pre-release at github.com/teazrq/RLT library(randomForest)   library(randomForestSRC)   library(ranger)   library(parallel)   library(survival)"},{"path":"https://teazrq.github.io/RLT/articles/Test-Surv.html","id":"benchmark-against-existing-packages","dir":"Articles","previous_headings":"","what":"Benchmark Against Existing Packages","title":"RLT Package Testing Survival Functions and Features","text":"generate dataset 300 observations 200 variables, 100 continuous variables 100 categorical ones three categories. survival time censoring time follow exponential distribution.","code":"# generate date   set.seed(1)   trainn = 500   testn = 1000   n = trainn + testn   p = 200   X1 = matrix(rnorm(n*p/2), n, p/2)   X2 = matrix(as.integer(runif(n*p/2)*3), n, p/2)    X = data.frame(X1, X2)   xlink <- function(x) exp(x[, 7] + x[, 16] + x[, 25] + x[, p])   FT = rexp(n, rate = xlink(X))   CT = rexp(n, rate = 0.5)      y = pmin(FT, CT)   Censor = as.numeric(FT <= CT)   mean(Censor) ## [1] 0.744      # parameters   ntrees = 500   ncores = 10   nmin = 25   mtry = p/3   sampleprob = 0.85   rule = \"random\"   nsplit = ifelse(rule == \"best\", 0, 3)   importance = TRUE      trainX = X[1:trainn, ]   trainY = y[1:trainn]   trainCensor = Censor[1:trainn]      testX = X[1:testn + trainn, ]   testY = y[1:testn + trainn]   testCensor = Censor[1:testn + trainn]      # get true survival function    timepoints = sort(unique(trainY[trainCensor==1]))      SurvMat = matrix(NA, testn, length(timepoints))   expxlink = xlink(testX)         for (j in 1:length(timepoints))   {     SurvMat[, j] = 1 - pexp(timepoints[j], rate = expxlink )   }      # Calculate timepoint indices for ranger (different from RLT)   yloc = rep(NA, length(timepoints))   for (i in 1:length(timepoints)) yloc[i] = sum( timepoints[i] >= trainY )      for (j in (p/2 + 1):p) X[,j] = as.factor(X[,j]) # recording results   metric = data.frame(matrix(NA, 6, 7))   rownames(metric) = c(\"rlt\", \"rltsup\", \"rltcox\", \"rltcoxpen\", \"rsf\", \"ranger\")   colnames(metric) = c(\"fit.time\", \"pred.time\", \"oob.error\", \"pred.error\", \"L1\",                         \"obj.size\", \"tree.size\")      # fit RLT with log-rank split   start_time <- Sys.time()      RLTfit.logrank <- RLT(trainX, trainY, trainCensor, model = \"survival\",                          ntrees = ntrees, ncores = ncores,                          nmin = nmin, mtry = mtry, nsplit = nsplit,                         split.gen = rule, resample.prob = sampleprob,                         importance = importance,                          param.control = list(split.rule = \"logrank\", \"alpha\" = 0.2),                          verbose = TRUE, resample.replace=FALSE) ## Fitting Survival Forest...  ## ---------- Parameters Summary ---------- ##               (N, P) = (500, 200) ##           # of trees = 500 ##         (mtry, nmin) = (66, 25) ##       split generate = Random, 3 ##             sampling = 0.85 w/o replace ##   (Obs, Var) weights = (No, No) ##                alpha = 0.2 ##           importance = permute ##        reinforcement = No ## ---------------------------------------- ## Do not have 10 cores, use maximum 4 cores.   metric[1, 1] = difftime(Sys.time(), start_time, units = \"secs\")   start_time <- Sys.time()   RLTPred <- predict(RLTfit.logrank, testX, ncores = ncores)   metric[1, 2] = difftime(Sys.time(), start_time, units = \"secs\")   metric[1, 3] = RLTfit.logrank$Error   metric[1, 4] = 1 - cindex(testY, testCensor, colSums(apply(RLTPred$Hazard, 1, cumsum)))   metric[1, 5] = mean(colMeans(abs(RLTPred$Survival - SurvMat)))   metric[1, 6] = format(object.size(RLTfit.logrank), units = \"MB\")   metric[1, 7] = mean(unlist(lapply(RLTfit.logrank$FittedForest$SplitVar, length)))      # fit RLT with sup-log-rank split     start_time <- Sys.time()   RLTfit.suplogrank <- RLT(trainX, trainY, trainCensor, model = \"survival\",                             ntrees = ntrees, ncores = ncores,                             nmin = nmin, mtry = mtry, nsplit = nsplit,                            split.gen = rule, resample.prob = sampleprob,                            importance = importance,                             param.control = list(split.rule = \"suplogrank\", \"alpha\" = 0),                             verbose = TRUE, resample.replace=FALSE) ## Fitting Survival Forest...  ## ---------- Parameters Summary ---------- ##               (N, P) = (500, 200) ##           # of trees = 500 ##         (mtry, nmin) = (66, 25) ##       split generate = Random, 3 ##             sampling = 0.85 w/o replace ##   (Obs, Var) weights = (No, No) ##           importance = permute ##        reinforcement = No ## ---------------------------------------- ## Do not have 10 cores, use maximum 4 cores.   metric[2, 1] = difftime(Sys.time(), start_time, units = \"secs\")   start_time <- Sys.time()   RLTPred <- predict(RLTfit.suplogrank, testX, ncores = ncores)   metric[2, 2] = difftime(Sys.time(), start_time, units = \"secs\")   metric[2, 3] = RLTfit.suplogrank$Error   metric[2, 4] = 1- cindex(testY, testCensor, colSums(apply(RLTPred$Hazard, 1, cumsum)))   metric[2, 5] = mean(colMeans(abs(RLTPred$Survival - SurvMat)))   metric[2, 6] = format(object.size(RLTfit.suplogrank), units = \"MB\")   metric[2, 7] = mean(unlist(lapply(RLTfit.suplogrank$FittedForest$SplitVar, length)))      # fit RLT with cox-grad split     start_time <- Sys.time()   RLTfit.cg <- RLT(trainX, trainY, trainCensor, model = \"survival\",                     ntrees = ntrees, ncores = ncores,                     nmin = nmin, mtry = mtry, nsplit = nsplit,                    split.gen = rule, resample.prob = sampleprob,                    importance = importance,                    param.control = list(split.rule = \"coxgrad\", \"alpha\" = 0),                     verbose = TRUE, resample.replace=FALSE) ## Fitting Survival Forest...  ## ---------- Parameters Summary ---------- ##               (N, P) = (500, 200) ##           # of trees = 500 ##         (mtry, nmin) = (66, 25) ##       split generate = Random, 3 ##             sampling = 0.85 w/o replace ##   (Obs, Var) weights = (No, No) ##           importance = permute ##        reinforcement = No ## ---------------------------------------- ## Do not have 10 cores, use maximum 4 cores.   metric[3, 1] = difftime(Sys.time(), start_time, units = \"secs\")   start_time <- Sys.time()   RLTPred <- predict(RLTfit.cg, testX, ncores = ncores)   metric[3, 2] = difftime(Sys.time(), start_time, units = \"secs\")   metric[3, 3] = RLTfit.cg$Error   metric[3, 4] = 1 - cindex(testY, testCensor, colSums(apply(RLTPred$Hazard, 1, cumsum)))   metric[3, 5] = mean(colMeans(abs(RLTPred$Survival - SurvMat)))   metric[3, 6] = format(object.size(RLTfit.cg), units = \"MB\")   metric[3, 7] = mean(unlist(lapply(RLTfit.cg$FittedForest$SplitVar, length)))      # fit RLT with penalized coxgrad split   start_time <- Sys.time()   RLTfit.pcg <- RLT(trainX, trainY, trainCensor, model = \"survival\",                      ntrees = ntrees, nmin = nmin, mtry = mtry, nsplit = nsplit,                     split.gen = rule, resample.prob = sampleprob, importance = importance,                      # var.w = ifelse(c(1:(p)) %in% c(7, 16, 25, p), 1, 0.5),                     var.w = pmax(max(0, mean(RLTfit.logrank$VarImp)), RLTfit.logrank$VarImp),                     param.control = list(split.rule = \"coxgrad\", \"alpha\" = 0),                      verbose = TRUE, ncores = ncores, resample.replace=FALSE) ## Fitting Survival Forest...  ## ---------- Parameters Summary ---------- ##               (N, P) = (500, 200) ##           # of trees = 500 ##         (mtry, nmin) = (66, 25) ##       split generate = Random, 3 ##             sampling = 0.85 w/o replace ##   (Obs, Var) weights = (No, Yes) ##           importance = permute ##        reinforcement = No ## ---------------------------------------- ## Do not have 10 cores, use maximum 4 cores.   metric[4, 1] = difftime(Sys.time(), start_time, units = \"secs\")   start_time <- Sys.time()   RLTPredp <- predict(RLTfit.pcg, testX, ncores = ncores)   metric[4, 2] = difftime(Sys.time(), start_time, units = \"secs\")   metric[4, 3] = RLTfit.pcg$Error   metric[4, 4] = 1 - cindex(testY, testCensor, colSums(apply(RLTPredp$Hazard, 1, cumsum)))   metric[4, 5] = mean(colMeans(abs(RLTPredp$Survival - SurvMat)))   metric[4, 6] = format(object.size(RLTfit.pcg), units = \"MB\")   metric[4, 7] = mean(unlist(lapply(RLTfit.pcg$FittedForest$SplitVar, length)))      # fit rsf   options(rf.cores = ncores)   start_time <- Sys.time()   rsffit <- rfsrc(Surv(trainY, trainCensor) ~ ., data = data.frame(trainX, trainY, trainCensor),                   ntree = ntrees, nodesize = nmin, mtry = mtry,                   nsplit = nsplit, sampsize = trainn*sampleprob,                    importance = ifelse(importance==TRUE,\"random\", \"none\"), samptype = \"swor\",                   block.size = 1, ntime = NULL)   metric[5, 1] = difftime(Sys.time(), start_time, units = \"secs\")   start_time <- Sys.time()   rsfpred = predict(rsffit, data.frame(testX))   metric[5, 2] = difftime(Sys.time(), start_time, units = \"secs\")   metric[5, 3] = tail(rsffit$err.rate, 1)   metric[5, 4] = 1 - cindex(testY, testCensor, rowSums(rsfpred$chf))   metric[5, 5] = mean(colMeans(abs(rsfpred$survival - SurvMat)))   metric[5, 6] = format(object.size(rsffit), units = \"MB\")   metric[5, 7] = rsffit$forest$totalNodeCount / rsffit$forest$ntree      # fit ranger   start_time <- Sys.time()   rangerfit <- ranger(Surv(trainY, trainCensor) ~ ., data = data.frame(trainX, trainY, trainCensor),                        num.trees = ntrees, min.node.size = nmin, mtry = mtry,                        splitrule = \"logrank\", num.threads = ncores,                        sample.fraction = sampleprob, importance = \"permutation\") ## Growing trees.. Progress: 89%. Estimated remaining time: 3 seconds.   metric[6, 1] = difftime(Sys.time(), start_time, units = \"secs\")   start_time <- Sys.time()   rangerpred = predict(rangerfit, data.frame(testX))   metric[6, 2] = difftime(Sys.time(), start_time, units = \"secs\")   metric[6, 3] = rangerfit$prediction.error   metric[6, 4] = 1 - cindex(testY, testCensor, rowSums(rangerpred$chf))   # For ranger, calculate L1 error using available timepoints   # Use the first min(ncol(rangerpred$survival), ncol(SurvMat)) columns   n_cols = min(ncol(rangerpred$survival), ncol(SurvMat))   metric[6, 5] = mean(colMeans(abs(rangerpred$survival[, 1:n_cols] - SurvMat[, 1:n_cols])))   metric[6, 6] = format(object.size(rangerfit), units = \"MB\")   metric[6, 7] = mean(unlist(lapply(rangerfit$forest$split.varIDs, length)))      metric ##             fit.time pred.time oob.error pred.error        L1 obj.size tree.size ## rlt        1.9148796 0.3016081 0.2236643  0.2031392 0.1551231  76.4 Mb    85.668 ## rltsup     2.4169734 0.3006041 0.2250207  0.2034734 0.1564303  75.4 Mb    84.444 ## rltcox     0.9343102 0.2635462 0.2523899  0.2149457 0.1740367  49.4 Mb    54.132 ## rltcoxpen  0.8211284 0.2793784 0.2140024  0.1911703 0.1343944  52.4 Mb    57.724 ## rsf       11.1473689 0.6099076 0.2276127  0.2088789 0.1497412  68.8 Mb    38.120 ## ranger    37.6971779 2.4660072 0.2606490  0.2434724 0.1733328  81.1 Mb    99.680"},{"path":"https://teazrq.github.io/RLT/articles/Test-Surv.html","id":"print-a-single-tree","dir":"Articles","previous_headings":"","what":"Print a Single Tree","title":"RLT Package Testing Survival Functions and Features","text":"can use get.one.tree() function peek single tree.","code":"# print one tree   tree_num = 1   get.one.tree(RLTfit.logrank, tree_num) ## Tree #1 in the fitted survival forest: ##    SplitVar  SplitValue LeftNode RightNode NodeWeight ## 1      X43   1.68288969        2         3          0 ## 2   X100.1   1.00000000        4         5          0 ## 3      <NA>          NA       NA        NA         24 ## 4      X41  -0.28036643        6         7          0 ## 5      X25   0.55939226       54        55          0 ## 6       X7  -0.59370554        8         9          0 ## 7      X16   0.69455914       28        29          0 ## 8      X13   0.14737996       10        11          0 ## 9       X5  -1.72496728       12        13          0 ## 10     <NA>          NA       NA        NA         17 ## 11     <NA>          NA       NA        NA         14 ## 12     <NA>          NA       NA        NA          4 ## 13     X58   0.91986866       14        15          0 ## 14      X7   0.38743297       16        17          0 ## 15     <NA>          NA       NA        NA         14 ## 16     X52   1.38145859       18        19          0 ## 17     X29  -2.35909845       24        25          0 ## 18     X56  -0.07985515       20        21          0 ## 19     <NA>          NA       NA        NA          2 ## 20     X16  -0.07383513       22        23          0 ## 21     <NA>          NA       NA        NA         13 ## 22     <NA>          NA       NA        NA         15 ## 23     <NA>          NA       NA        NA         12 ## 24     <NA>          NA       NA        NA          1 ## 25     X25   0.61244108       26        27          0 ## 26     <NA>          NA       NA        NA         21 ## 27     <NA>          NA       NA        NA          6 ## 28     X85   0.52164387       30        31          0 ## 29     X21   0.64551395       50        51          0 ## 30     X24  -1.64435826       32        33          0 ## 31     X19  -0.98728279       46        47          0 ## 32     <NA>          NA       NA        NA          3 ## 33     X42   0.33495776       34        35          0 ## 34      X7   1.17477657       36        37          0 ## 35     <NA>          NA       NA        NA         23 ## 36      X6  -1.35745218       38        39          0 ## 37     <NA>          NA       NA        NA          6 ## 38     <NA>          NA       NA        NA          5 ## 39     X95  -1.11877220       40        41          0 ## 40     <NA>          NA       NA        NA          2 ## 41     X91  -1.90238058       42        43          0 ## 42     <NA>          NA       NA        NA          2 ## 43     X18  -0.21339745       44        45          0 ## 44     <NA>          NA       NA        NA         15 ## 45     <NA>          NA       NA        NA         17 ## 46     <NA>          NA       NA        NA          6 ## 47   X42.1   0.00000000       48        49          0 ## 48     <NA>          NA       NA        NA          9 ## 49     <NA>          NA       NA        NA         23 ## 50     X67   1.38454871       52        53          0 ## 51     <NA>          NA       NA        NA          8 ## 52     <NA>          NA       NA        NA         24 ## 53     <NA>          NA       NA        NA          2 ## 54     X16   0.52847380       56        57          0 ## 55   X33.1   1.00000000       80        81          0 ## 56     X81   1.11513339       58        59          0 ## 57     X65   0.86251034       74        75          0 ## 58      X7   1.98049752       60        61          0 ## 59     <NA>          NA       NA        NA          4 ## 60     X84   1.26782462       62        63          0 ## 61     <NA>          NA       NA        NA          3 ## 62     X21   1.39688296       64        65          0 ## 63     <NA>          NA       NA        NA          4 ## 64     X16   0.10839234       66        67          0 ## 65     <NA>          NA       NA        NA          6 ## 66     X92  -1.30290477       68        69          0 ## 67     <NA>          NA       NA        NA          8 ## 68     <NA>          NA       NA        NA          2 ## 69     X59   1.47732823       70        71          0 ## 70   X47.1   1.00000000       72        73          0 ## 71     <NA>          NA       NA        NA          1 ## 72     <NA>          NA       NA        NA         23 ## 73     <NA>          NA       NA        NA         18 ## 74     X55  -1.75655797       76        77          0 ## 75     <NA>          NA       NA        NA          1 ## 76     <NA>          NA       NA        NA          1 ## 77      X7   0.15223985       78        79          0 ## 78     <NA>          NA       NA        NA         14 ## 79     <NA>          NA       NA        NA         13 ## 80     <NA>          NA       NA        NA         23 ## 81     <NA>          NA       NA        NA         16    # to get the estimated hazard function of a terminal node   terminal_nodes = RLTfit.logrank$FittedForest$SplitVar[[tree_num]] == -1   haz = RLTfit.logrank$FittedForest$NodeHaz[[tree_num]][[which(terminal_nodes)[1]]]      # to get the estimated survival function of a terminal node   plot(c(0, RLTfit.logrank$timepoints),        exp(-cumsum(haz)),        xlab = \"time\", ylab = \"survival\", type = \"l\",         main = paste(\"First terminal node of Tree\", tree_num))"},{"path":"https://teazrq.github.io/RLT/articles/Test-Surv.html","id":"random-forest-kernel","dir":"Articles","previous_headings":"","what":"Random Forest Kernel","title":"RLT Package Testing Survival Functions and Features","text":"","code":"KernelW = forest.kernel(RLTfit.logrank, X1 = testX[1, ], X2 = trainX)$Kernel"},{"path":"https://teazrq.github.io/RLT/articles/Test-Surv.html","id":"setting-random-seed","dir":"Articles","previous_headings":"","what":"Setting random seed","title":"RLT Package Testing Survival Functions and Features","text":"","code":"## Fitting a forest   RLTfit1 <- RLT(trainX, trainY, trainCensor, model = \"survival\",                   ntrees = 100, importance = TRUE, nmin = 1)    RLTfit2 <- RLT(trainX, trainY, trainCensor, model = \"survival\",                   ntrees = 100, importance = TRUE, nmin = 1,                  seed = RLTfit1$parameters$seed)      # check if importance are identical   all(RLTfit1$VarImp == RLTfit2$VarImp) ## [1] TRUE      # prediction   RLTPred1 <- predict(RLTfit1, testX, keep.all = TRUE)   RLTPred2 <- predict(RLTfit2, testX, keep.all = TRUE)    # check predictions are identical   all(RLTPred1$Prediction == RLTPred2$Prediction) ## [1] TRUE"},{"path":"https://teazrq.github.io/RLT/articles/Test-Surv.html","id":"categorical-variables","dir":"Articles","previous_headings":"","what":"Categorical Variables","title":"RLT Package Testing Survival Functions and Features","text":"section demonstrates RLT handles categorical variables survival analysis.","code":""},{"path":"https://teazrq.github.io/RLT/articles/Variable-Importance.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Variable Importance in RLT","text":"Variable importance crucial aspect random forest analysis helps identify variables influential making predictions. RLT package provides multiple approaches calculating variable importance, including permutation-based methods distributed assignment approaches.","code":""},{"path":[]},{"path":"https://teazrq.github.io/RLT/articles/Variable-Importance.html","id":"permutation-based-variable-importance","dir":"Articles","previous_headings":"Variable Importance Methods","what":"1. Permutation-based Variable Importance","title":"Variable Importance in RLT","text":"permutation approach measures variable importance randomly permuting values variable measuring resulting change prediction accuracy. method requires significant number --bag samples.","code":""},{"path":"https://teazrq.github.io/RLT/articles/Variable-Importance.html","id":"distributed-assignment-variable-importance","dir":"Articles","previous_headings":"Variable Importance Methods","what":"2. Distributed Assignment Variable Importance","title":"Variable Importance in RLT","text":"approach calculates possible terminal nodes probabilities subject land terminal node. sends observations child nodes weights proportional child node sizes, allowing calculation variable importance sampling without replacement just one --bag sample.","code":""},{"path":"https://teazrq.github.io/RLT/articles/Variable-Importance.html","id":"regression-example","dir":"Articles","previous_headings":"","what":"Regression Example","title":"Variable Importance in RLT","text":"","code":"library(RLT) #> RLT and Random Forests v4.2.6 #> pre-release at github.com/teazrq/RLT  # Generate sample data set.seed(1) n = 1000 p = 20 X = matrix(rnorm(n*p), n, p) y = X[, 1] + X[, 2] + rnorm(n)  # Split data trainn = 800 trainX = X[1:trainn, ] trainY = y[1:trainn] testX = X[(trainn+1):n, ] testY = y[(trainn+1):n]  # Model parameters ntrees = 100 mtry = 5 nmin = 5 nsplit = 10 ncores = 1 rule = \"best\""},{"path":"https://teazrq.github.io/RLT/articles/Variable-Importance.html","id":"permutation-based-variable-importance-1","dir":"Articles","previous_headings":"Regression Example","what":"Permutation-based Variable Importance","title":"Variable Importance in RLT","text":"","code":"# Fit RLT model with permutation importance RLTfit <- RLT(trainX, trainY, model = \"regression\",                ntrees = ntrees, mtry = mtry, nmin = nmin,                split.gen = rule, nsplit = nsplit,                resample.prob = 0.8,                resample.replace = FALSE,                importance = \"permute\",                ncores = ncores,                verbose = TRUE) #> Regression Random Forest ...  #> ---------- Parameters Summary ---------- #>               (N, P) = (800, 20) #>           # of trees = 100 #>         (mtry, nmin) = (5, 5) #>       split generate = Best #>             sampling = 0.8 w/o replace #>   (Obs, Var) weights = (No, No) #>           importance = permute #>        reinforcement = No #> ----------------------------------------  # Plot variable importance par(mfrow=c(2,2)) par(mar = c(1, 2, 2, 2))  barplot(as.vector(RLTfit$VarImp), main = \"RLT Permutation\",          names.arg = paste0(\"X\", 1:p), las = 2, cex.names = 0.7)"},{"path":"https://teazrq.github.io/RLT/articles/Variable-Importance.html","id":"distributed-assignment-variable-importance-1","dir":"Articles","previous_headings":"Regression Example","what":"Distributed Assignment Variable Importance","title":"Variable Importance in RLT","text":"","code":"# Fit RLT model with distributed importance RLTfit_dist <- RLT(trainX, trainY, model = \"regression\",                     ntrees = ntrees, mtry = mtry, nmin = nmin,                     split.gen = rule, nsplit = nsplit,                     resample.prob = (trainn - 1)/trainn,                     resample.replace = FALSE,                     importance = \"distribute\",                     ncores = ncores,                     verbose = TRUE) #> Regression Random Forest ...  #> ---------- Parameters Summary ---------- #>               (N, P) = (800, 20) #>           # of trees = 100 #>         (mtry, nmin) = (5, 5) #>       split generate = Best #>             sampling = 0.99875 w/o replace #>   (Obs, Var) weights = (No, No) #>           importance = distribute #>        reinforcement = No #> ----------------------------------------  # Plot distributed importance par(mfrow=c(1,2)) par(mar = c(1, 2, 2, 2))  barplot(as.vector(RLTfit$VarImp), main = \"RLT Permutation\",          names.arg = paste0(\"X\", 1:p), las = 2, cex.names = 0.7) barplot(as.vector(RLTfit_dist$VarImp), main = \"RLT Distributed\",          names.arg = paste0(\"X\", 1:p), las = 2, cex.names = 0.7)"},{"path":"https://teazrq.github.io/RLT/articles/Variable-Importance.html","id":"classification-example","dir":"Articles","previous_headings":"","what":"Classification Example","title":"Variable Importance in RLT","text":"","code":"# Generate classification data set.seed(1) n = 1000 p = 20 X = matrix(rnorm(n*p), n, p) y = as.factor(ifelse(X[, 1] + X[, 2] > 0, 1, 0))  # Split data trainn = 800 trainX = X[1:trainn, ] trainY = y[1:trainn] testX = X[(trainn+1):n, ] testY = y[(trainn+1):n]"},{"path":"https://teazrq.github.io/RLT/articles/Variable-Importance.html","id":"classification-variable-importance","dir":"Articles","previous_headings":"Classification Example","what":"Classification Variable Importance","title":"Variable Importance in RLT","text":"","code":"# Fit classification model with importance RLTfit_cla <- RLT(trainX, trainY, model = \"classification\",                    ntrees = ntrees, mtry = mtry, nmin = nmin,                    split.gen = rule, nsplit = nsplit,                    resample.prob = 0.8,                    resample.replace = FALSE,                    importance = \"permute\",                    ncores = ncores,                    verbose = TRUE) #> Classification Random Forest ...  #> ---------- Parameters Summary ---------- #>               (N, P) = (800, 20) #>           # of trees = 100 #>         (mtry, nmin) = (5, 5) #>       split generate = Best #>             sampling = 0.8 w/o replace #>   (Obs, Var) weights = (No, No) #>           importance = permute #>        reinforcement = No #> ----------------------------------------  # Plot variable importance par(mfrow=c(2,2)) par(mar = c(1, 2, 2, 2))  barplot(as.vector(RLTfit_cla$VarImp), main = \"RLT Classification\",          names.arg = paste0(\"X\", 1:p), las = 2, cex.names = 0.7)  # Distributed importance for classification RLTfit_cla_dist <- RLT(trainX, trainY, model = \"classification\",                         ntrees = ntrees, mtry = mtry, nmin = nmin,                         split.gen = rule, nsplit = nsplit,                        resample.prob = (trainn - 1)/trainn,                         resample.replace = FALSE,                         importance = \"distribute\",                         ncores = ncores,                         verbose = TRUE) #> Classification Random Forest ...  #> ---------- Parameters Summary ---------- #>               (N, P) = (800, 20) #>           # of trees = 100 #>         (mtry, nmin) = (5, 5) #>       split generate = Best #>             sampling = 0.99875 w/o replace #>   (Obs, Var) weights = (No, No) #>           importance = distribute #>        reinforcement = No #> ----------------------------------------  barplot(as.vector(RLTfit_cla_dist$VarImp), main = \"RLT Classification Distributed\",          names.arg = paste0(\"X\", 1:p), las = 2, cex.names = 0.7)"},{"path":"https://teazrq.github.io/RLT/articles/Variable-Importance.html","id":"survival-analysis-example","dir":"Articles","previous_headings":"","what":"Survival Analysis Example","title":"Variable Importance in RLT","text":"","code":"# Generate survival data set.seed(1) n = 600 p = 20 X = matrix(rnorm(n*p), n, p)  # Create survival times xlink <- function(x) exp(x[, 1] + x[, 3]/2) FT = rexp(n, rate = xlink(X)) CT = pmin(6, rexp(n, rate = 0.25))  Y = pmin(FT, CT) Censor = as.numeric(FT <= CT)  # Split data trainn = 500 trainX = X[1:trainn, ] trainY = Y[1:trainn] trainCensor = Censor[1:trainn] testX = X[(trainn+1):n, ] testY = Y[(trainn+1):n] testCensor = Censor[(trainn+1):n]"},{"path":"https://teazrq.github.io/RLT/articles/Variable-Importance.html","id":"survival-variable-importance","dir":"Articles","previous_headings":"Survival Analysis Example","what":"Survival Variable Importance","title":"Variable Importance in RLT","text":"","code":"# Fit survival models with different splitting rules RLTfit_logrank <- RLT(trainX, trainY, trainCensor, model = \"survival\",                        ntrees = ntrees, ncores = ncores,                        nmin = nmin, mtry = mtry, nsplit = nsplit,                       split.gen = rule, resample.prob = 0.8,                       importance = TRUE,                        param.control = list(split.rule = \"logrank\", \"alpha\" = 0.2),                        verbose = TRUE, resample.replace=FALSE) #> Fitting Survival Forest...  #> ---------- Parameters Summary ---------- #>               (N, P) = (500, 20) #>           # of trees = 100 #>         (mtry, nmin) = (5, 5) #>       split generate = Best #>             sampling = 0.8 w/o replace #>   (Obs, Var) weights = (No, No) #>                alpha = 0.2 #>           importance = permute #>        reinforcement = No #> ----------------------------------------  RLTfit_suplogrank <- RLT(trainX, trainY, trainCensor, model = \"survival\",                           ntrees = ntrees, ncores = ncores,                           nmin = nmin, mtry = mtry, nsplit = nsplit,                          split.gen = rule, resample.prob = 0.8,                          importance = TRUE,                           param.control = list(split.rule = \"suplogrank\", \"alpha\" = 0),                           verbose = TRUE, resample.replace=FALSE) #> Fitting Survival Forest...  #> ---------- Parameters Summary ---------- #>               (N, P) = (500, 20) #>           # of trees = 100 #>         (mtry, nmin) = (5, 5) #>       split generate = Best #>             sampling = 0.8 w/o replace #>   (Obs, Var) weights = (No, No) #>           importance = permute #>        reinforcement = No #> ----------------------------------------  RLTfit_cg <- RLT(trainX, trainY, trainCensor, model = \"survival\",                   ntrees = ntrees, ncores = ncores,                   nmin = nmin, mtry = mtry, nsplit = nsplit,                  split.gen = rule, resample.prob = 0.8,                  importance = TRUE,                  param.control = list(split.rule = \"coxgrad\", \"alpha\" = 0),                   verbose = TRUE, resample.replace=FALSE) #> Fitting Survival Forest...  #> ---------- Parameters Summary ---------- #>               (N, P) = (500, 20) #>           # of trees = 100 #>         (mtry, nmin) = (5, 5) #>       split generate = Best #>             sampling = 0.8 w/o replace #>   (Obs, Var) weights = (No, No) #>           importance = permute #>        reinforcement = No #> ----------------------------------------  # Plot variable importance for different splitting rules par(mfrow=c(2,2)) par(mar = c(1, 2, 2, 2))  barplot(as.vector(RLTfit_logrank$VarImp), main = \"RLT Log-rank\",          names.arg = paste0(\"X\", 1:p), las = 2, cex.names = 0.7) barplot(as.vector(RLTfit_suplogrank$VarImp), main = \"RLT Sup Log-rank\",          names.arg = paste0(\"X\", 1:p), las = 2, cex.names = 0.7) barplot(as.vector(RLTfit_cg$VarImp), main = \"RLT Cox-grad\",          names.arg = paste0(\"X\", 1:p), las = 2, cex.names = 0.7)"},{"path":"https://teazrq.github.io/RLT/articles/Variable-Importance.html","id":"comparison-with-other-packages","dir":"Articles","previous_headings":"","what":"Comparison with Other Packages","title":"Variable Importance in RLT","text":"","code":"# Load required packages for comparison if (requireNamespace(\"randomForest\", quietly = TRUE)) {   library(randomForest)      # Random Forest comparison   rf.fit <- randomForest(trainX, trainY, ntree = ntrees,                           mtry = mtry, nodesize = nmin,                           replace = FALSE,                          sampsize = trainn*0.8,                           importance = TRUE)      par(mfrow=c(1,2))   par(mar = c(1, 2, 2, 2))      barplot(as.vector(RLTfit$VarImp), main = \"RLT\",            names.arg = paste0(\"X\", 1:p), las = 2, cex.names = 0.7)   barplot(rf.fit$importance[, 1], main = \"randomForest\",            names.arg = paste0(\"X\", 1:p), las = 2, cex.names = 0.7) }"},{"path":"https://teazrq.github.io/RLT/articles/Variable-Importance.html","id":"key-points","dir":"Articles","previous_headings":"","what":"Key Points","title":"Variable Importance in RLT","text":"Permutation Importance: Requires --bag samples provides standard variable importance measures Distributed Importance: efficient sampling without replacement, can work fewer samples Different Models: Variable importance can calculated regression, classification, survival models Splitting Rules: survival analysis, different splitting rules may produce different variable importance rankings Interpretation: Higher importance values indicate variables greater influence predictions","code":""},{"path":"https://teazrq.github.io/RLT/articles/Variable-Importance.html","id":"summary","dir":"Articles","previous_headings":"","what":"Summary","title":"Variable Importance in RLT","text":"RLT package provides flexible efficient methods calculating variable importance across different types models. choice permutation distributed methods depends specific needs computational constraints.","code":""},{"path":"https://teazrq.github.io/RLT/articles/tutorial_rl.html","id":"reinforcement-learning-splitting-rule","dir":"Articles","previous_headings":"","what":"Reinforcement Learning Splitting Rule","title":"Reinforcement Learning Splitting Rule","text":"page provides links tutorials demonstrating use reinforcement learning-based splitting rules classification regression settings.","code":""},{"path":"https://teazrq.github.io/RLT/articles/tutorial_rl.html","id":"rl-classification-example","dir":"Articles","previous_headings":"Reinforcement Learning Splitting Rule","what":"RL Classification Example","title":"Reinforcement Learning Splitting Rule","text":"Learn reinforcement learning applied tree-based classification tasks: View RL Classification Example","code":""},{"path":"https://teazrq.github.io/RLT/articles/tutorial_rl.html","id":"rl-regression-example","dir":"Articles","previous_headings":"Reinforcement Learning Splitting Rule","what":"RL Regression Example","title":"Reinforcement Learning Splitting Rule","text":"Explore use reinforcement learning regression-based tree models: View RL Regression Example","code":""},{"path":"https://teazrq.github.io/RLT/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Ruoqing Zhu. Author, maintainer, copyright holder. Sarah Formentini. Author. Haowen Zhou. Contributor. Tianning Xu. Contributor. Zhechao Huang. Contributor.","code":""},{"path":"https://teazrq.github.io/RLT/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Zhu R. (). Reinforcement Learning Trees, R package version 4.2.6. Zhu, R., Zeng, D., & Kosorok, M. R. (2015). Reinforcement learning trees. Journal American Statistical Association, 110(512), 1770-1784. Zhu, R. & Kosorok, M. R. (2012). Recursively imputed survival trees. Journal American Statistical Association, 107(497), 331-340.","code":"@Manual{,   title = {RLT: Reinforcement Learning Trees},   author = {R. Zhu},   publisher = {manual},   note = {R package version 4.2.6},   url = {https://cran.r-project.org/package=RLT},   pdf = {https://cran.r-project.org/web/packages/RLT/RLT.pdf}, } @Article{,   title = {Reinforcement Learning Trees},   author = {Ruoqing Zhu and Donglin Zeng and Michael R. Kosorok},   journal = {Journal of the American Statistical Association},   year = {2015},   volume = {110},   number = {512},   pages = {1770-1784},   doi = {10.1080/01621459.2015.1036994},   url = {http://dx.doi.org/10.1080/01621459.2015.1036994}, } @Article{,   title = {Recursively imputed survival trees},   author = {Ruoqing Zhu and Michael R. Kosorok},   journal = {Journal of the American Statistical Association},   year = {2012},   volume = {107},   number = {497},   pages = {331-340},   doi = {10.1080/01621459.2011.637468},   url = {http://dx.doi.org/10.1080/01621459.2011.637468}, }"},{"path":"https://teazrq.github.io/RLT/index.html","id":"rlt","dir":"","previous_headings":"","what":"Reinforcement Learning Trees","title":"Reinforcement Learning Trees","text":"new version (>= 4.0.0) RLT package. Versions prior 4.0.0 written C (available RLT-Archive), newer versions based C++. new version replace original CRAN package finished. goal RLT provide new functionalities random forest models. includes embedded model fit learning better splitting rule; linear combination splits, confidence intervals, several new approaches currently developed.","code":""},{"path":"https://teazrq.github.io/RLT/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Reinforcement Learning Trees","text":"can install version using {r}     # install.packages(\"devtools\")     devtools::install_github(\"teazrq/RLT\") use MacOS, need install libraries able compile package. Please follow guild.","code":""},{"path":"https://teazrq.github.io/RLT/index.html","id":"new-features-highlight","dir":"","previous_headings":"","what":"New features highlight","title":"Reinforcement Learning Trees","text":"Unbiased variance estimation (regression forest) based Xu, Zhu Shao (2022+) Unbiased survival function confidence band estimation based Formentini, Liang Zhu (2022+) Reproducibility parallel tree version xoshiro256plus random number generator Speed space improvement earlier c version [implemented] Graph random forests [implemented] Python API","code":""},{"path":"https://teazrq.github.io/RLT/reference/QuanForest.html","id":null,"dir":"Reference","previous_headings":"","what":"QuanForest — QuanForest","title":"QuanForest — QuanForest","text":"Internal function fitting quantile forest","code":""},{"path":"https://teazrq.github.io/RLT/reference/QuanForest.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"QuanForest — QuanForest","text":"","code":"QuanForest(x, y, ncat, obs.w, var.w, resample.preset, param, ...)"},{"path":"https://teazrq.github.io/RLT/reference/RLT.html","id":null,"dir":"Reference","previous_headings":"","what":" Reinforcement Learning Trees  — RLT","title":" Reinforcement Learning Trees  — RLT","text":"","code":"Fit models for regression, classification and survival                    analysis using reinforced splitting rules. The model                    fits regular random forest models by default unless the                    parameter \\code{reinforcement} is set to `\"TRUE\"`. Using                    \\code{reinforcement = TRUE} activates embedded model for                    splitting variable selection and allows linear combination                    split. To specify parameters of embedded models, see                    definition of \\code{param.control} for details."},{"path":"https://teazrq.github.io/RLT/reference/RLT.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":" Reinforcement Learning Trees  — RLT","text":"","code":"RLT(   x,   y,   censor = NULL,   model = NULL,   ntrees = if (reinforcement) 100 else 500,   mtry = max(1, as.integer(ncol(x)/3)),   nmin = max(1, as.integer(log(nrow(x)))),   split.gen = \"random\",   nsplit = 1,   resample.replace = TRUE,   resample.prob = if (resample.replace) 1 else 0.8,   resample.preset = NULL,   obs.w = NULL,   var.w = NULL,   importance = FALSE,   reinforcement = FALSE,   param.control = list(),   ncores = 0,   verbose = 0,   seed = NULL,   ... )"},{"path":"https://teazrq.github.io/RLT/reference/RLT.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":" Reinforcement Learning Trees  — RLT","text":"x matrix data.frame features. x data.frame, factors treated categorical variables, go exhaustive search splitting criteria. y Response variable. numeric/factor vector. censor Censoring indicator survival model used. model model type: \"regression\", \"classification\", \"quantile\", \"survival\" \"graph\". ntrees Number trees, ntrees = 100 reinforcement used ntrees = 1000 otherwise. mtry Number randomly selected variables used internal node. nmin Terminal node size. Splitting stop internal node size less equal nmin. split.gen cutting points generated: \"random\", \"rank\" \"best\". minimum child node size enforced (alpha $> 0$), \"rank\" \"best\" used. nsplit Number random cutting points compare variable internal node. resample.replace Whether -bag samples obtained replacement. resample.prob Proportion -bag samples. resample.preset pre-specified matrix -bag data indicator/count matrix. must \\(n \\times\\) ntrees matrix integer entries. Positive number indicates number copies observation (row) corresponding tree (column); zero indicates --bag; negative values indicates used either. Extremely large counts avoided. sum column exceed \\(n\\). obs.w Observation weights. weights used calculating splitting scores, weighted variance reduction weighted gini index. used sampling observations. case, one can pre-specify resample.preset instead balanced sampling, etc. survival analysis, observation weights implemented \"logrank\" \"suplogrank\" tests, due difficulty calculating variance test statistic. However, used \"coxgrad\" splitting rule. models, feature currently available. var.w Variable weights. supplied, default perform weighted sampling mtry variables. usage, see details split.rule param.control. importance Whether calculate variable importance measures. set \"TRUE\" (\"permute\"), calculation follows Breiman's original permutation strategy. set \"distribute\", sends oob data child nodes weights proportional sample sizes. Hence final prediction weighted average possible terminal nodes perturbed observation fall . feature currently available regression classification models. reinforcement reinforcement splitting rule used. Default \"FALSE\", .e., regular random forests marginal search splitting variable. activated, embedded model fitted find best splitting variable linear combination , linear.comb $> 1$. can also specified param.control. param.control list additional parameters. can used specify features random forest set embedded model parameters reinforcement splitting rules. Using reinforcement = TRUE automatically generate default tuning embedded model. mode currently available regression. necessarily optimized. embed.ntrees: number trees embedded model embed.mtry: number proportion variables embed.nmin: terminal node size embed.split.gen random cutting point search method (\"random\", \"rank\" \"best\") embed.nsplit number random cutting points embed.resample.replace whether sample replacement embed.resample.prob: proportion samples (internal node) embedded model embed.mute muting rate embed.protect number protected variables embed.threshold threshold, fraction best VI, included protected set internal node.   ncores Number CPU logical cores. Default 0 (using available cores). verbose Whether info printed. seed Random seed number replicate previously fitted forest. Internally, xoshiro256++ generator used. specified, seed generated automatically recorded. ... Additional arguments.","code":"\\code{linear.comb} is a separate feature that can be                    activated with or without using reinforcement. It creates                    linear combination of features as the splitting rule.                    Currently only available for regression.                    \\itemize{                    \\item In reinforcement mode, a linear combination is created                          using the top continuous variables from the embedded                          model. If a categorical variable is the best, then                          a regular split will be used. The splitting point                          will be searched based on \\code{split.rule} of the                          model.                    \\item In non-reinforcement mode, a marginal screening                          is performed and the top features are used to construct                          the linear combination. This is an experimental feature.                    }                     \\code{split.rule} is used to specify the criteria used                    to compare different splittings. Here are the available                    choices. The first one is the default:                    \\itemize{                    \\item Regression: `\"var\"` (variance reduction); `\"pca\"`                          and `\"sir\"` can be used for linear combination splits                    \\item Classification: `\"gini\"` (gini index)                    \\item Survival: `\"logrank\"` (log-rank test), `\"suplogrank\"`,                          `\"coxgrad\"`.                    \\item Quantile: `\"ks\"` (Kolmogorov-Smirnov test)                    \\item Graph: `\"spectral\"` (spectral embedding with variance                    reduction)                    }                     \\code{resample.track} indicates whether to keep track                    of the observations used in each tree.                     \\code{var.ready} this is a feature to allow calculating variance                    (hence confidence intervals) of the random forest prediction.                    Currently only available for regression (Xu, Zhu & Shao, 2023)                    and confidence band in survival models (Formentini, Liang & Zhu, 2023).                    Please note that this only perpares the model fitting                    so that it is ready for the calculation. To obtain the                    confidence intervals, please see the prediction function.                    Specifying \\code{var.ready = TRUE} has the following effect                    if these parameters are not already provided. For details                    of their restrictions, please see the orignal paper.                    \\itemize{                    \\item \\code{resample.preset} is constructed automatically                    \\item \\code{resample.replace} is set to `FALSE`                    \\item \\code{resample.prob} is set to \\eqn{n / 2}                    \\item \\code{resample.track} is set to `TRUE`                    }                     It is recommended to use a very large \\code{ntrees},                    e.g, 10000 or larger. For \\code{resample.prob} greater                    than \\eqn{n / 2}, one should consider the bootstrap                    approach in Xu, Zhu & Shao (2023).                     \\code{alpha} force a minimum proportion of samples                    (of the parent node) in each child node.                     \\code{failcount} specifies the unique number of failure                    time points used in survival model. By default, all failure                    time points will be used. A smaller number may speed up                    the computation. The time points will be chosen uniformly                    on the quantiles of failure times, while must include the                    minimum and the maximum."},{"path":"https://teazrq.github.io/RLT/reference/RLT.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":" Reinforcement Learning Trees  — RLT","text":"RLT fitted object, constructed list consisting FittedForestFitted tree structures VarImpVariable importance measures, importance = TRUE PredictionOut--bag prediction ErrorOut--bag prediction error, adaptive model type ObsTrackProvided resample.track = TRUE, var.ready = TRUE, resample.preset supplied. n \\(\\times\\) ntrees matrix meaning resample.preset. classification forests, items provided replace regression version NClassThe number classes ProbOut--bag predicted probability survival forests, items provided replace regression version timepointsordered observed failure times NFailThe number observed failure times PredictionOut--bag prediciton hazard function","code":""},{"path":"https://teazrq.github.io/RLT/reference/RLT.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":" Reinforcement Learning Trees  — RLT","text":"Zhu, R., Zeng, D., & Kosorok, M. R. (2015) \"Reinforcement Learning Trees.\" Journal American Statistical Association. 110(512), 1770-1784. Xu, T., Zhu, R., & Shao, X. (2023) \"Variance Estimation Random Forests Infinite-Order U-statistics.\" arXiv preprint arXiv:2202.09008. Formentini, S. E., Wei L., & Zhu, R. (2022) \"Confidence Band Estimation Survival Random Forests.\" arXiv preprint arXiv:2204.12038.","code":""},{"path":"https://teazrq.github.io/RLT/reference/RegForest.html","id":null,"dir":"Reference","previous_headings":"","what":"ClaForest — RegForest","title":"ClaForest — RegForest","text":"Internal function fitting classification forest Internal function fitting regression forest","code":""},{"path":"https://teazrq.github.io/RLT/reference/RegForest.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"ClaForest — RegForest","text":"","code":"ClaForest(x, y, ncat, obs.w, var.w, resample.preset, param, ...)  RegForest(x, y, ncat, obs.w, var.w, resample.preset, param, ...)"},{"path":"https://teazrq.github.io/RLT/reference/SurvForest.html","id":null,"dir":"Reference","previous_headings":"","what":"SurvForest — SurvForest","title":"SurvForest — SurvForest","text":"Internal function fitting survival forest","code":""},{"path":"https://teazrq.github.io/RLT/reference/SurvForest.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"SurvForest — SurvForest","text":"","code":"SurvForest(   x,   y,   censor,   ncat,   failcount,   obs.w,   var.w,   resample.preset,   param,   ... )"},{"path":"https://teazrq.github.io/RLT/reference/check_control.html","id":null,"dir":"Reference","previous_headings":"","what":"check_control — check_control","title":"check_control — check_control","text":"check_control","code":""},{"path":"https://teazrq.github.io/RLT/reference/check_control.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"check_control — check_control","text":"","code":"check_control(control, param)"},{"path":"https://teazrq.github.io/RLT/reference/check_importance.html","id":null,"dir":"Reference","previous_headings":"","what":"check_importance — check_importance","title":"check_importance — check_importance","text":"check_importance","code":""},{"path":"https://teazrq.github.io/RLT/reference/check_importance.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"check_importance — check_importance","text":"","code":"check_importance(importance)"},{"path":"https://teazrq.github.io/RLT/reference/check_mtry.html","id":null,"dir":"Reference","previous_headings":"","what":"check_mtry — check_mtry","title":"check_mtry — check_mtry","text":"check_mtry","code":""},{"path":"https://teazrq.github.io/RLT/reference/check_mtry.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"check_mtry — check_mtry","text":"","code":"check_mtry(mtry, p)"},{"path":"https://teazrq.github.io/RLT/reference/check_ncores.html","id":null,"dir":"Reference","previous_headings":"","what":"check_ncores — check_ncores","title":"check_ncores — check_ncores","text":"check_ncores","code":""},{"path":"https://teazrq.github.io/RLT/reference/check_ncores.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"check_ncores — check_ncores","text":"","code":"check_ncores(ncores)"},{"path":"https://teazrq.github.io/RLT/reference/check_nmin.html","id":null,"dir":"Reference","previous_headings":"","what":"check_nmin — check_nmin","title":"check_nmin — check_nmin","text":"check_nmin","code":""},{"path":"https://teazrq.github.io/RLT/reference/check_nmin.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"check_nmin — check_nmin","text":"","code":"check_nmin(nmin)"},{"path":"https://teazrq.github.io/RLT/reference/check_nsplit.html","id":null,"dir":"Reference","previous_headings":"","what":"check_nsplit — check_nsplit","title":"check_nsplit — check_nsplit","text":"check_nsplit","code":""},{"path":"https://teazrq.github.io/RLT/reference/check_nsplit.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"check_nsplit — check_nsplit","text":"","code":"check_nsplit(nsplit)"},{"path":"https://teazrq.github.io/RLT/reference/check_ntrees.html","id":null,"dir":"Reference","previous_headings":"","what":"check_ntrees — check_ntrees","title":"check_ntrees — check_ntrees","text":"check_ntrees","code":""},{"path":"https://teazrq.github.io/RLT/reference/check_ntrees.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"check_ntrees — check_ntrees","text":"","code":"check_ntrees(ntrees)"},{"path":"https://teazrq.github.io/RLT/reference/check_obsw.html","id":null,"dir":"Reference","previous_headings":"","what":"check_obsw — check_obsw","title":"check_obsw — check_obsw","text":"check_obsw","code":""},{"path":"https://teazrq.github.io/RLT/reference/check_obsw.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"check_obsw — check_obsw","text":"","code":"check_obsw(obs.w, n)"},{"path":"https://teazrq.github.io/RLT/reference/check_reinforcement.html","id":null,"dir":"Reference","previous_headings":"","what":"check_reinforcement — check_reinforcement","title":"check_reinforcement — check_reinforcement","text":"check_reinforcement","code":""},{"path":"https://teazrq.github.io/RLT/reference/check_reinforcement.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"check_reinforcement — check_reinforcement","text":"","code":"check_reinforcement(reinforcement)"},{"path":"https://teazrq.github.io/RLT/reference/check_resamplepreset.html","id":null,"dir":"Reference","previous_headings":"","what":"check_resamplepreset — check_resamplepreset","title":"check_resamplepreset — check_resamplepreset","text":"check_resamplepreset","code":""},{"path":"https://teazrq.github.io/RLT/reference/check_resamplepreset.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"check_resamplepreset — check_resamplepreset","text":"","code":"check_resamplepreset(resample.preset, param, param.control)"},{"path":"https://teazrq.github.io/RLT/reference/check_resampleprob.html","id":null,"dir":"Reference","previous_headings":"","what":"check_resampleprob — check_resampleprob","title":"check_resampleprob — check_resampleprob","text":"check_resampleprob","code":""},{"path":"https://teazrq.github.io/RLT/reference/check_resampleprob.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"check_resampleprob — check_resampleprob","text":"","code":"check_resampleprob(resample.prob)"},{"path":"https://teazrq.github.io/RLT/reference/check_resamplereplace.html","id":null,"dir":"Reference","previous_headings":"","what":"check_resamplereplace — check_resamplereplace","title":"check_resamplereplace — check_resamplereplace","text":"check_resamplereplace","code":""},{"path":"https://teazrq.github.io/RLT/reference/check_resamplereplace.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"check_resamplereplace — check_resamplereplace","text":"","code":"check_resamplereplace(resample.replace)"},{"path":"https://teazrq.github.io/RLT/reference/check_seed.html","id":null,"dir":"Reference","previous_headings":"","what":"check_seed — check_seed","title":"check_seed — check_seed","text":"check_seed","code":""},{"path":"https://teazrq.github.io/RLT/reference/check_seed.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"check_seed — check_seed","text":"","code":"check_seed(seed)"},{"path":"https://teazrq.github.io/RLT/reference/check_splitgen.html","id":null,"dir":"Reference","previous_headings":"","what":"check_splitgen — check_splitgen","title":"check_splitgen — check_splitgen","text":"check_splitgen","code":""},{"path":"https://teazrq.github.io/RLT/reference/check_splitgen.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"check_splitgen — check_splitgen","text":"","code":"check_splitgen(split.gen)"},{"path":"https://teazrq.github.io/RLT/reference/check_varw.html","id":null,"dir":"Reference","previous_headings":"","what":"check_varw — check_varw","title":"check_varw — check_varw","text":"check_varw","code":""},{"path":"https://teazrq.github.io/RLT/reference/check_varw.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"check_varw — check_varw","text":"","code":"check_varw(var.w, p)"},{"path":"https://teazrq.github.io/RLT/reference/check_verbose.html","id":null,"dir":"Reference","previous_headings":"","what":"check_verbose — check_verbose","title":"check_verbose — check_verbose","text":"check_verbose","code":""},{"path":"https://teazrq.github.io/RLT/reference/check_verbose.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"check_verbose — check_verbose","text":"","code":"check_verbose(verbose)"},{"path":"https://teazrq.github.io/RLT/reference/cindex.html","id":null,"dir":"Reference","previous_headings":"","what":"C-index — cindex","title":"C-index — cindex","text":"Calculate c-index survival data","code":""},{"path":"https://teazrq.github.io/RLT/reference/cindex.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"C-index — cindex","text":"","code":"cindex(y, censor, pred)"},{"path":"https://teazrq.github.io/RLT/reference/cindex.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"C-index — cindex","text":"y survival time censor censoring indicator survival model used pred predicted value subject","code":""},{"path":"https://teazrq.github.io/RLT/reference/cindex.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"C-index — cindex","text":"c-index","code":""},{"path":"https://teazrq.github.io/RLT/reference/forest.kernel.html","id":null,"dir":"Reference","previous_headings":"","what":" random forest kernel  — forest.kernel","title":" random forest kernel  — forest.kernel","text":"","code":"Get random forest induced kernel weight matrix of testing samples              or between any two sets of data. This is an experimental feature.              Use at your own risk."},{"path":"https://teazrq.github.io/RLT/reference/forest.kernel.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":" random forest kernel  — forest.kernel","text":"","code":"forest.kernel(   object,   X1 = NULL,   X2 = NULL,   vs.train = FALSE,   verbose = FALSE,   ... )"},{"path":"https://teazrq.github.io/RLT/reference/forest.kernel.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":" random forest kernel  — forest.kernel","text":"object fitted RLT object. X1 dataset prediction. calculates \\(n_1 \\times n_1\\) kernel matrix X1. X2 dataset reference/training. X2 supplied, calculate \\(n_1 \\times n_2\\) kernel matrix. vs.train used, must original training data. vs.train calculate kernel weights respect training data. slightly different supplying training data X2 due re-samplings training process. use feature, must specify resample.track = TRUE param.control fitting forest verbose Whether fitting printed. ... ...   Additional arguments.","code":""},{"path":"https://teazrq.github.io/RLT/reference/forest.kernel.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":" random forest kernel  — forest.kernel","text":"kernel matrix contains kernel weights observation X1 respect X1","code":""},{"path":"https://teazrq.github.io/RLT/reference/get.one.tree.html","id":null,"dir":"Reference","previous_headings":"","what":"Print a single tree — get.one.tree","title":"Print a single tree — get.one.tree","text":"Print single fitted tree forest object","code":""},{"path":"https://teazrq.github.io/RLT/reference/get.one.tree.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Print a single tree — get.one.tree","text":"","code":"get.one.tree(x, tree = 1, ...)"},{"path":"https://teazrq.github.io/RLT/reference/get.one.tree.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Print a single tree — get.one.tree","text":"x fitted RLT object tree tree number, starting 1 ntrees. ... ...","code":""},{"path":"https://teazrq.github.io/RLT/reference/get.surv.band.html","id":null,"dir":"Reference","previous_headings":"","what":" get.surv.band  — get.surv.band","title":" get.surv.band  — get.surv.band","text":"","code":"Calculate the survival function (two-sided) confidence band from              a RLT survival prediction."},{"path":"https://teazrq.github.io/RLT/reference/get.surv.band.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":" get.surv.band  — get.surv.band","text":"","code":"get.surv.band(   x,   i = 0,   alpha = 0.05,   approach = \"naive-mc\",   nsim = 1000,   r = 3,   ... )"},{"path":"https://teazrq.github.io/RLT/reference/get.surv.band.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":" get.surv.band  — get.surv.band","text":"x RLT prediction object. must object calculated forest var.ready = TRUE. Observation number prediction. Default calculate (\\(= 0\\)) alpha alpha level interval \\((\\alpha/2, 1 - \\alpha/2)\\) approach approach used calculate confidence band. Can naive-mc: positive-definite projection covariance matrix. confidence band non-smooth smoothed-mc: use smoothed marginal variance perform Monte Carlo approximation critical value. recommended large number time points. smoothed-lr: use smoothed low-rank approximation covariance matrix apply adaptive Bonferroni correction derive critical values. Note approach relies assumption smoothness low rank covariance matrix. nsim number simulations estimating Monte Carlo critical value. Set large number. Default 1000. r maximum number ranks used smoothed-lr approximation. Usually 5 enough approximating covariance matrix due smoothness. ... ...","code":""},{"path":"https://teazrq.github.io/RLT/reference/mytest.html","id":null,"dir":"Reference","previous_headings":"","what":"mytest — mytest","title":"mytest — mytest","text":"function","code":""},{"path":"https://teazrq.github.io/RLT/reference/mytest.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"mytest — mytest","text":"","code":"mytest(n, ...)"},{"path":"https://teazrq.github.io/RLT/reference/mytest.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"mytest — mytest","text":"n n ... arguments","code":""},{"path":"https://teazrq.github.io/RLT/reference/mytest.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"mytest — mytest","text":"output","code":""},{"path":"https://teazrq.github.io/RLT/reference/predict.RLT.html","id":null,"dir":"Reference","previous_headings":"","what":"prediction using RLT — predict.RLT","title":"prediction using RLT — predict.RLT","text":"Predict outcome (regression, classification survival) using fitted RLT object","code":""},{"path":"https://teazrq.github.io/RLT/reference/predict.RLT.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"prediction using RLT — predict.RLT","text":"","code":"# S3 method for class 'RLT' predict(   object,   testx = NULL,   var.est = FALSE,   keep.all = FALSE,   ncores = 1,   verbose = 0,   ... )"},{"path":"https://teazrq.github.io/RLT/reference/predict.RLT.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"prediction using RLT — predict.RLT","text":"object fitted RLT object testx testing samples, must structure training samples var.est Whether estimate variance testing data. original forest must fitted var.ready = TRUE. survival forests, calculates covariance matrix observed time points calculates critical value confidence band. keep.whether keep prediction trees. Warning: can occupy large storage space, especially survival model ncores number cores verbose print additional information ... ...","code":""},{"path":"https://teazrq.github.io/RLT/reference/predict.RLT.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"prediction using RLT — predict.RLT","text":"RLT prediction object, constructed list consisting Prediction Prediction Variance var.est = TRUE fitted object var.ready = TRUE Survival Forests hazard predicted hazard functions CumHazard predicted cumulative hazard function Survival predicted survival function Allhazard keep.= TRUE, predicted hazard function observation tree AllCHF keep.= TRUE, predicted cumulative hazard function observation tree Cov var.est = TRUE fitted object var.ready = TRUE. test subject, matrix size NFail\\(\\times\\)NFail NFail number observed failure times training data Var var.est = TRUE fitted object var.ready = TRUE. Marginal variance subject timepoints ordered observed failure times training data MarginalVar var.est = TRUE fitted object var.ready = TRUE. Marginal variance subject Cov matrix projected nearest positive definite matrix MarginalVarSmooth var.est = TRUE fitted object var.ready = TRUE. Marginal variance subject Cov matrix projected nearest positive definite matrix smoothed using Gaussian kernel smoothing CVproj var.est = TRUE fitted object var.ready = TRUE. Critical values calculate confidence bands around cumulative hazard predictions several confidence levels. Calculated using MarginalVar CVprojSmooth var.est = TRUE fitted object var.ready = TRUE. Critical values calculate confidence bands around cumulative hazard predictions several confidence levels. Calculated using MarginalVarSmooth","code":""},{"path":"https://teazrq.github.io/RLT/reference/print.RLT.html","id":null,"dir":"Reference","previous_headings":"","what":"Print a RLT object — print.RLT","title":"Print a RLT object — print.RLT","text":"Print RLT object","code":""},{"path":"https://teazrq.github.io/RLT/reference/print.RLT.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Print a RLT object — print.RLT","text":"","code":"# S3 method for class 'RLT' print(x, ...)"},{"path":"https://teazrq.github.io/RLT/reference/print.RLT.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Print a RLT object — print.RLT","text":"x fitted RLT object ... ...","code":""},{"path":[]},{"path":"https://teazrq.github.io/RLT/news/index.html","id":"rlt-425","dir":"Changelog","previous_headings":"","what":"RLT 4.2.5","title":"RLT 4.2.5","text":"Completed linear combination splits reinforcement learning mode regression. Single variable reinforcement learning mode classification also done.","code":""},{"path":"https://teazrq.github.io/RLT/news/index.html","id":"rlt-413","dir":"Changelog","previous_headings":"","what":"RLT 4.1.3","title":"RLT 4.1.3","text":"Various updates including reproducibility, new models, speed improvements. Embedded model (RLT) temporally removed. added next version.","code":""},{"path":"https://teazrq.github.io/RLT/news/index.html","id":"rlt-400","dir":"Changelog","previous_headings":"","what":"RLT 4.0.0","title":"RLT 4.0.0","text":"Updated entire package Rcpp Rcpparmadillo.","code":""},{"path":"https://teazrq.github.io/RLT/news/index.html","id":"rlt-323","dir":"Changelog","previous_headings":"","what":"RLT 3.2.3","title":"RLT 3.2.3","text":"CRAN release: 2021-09-04 Removed S.h header file. Changes header files avoid conflict Rinternals.h clang 13.0.0.","code":""},{"path":"https://teazrq.github.io/RLT/news/index.html","id":"rlt-322","dir":"Changelog","previous_headings":"","what":"RLT 3.2.2","title":"RLT 3.2.2","text":"CRAN release: 2018-08-20 Changed variable weights mechanism classification setting.","code":""},{"path":"https://teazrq.github.io/RLT/news/index.html","id":"rlt-321","dir":"Changelog","previous_headings":"","what":"RLT 3.2.1","title":"RLT 3.2.1","text":"CRAN release: 2017-11-19 Fixed compiling issues solaris.","code":""},{"path":"https://teazrq.github.io/RLT/news/index.html","id":"rlt-320","dir":"Changelog","previous_headings":"","what":"RLT 3.2.0","title":"RLT 3.2.0","text":"CRAN release: 2017-11-17 Fixed compiling issues omp.h.","code":""},{"path":"https://teazrq.github.io/RLT/news/index.html","id":"rlt-311","dir":"Changelog","previous_headings":"","what":"RLT 3.1.1","title":"RLT 3.1.1","text":"Fixed small bug survival model, “rank” splitting rule may lead crash.","code":""},{"path":"https://teazrq.github.io/RLT/news/index.html","id":"rlt-310","dir":"Changelog","previous_headings":"","what":"RLT 3.1.0","title":"RLT 3.1.0","text":"CRAN release: 2017-04-09 first release package CRAN. Previous versions (<= 3.0.0) released author’s personal website. minor changes parameter names compared previous versions.","code":""}]
