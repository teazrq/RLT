---
title: "Regression â€” Minimal Tutorial (RLT)"
author: "Ruoqing Zhu"
date: "Last Updated: `r format(Sys.time(), '%B %d, %Y')`"
output:
  html_document:
    code_folding: hide
    df_print: paged
    toc: yes
    toc_float:
      collapsed: true
      smooth_scroll: true
    toc_depth: '2'
  pdf_document:
    toc: yes
    toc_depth: 2
---

```{r setup, include=FALSE}
# Clean, consistent output for all chunks
knitr::opts_chunk$set(
  message = FALSE, warning = FALSE,
  fig.width = 7, fig.height = 5, out.width = "70%", fig.align = "center",
  collapse = TRUE, class.source = "fold-show"
)
```

## Overview

This page shows how to fit and predict a regression model with RLT.

## Prerequisites

See [Get Started](articles/Test-RLT.html) for installation and loading instructions.

## Data

We generate continuous and categorical predictors with a continuous outcome.

```{r data-prep}
# (Optional) For reproducibility in this tutorial only.
# Detailed notes on random seeds are in the Seed & Reproducibility feature page.
set.seed(1)

# ---- Generate a small synthetic dataset ----
trainn <- 80
testn  <- 20
n <- trainn + testn
p <- 10

# Continuous + categorical predictors (last half as factors)
X1 <- matrix(rnorm(n * (p/2)), n, p/2)
X2 <- matrix(as.integer(runif(n * (p/2)) * 3), n, p/2)  # integers 0,1,2

X <- data.frame(X1, X2)
X[, (p/2 + 1):p] <- lapply(X[, (p/2 + 1):p], as.factor)

# Continuous outcome with a simple signal + noise
y <- 1 + rowSums(X[, 2:6]) + 2 * (X[, p/2 + 1] %in% c(1, 3)) + rnorm(n)

# Train / test split
trainX <- X[1:trainn, ]
trainY <- y[1:trainn]
testX  <- X[(trainn + 1):(trainn + testn), ]
testY  <- y[(trainn + 1):(trainn + testn)]
```

## Fit

```{r fit-model}
ntrees <- 200
ncores <- max(1, parallel::detectCores() - 1)
nmin   <- 5
mtry   <- p/2
samplereplace <- TRUE
sampleprob    <- 0.80
rule    <- "best"
nsplit  <- ifelse(rule == "best", 0, 3)
importance <- TRUE

fit <- RLT(
  trainX, trainY, model = "regression",
  ntrees = ntrees, mtry = mtry, nmin = nmin,
  resample.prob = sampleprob, split.gen = rule,
  resample.replace = samplereplace,
  nsplit = nsplit, importance = importance,
  param.control = list(alpha = 0),
  ncores = ncores, verbose = TRUE
)
```

## Predict

```{r predict}
pred <- predict(fit, testX, ncores = ncores)

# Helper in case predict() returns a list with $Prediction
get_pred <- function(obj) if (is.list(obj) && !is.null(obj$Prediction)) obj$Prediction else as.numeric(obj)

train_pred <- if (!is.null(fit$Prediction)) fit$Prediction else get_pred(predict(fit, trainX, ncores = ncores))
test_pred  <- get_pred(pred)
```

## Evaluate

```{r evaluate}
mse_train <- mean((train_pred - trainY)^2)
mse_test  <- mean((test_pred  - testY)^2)

# A compact summary
list(
  Train_MSE = round(mse_train, 4),
  Test_MSE  = round(mse_test, 4)
)
```

## Inspect

```{r inspect}
print(fit)
```
