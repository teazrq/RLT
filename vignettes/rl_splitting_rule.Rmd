---
title: "Reinforcement Learning Splitting Rule â€” Tutorial (RLT)"
author: "Ruoqing Zhu"
date: "Last Updated: `r format(Sys.time(), '%B %d, %Y')`"
output:
  html_document:
    code_folding: hide
    df_print: paged
    toc: yes
    toc_float:
      collapsed: true
      smooth_scroll: true
    toc_depth: '2'
  pdf_document:
    toc: yes
    toc_depth: 2
---

```{r setup, include=FALSE}
# Clean, consistent output for all chunks
knitr::opts_chunk$set(
  message = FALSE, warning = FALSE,
  fig.width = 7, fig.height = 5, out.width = "70%", fig.align = "center",
  collapse = TRUE, class.source = "fold-show"
)
```

## Overview

This page shows how to configure and use the reinforcement learning (RL) splitting rule in RLT.

## Data

We generate continuous and categorical predictors with a binary outcome.

```{r data-prep}
# (Optional) For reproducibility in this tutorial only.
set.seed(1)

# Keep the example compact (~100 obs)
trainn <- 80
testn  <- 20
n <- trainn + testn
p <- 10

# Continuous + categorical predictors (last half as factors)
X1 <- matrix(rnorm(n * (p/2)), n, p/2)
X2 <- matrix(as.integer(runif(n * (p/2)) * 10), n, p/2)  # integers 0-9

X <- data.frame(X1, X2)
X[, (p/2 + 1):p] <- lapply(X[, (p/2 + 1):p], as.factor)

# Binary outcome via a simple logistic signal
logit <- function(x) exp(x) / (1 + exp(x))
prob  <- logit(-0.5 + 2 * X[, 1] + 0.5 * (X[, p] %in% c(1, 3, 5, 7)))
y     <- factor(rbinom(n, 1, prob = prob), levels = c(0, 1))

# Split
trainX <- X[1:trainn, ]; trainY <- y[1:trainn]
testX  <- X[(trainn + 1):(trainn + testn), ]; testY <- y[(trainn + 1):(trainn + testn)]
```

## Fit with RL splitting

Settings mirror your original Rmd: random linear-combination candidates with RL enabled.
We also request distributed variable importance.

```{r fit-model}
# install.packages("devtools"); devtools::install_github("teazrq/RLT")
library(RLT)

# Minimal but sensible defaults (keep consistent with original style)
ntrees <- 500
ncores <- max(1, parallel::detectCores() - 1)
nmin   <- 10
mtry   <- p/2

RLTfit <- RLT(
  trainX, trainY,
  ntrees = ntrees, ncores = ncores, nmin = nmin, mtry = mtry,
  split.gen = "random", nsplit = 2,                   # linear-combination candidates
  resample.prob = 0.8, resample.replace = FALSE,      # sampling settings
  reinforcement = TRUE,                                # <-- RL is ON
  importance = "distribute",
  param.control = list(
    "embed.ntrees" = 50,   # embedded model size
    "embed.mtry"   = 2/3,  # embedded mtry
    "embed.nmin"   = 5,    # embedded min node size
    "alpha"        = 0.1   # regularization parameter as in your examples
  ),
  verbose = TRUE
)
```

## Predict and evaluate

```{r predict-evaluate}
RLTPred <- predict(RLTfit, testX, ncores = ncores)

# Classification error / accuracy (original style using $Prediction)
train_error <- mean(RLTfit$Prediction != trainY)
test_error  <- mean(RLTPred$Prediction != testY)

list(
  Train_Error = round(train_error, 4),
  Test_Error  = round(test_error, 4)
)
```

## Variable importance

```{r importance}
# Distributed assignment importance
barplot(RLTfit$VarImp, main = "RLT Variable Importance (Distributed)")
```

## Inspect one tree

```{r inspect-tree}
# Look at the first tree structure
get.one.tree(RLTfit, 1)
```