---
title: "RLT Regression Quality Control and Validation"
author: "Ruoqing Zhu"
date: "Last Updated: `r format(Sys.time(), '%B %d, %Y')`"
output:
  html_document:
    code_folding: hide
    df_print: paged
    toc: yes
    toc_float:
      collapsed: true
      smooth_scroll: true 
    toc_depth: '2'
  pdf_document:
    toc: yes
    toc_depth: 2
---

```{r set-options, echo=FALSE, cache=FALSE}
  options(width = 1000)
  knitr::opts_chunk$set(fig.width = 9, fig.height = 7,
                        out.width = "75%", fig.align = "center")
  knitr::opts_chunk$set(class.source = "fold-show")
  knitr::opts_chunk$set(collapse = TRUE)
  pkgs <- c("randomForest", "randomForestSRC", "ranger")
  for (p in pkgs) if (!requireNamespace(p, quietly = TRUE)) install.packages(p)
```

## Install and Load Package

Install and load the GitHub version of the RLT package. Do not use the CRAN version. 

```{r}
  # install.packages("devtools")
  # devtools::install_github("teazrq/RLT")
  library(RLT)
```

Load other packages used in this guide.

```{r message=FALSE}
  library(randomForest)
  library(randomForestSRC)
  library(ranger)
```

## Load QC Functions

Define all QC functions directly in this document for comprehensive validation.

```{r}
  # ============================================================================
  # QC Function Definitions
  # ============================================================================
  
  #' @title validate_regression_data
  #' @description Validate input data for regression models
  validate_regression_data <- function(x, y, obs.w = NULL, var.w = NULL, ncat = NULL) {
    
    validation_results <- list(
      status = TRUE,
      errors = character(0),
      warnings = character(0),
      data_info = list()
    )
    
    # Check y is vector
    if (!is.vector(y)) {
      validation_results$status <- FALSE
      validation_results$errors <- c(validation_results$errors, "y must be a vector")
    }
    
    # Check y is numeric
    if (!is.numeric(y)) {
      validation_results$status <- FALSE
      validation_results$errors <- c(validation_results$errors, "y must be numerical for regression")
    }
    
    # Check for NA values
    if (any(is.na(x))) {
      validation_results$status <- FALSE
      validation_results$errors <- c(validation_results$errors, "NA not permitted in x")
    }
    
    if (any(is.na(y))) {
      validation_results$status <- FALSE
      validation_results$errors <- c(validation_results$errors, "NA not permitted in y")
    }
    
    # Check dimensions match
    if (nrow(x) != length(y)) {
      validation_results$status <- FALSE
      validation_results$errors <- c(validation_results$errors, "number of observations does not match: x & y")
    }
    
    # Check observation weights
    if (!is.null(obs.w)) {
      if (length(obs.w) != length(y)) {
        validation_results$status <- FALSE
        validation_results$errors <- c(validation_results$errors, "length of observation weights (obs.w) must match y")
      }
      if (any(obs.w < 0)) {
        validation_results$status <- FALSE
        validation_results$errors <- c(validation_results$errors, "observation weights (obs.w) cannot be negative")
      }
    }
    
    # Check variable weights
    if (!is.null(var.w)) {
      if (length(var.w) != ncol(x)) {
        validation_results$status <- FALSE
        validation_results$errors <- c(validation_results$errors, "length of variable weights (var.w) must match number of columns in x")
      }
      if (any(var.w < 0)) {
        validation_results$status <- FALSE
        validation_results$errors <- c(validation_results$errors, "variable weights (var.w) cannot be negative")
      }
    }
    
    # Check ncat
    if (!is.null(ncat)) {
      if (length(ncat) != ncol(x)) {
        validation_results$status <- FALSE
        validation_results$errors <- c(validation_results$errors, "length of ncat must match number of columns in x")
      }
      if (any(ncat < 0)) {
        validation_results$status <- FALSE
        validation_results$errors <- c(validation_results$errors, "ncat values cannot be negative")
      }
    }
    
    # Data information
    validation_results$data_info <- list(
      n_obs = nrow(x),
      n_vars = ncol(x),
      y_range = range(y, na.rm = TRUE),
      y_mean = mean(y, na.rm = TRUE),
      y_sd = sd(y, na.rm = TRUE),
      x_has_categorical = ifelse(!is.null(ncat), any(ncat > 0), FALSE)
    )
    
    return(validation_results)
  }
  
  #' @title validate_regression_parameters
  #' @description Validate parameters for regression models
  validate_regression_parameters <- function(param, n, p) {
    
    validation_results <- list(
      status = TRUE,
      errors = character(0),
      warnings = character(0),
      parameter_info = list()
    )
    
    # Check ntrees
    if (!is.null(param$ntrees)) {
      if (param$ntrees < 1) {
        validation_results$status <- FALSE
        validation_results$errors <- c(validation_results$errors, "ntrees should be greater than 0")
      }
    }
    
    # Check mtry
    if (!is.null(param$mtry)) {
      if (param$mtry < 1) {
        validation_results$status <- FALSE
        validation_results$errors <- c(validation_results$errors, "mtry cannot be less than 1")
      }
      if (param$mtry > p) {
        validation_results$status <- FALSE
        validation_results$errors <- c(validation_results$errors, "mtry cannot be larger than p")
      }
    }
    
    # Check nmin
    if (!is.null(param$nmin)) {
      if (param$nmin < 1) {
        validation_results$status <- FALSE
        validation_results$errors <- c(validation_results$errors, "nmin cannot be less than 1")
      }
      if (param$nmin > n/2) {
        validation_results$warnings <- c(validation_results$warnings, "nmin is very large relative to sample size")
      }
    }
    
    # Check resample.prob
    if (!is.null(param$resample.prob)) {
      if (param$resample.prob <= 0 || param$resample.prob > 1) {
        validation_results$status <- FALSE
        validation_results$errors <- c(validation_results$errors, "resample.prob should be within the interval (0, 1]")
      }
    }
    
    # Check split.rule for linear combination
    if (!is.null(param$linear.comb) && param$linear.comb > 1) {
      if (!is.null(param$split.rule)) {
        valid_rules <- c("naive", "pca", "lm", "sir")
        if (!(param$split.rule %in% valid_rules)) {
          validation_results$status <- FALSE
          validation_results$errors <- c(validation_results$errors, 
                                       paste("split.rule must be one of:", paste(valid_rules, collapse = ", ")))
        }
      }
    }
    
    # Check linear.comb
    if (!is.null(param$linear.comb)) {
      if (param$linear.comb > p) {
        validation_results$status <- FALSE
        validation_results$errors <- c(validation_results$errors, "linear.comb cannot be larger than p")
      }
      if (param$linear.comb > 5) {
        validation_results$warnings <- c(validation_results$warnings, "very large linear.comb is not recommended")
      }
    }
    
    # Parameter information
    validation_results$parameter_info <- list(
      ntrees = param$ntrees,
      mtry = param$mtry,
      nmin = param$nmin,
      linear_comb = param$linear.comb,
      split_rule = param$split.rule
    )
    
    return(validation_results)
  }
  
  #' @title validate_regression_model
  #' @description Validate fitted regression model
  validate_regression_model <- function(fit, x, y) {
    
    validation_results <- list(
      status = TRUE,
      errors = character(0),
      warnings = character(0),
      model_info = list()
    )
    
    # Check if fit is valid
    if (!inherits(fit, "RLT")) {
      validation_results$status <- FALSE
      validation_results$errors <- c(validation_results$errors, "fit must be an RLT object")
      return(validation_results)
    }
    
    # Check if it's a regression model
    if (!("reg" %in% class(fit))) {
      validation_results$status <- FALSE
      validation_results$errors <- c(validation_results$errors, "fit must be a regression model")
      return(validation_results)
    }
    
    # Check forest structure
    if (is.null(fit$FittedForest)) {
      validation_results$status <- FALSE
      validation_results$errors <- c(validation_results$errors, "FittedForest is missing")
    } else {
      forest <- fit$FittedForest
      
      # Check required forest components
      required_components <- c("SplitVar", "SplitValue", "LeftNode", "RightNode", "NodeWeight", "NodeAve")
      missing_components <- setdiff(required_components, names(forest))
      
      if (length(missing_components) > 0) {
        validation_results$status <- FALSE
        validation_results$errors <- c(validation_results$errors, 
                                     paste("Missing forest components:", paste(missing_components, collapse = ", ")))
      }
      
      # Check number of trees
      if (!is.null(forest$SplitVar)) {
        n_trees <- length(forest$SplitVar)
        if (n_trees == 0) {
          validation_results$status <- FALSE
          validation_results$errors <- c(validation_results$errors, "No trees in forest")
        }
      }
    }
    
    # Check predictions if available
    if (!is.null(fit$Prediction)) {
      if (length(fit$Prediction) != length(y)) {
        validation_results$status <- FALSE
        validation_results$errors <- c(validation_results$errors, "Prediction length does not match y length")
      }
      
      # Check for infinite predictions
      if (any(is.infinite(fit$Prediction))) {
        validation_results$warnings <- c(validation_results$warnings, "Infinite predictions detected")
      }
      
      # Check for NaN predictions
      if (any(is.nan(fit$Prediction))) {
        validation_results$warnings <- c(validation_results$warnings, "NaN predictions detected")
      }
    }
    
    # Check variable importance if available
    if (!is.null(fit$VarImp)) {
      if (length(fit$VarImp) != ncol(x)) {
        validation_results$status <- FALSE
        validation_results$errors <- c(validation_results$errors, "VarImp length does not match number of variables")
      }
      
      # Check for negative importance
      if (any(fit$VarImp < 0)) {
        validation_results$warnings <- c(validation_results$warnings, "Negative variable importance detected")
      }
    }
    
    # Model information
    validation_results$model_info <- list(
      n_trees = ifelse(!is.null(fit$FittedForest$SplitVar), length(fit$FittedForest$SplitVar), NA),
      model_type = ifelse("comb" %in% class(fit), "linear_combination", "single_variable"),
      has_predictions = !is.null(fit$Prediction),
      has_importance = !is.null(fit$VarImp),
      oob_error = ifelse(!is.null(fit$Error), fit$Error, NA)
    )
    
    return(validation_results)
  }
  
  #' @title test_regression_reproducibility
  #' @description Test reproducibility of regression model with same seed
  test_regression_reproducibility <- function(x, y, param, n_tests = 3) {
    
    test_results <- list(
      status = TRUE,
      errors = character(0),
      warnings = character(0),
      reproducibility_info = list()
    )
    
    # Set a fixed seed for testing
    test_seed <- 12345
    param$seed <- test_seed
    
    # Fit first model
    tryCatch({
      fit1 <- RLT(x, y, model = "regression", 
                  ntrees = param$ntrees,
                  mtry = param$mtry,
                  nmin = param$nmin,
                  split.gen = "random",
                  nsplit = param$nsplit,
                  resample.prob = param$resample.prob,
                  resample.replace = param$resample.replace,
                  reinforcement = param$reinforcement,
                  importance = FALSE,
                  verbose = param$verbose,
                  ncores = param$ncores,
                  seed = param$seed)
    }, error = function(e) {
      test_results$status <- FALSE
      test_results$errors <- c(test_results$errors, paste("Error fitting first model:", e$message))
    })
    
    if (!test_results$status) return(test_results)
    
    # Store first model results
    pred1 <- fit1$Prediction
    imp1 <- fit1$VarImp
    error1 <- fit1$Error
    
    # Test reproducibility
    reproducible_predictions <- TRUE
    reproducible_importance <- TRUE
    reproducible_error <- TRUE
    
    for (i in 2:n_tests) {
      tryCatch({
        fit_i <- RLT(x, y, model = "regression", 
                     ntrees = param$ntrees,
                     mtry = param$mtry,
                     nmin = param$nmin,
                     split.gen = "random",
                     nsplit = param$nsplit,
                     resample.prob = param$resample.prob,
                     resample.replace = param$resample.replace,
                     reinforcement = param$reinforcement,
                     importance = FALSE,
                     verbose = param$verbose,
                     ncores = param$ncores,
                     seed = param$seed)
        
        # Check predictions
        if (!all.equal(fit_i$Prediction, pred1, tolerance = 1e-10)) {
          reproducible_predictions <- FALSE
        }
        
        # Check importance
        if (!all.equal(fit_i$VarImp, imp1, tolerance = 1e-10)) {
          reproducible_importance <- FALSE
        }
        
        # Check error
        if (!all.equal(fit_i$Error, error1, tolerance = 1e-10)) {
          reproducible_error <- FALSE
        }
        
      }, error = function(e) {
        test_results$status <- FALSE
        test_results$errors <- c(test_results$errors, paste("Error in reproducibility test", i, ":", e$message))
      })
    }
    
    # Report results
    if (!reproducible_predictions) {
      test_results$warnings <- c(test_results$warnings, "Predictions are not reproducible across runs")
    }
    
    if (!reproducible_importance) {
      test_results$warnings <- c(test_results$warnings, "Variable importance is not reproducible across runs")
    }
    
    if (!reproducible_error) {
      test_results$warnings <- c(test_results$warnings, "OOB error is not reproducible across runs")
    }
    
    test_results$reproducibility_info <- list(
      n_tests = n_tests,
      reproducible_predictions = reproducible_predictions,
      reproducible_importance = reproducible_importance,
      reproducible_error = reproducible_error
    )
    
    return(test_results)
  }
  
  #' @title validate_regression_inputs
  #' @description Quick validation of regression inputs
  validate_regression_inputs <- function(x, y, param) {
    
    cat("=== Input Validation ===\n")
    
    # Check data types
    if (!is.matrix(x)) {
      cat("❌ x must be a matrix\n")
      return(FALSE)
    }
    
    if (!is.vector(y)) {
      cat("❌ y must be a vector\n")
      return(FALSE)
    }
    
    if (!is.numeric(y)) {
      cat("❌ y must be numeric\n")
      return(FALSE)
    }
    
    # Check dimensions
    if (nrow(x) != length(y)) {
      cat("❌ Dimensions don't match: x has", nrow(x), "rows, y has", length(y), "elements\n")
      return(FALSE)
    }
    
    # Check for NA values
    if (any(is.na(x))) {
      cat("❌ x contains NA values\n")
      return(FALSE)
    }
    
    if (any(is.na(y))) {
      cat("❌ y contains NA values\n")
      return(FALSE)
    }
    
    # Check parameters
    if (!is.null(param$mtry) && param$mtry > ncol(x)) {
      cat("❌ mtry (", param$mtry, ") cannot be larger than number of variables (", ncol(x), ")\n")
      return(FALSE)
    }
    
    if (!is.null(param$nmin) && param$nmin < 1) {
      cat("❌ nmin cannot be less than 1\n")
      return(FALSE)
    }
    
    if (!is.null(param$resample.prob) && (param$resample.prob <= 0 || param$resample.prob > 1)) {
      cat("❌ resample.prob must be in (0, 1]\n")
      return(FALSE)
    }
    
    cat("✅ All inputs are valid\n")
    cat("Data: n =", nrow(x), ", p =", ncol(x), "\n")
    cat("Response range:", round(range(y), 3), "\n")
    
    return(TRUE)
  }
  
  #' @title test_regression_edge_cases
  #' @description Test regression model with edge cases
  test_regression_edge_cases <- function(param) {
    
    test_results <- list(
      status = TRUE,
      errors = character(0),
      warnings = character(0),
      edge_case_info = list()
    )
    
    # Test 1: Single observation
    x1 <- matrix(1, 1, 2)
    y1 <- 1
    
    tryCatch({
      fit1 <- RLT(x1, y1, model = "regression", 
                  ntrees = param$ntrees,
                  mtry = 1,
                  nmin = 1,
                  split.gen = "random",
                  nsplit = param$nsplit,
                  resample.prob = param$resample.prob,
                  resample.replace = param$resample.replace,
                  reinforcement = param$reinforcement,
                  importance = FALSE,
                  verbose = param$verbose,
                  ncores = param$ncores,
                  seed = param$seed)
    }, error = function(e) {
      test_results$warnings <- c(test_results$warnings, "Single observation test failed")
    })
    
    # Test 2: Single variable
    x2 <- matrix(rnorm(100), 100, 1)
    y2 <- rnorm(100)
    
    tryCatch({
      fit2 <- RLT(x2, y2, model = "regression", 
                  ntrees = param$ntrees,
                  mtry = 1,
                  nmin = param$nmin,
                  split.gen = "random",
                  nsplit = param$nsplit,
                  resample.prob = param$resample.prob,
                  resample.replace = param$resample.replace,
                  reinforcement = param$reinforcement,
                  importance = FALSE,
                  verbose = param$verbose,
                  ncores = param$ncores,
                  seed = param$seed)
    }, error = function(e) {
      test_results$warnings <- c(test_results$warnings, "Single variable test failed")
    })
    
    # Test 3: Constant response
    x3 <- matrix(rnorm(100), 100, 5)
    y3 <- rep(1, 100)
    
    tryCatch({
      fit3 <- RLT(x3, y3, model = "regression", 
                  ntrees = param$ntrees,
                  mtry = min(3, ncol(x3)),
                  nmin = param$nmin,
                  split.gen = "random",
                  nsplit = param$nsplit,
                  resample.prob = param$resample.prob,
                  resample.replace = param$resample.replace,
                  reinforcement = param$reinforcement,
                  importance = FALSE,
                  verbose = param$verbose,
                  ncores = param$ncores,
                  seed = param$seed)
    }, error = function(e) {
      test_results$warnings <- c(test_results$warnings, "Constant response test failed")
    })
    
    # Test 4: Very small nmin
    param_small_nmin <- param
    param_small_nmin$nmin <- 1
    
    tryCatch({
      fit4 <- RLT(x3, y3, model = "regression", 
                  ntrees = param_small_nmin$ntrees,
                  mtry = min(3, ncol(x3)),
                  nmin = param_small_nmin$nmin,
                  split.gen = "random",
                  nsplit = param_small_nmin$nsplit,
                  resample.prob = param_small_nmin$resample.prob,
                  resample.replace = param_small_nmin$resample.replace,
                  reinforcement = param_small_nmin$reinforcement,
                  importance = FALSE,
                  verbose = param_small_nmin$verbose,
                  ncores = param_small_nmin$ncores,
                  seed = param_small_nmin$seed)
    }, error = function(e) {
      test_results$warnings <- c(test_results$warnings, "Small nmin test failed")
    })
    
    test_results$edge_case_info <- list(
      single_observation = TRUE,
      single_variable = TRUE,
      constant_response = TRUE,
      small_nmin = TRUE
    )
    
    return(test_results)
  }
  
  #' @title comprehensive_regression_qc
  #' @description Comprehensive quality control for regression models
  comprehensive_regression_qc <- function(x, y, param, obs.w = NULL, var.w = NULL, ncat = NULL) {
    
    cat("=== RLT Regression Quality Control Report ===\n\n")
    
    # Initialize overall status
    overall_status <- TRUE
    all_errors <- character(0)
    all_warnings <- character(0)
    
    # 1. Data validation
    cat("1. Data Validation:\n")
    data_validation <- validate_regression_data(x, y, obs.w, var.w, ncat)
    
    if (!data_validation$status) {
      overall_status <- FALSE
      all_errors <- c(all_errors, data_validation$errors)
      cat("   ❌ FAILED\n")
      for (error in data_validation$errors) {
        cat("      -", error, "\n")
      }
    } else {
      cat("   ✅ PASSED\n")
    }
    
    for (warning in data_validation$warnings) {
      all_warnings <- c(all_warnings, warning)
      cat("      ⚠️", warning, "\n")
    }
    
    cat("   Data Info: n =", data_validation$data_info$n_obs, 
        ", p =", data_validation$data_info$n_vars, "\n\n")
    
    # 2. Parameter validation
    cat("2. Parameter Validation:\n")
    param_validation <- validate_regression_parameters(param, nrow(x), ncol(x))
    
    if (!param_validation$status) {
      overall_status <- FALSE
      all_errors <- c(all_errors, param_validation$errors)
      cat("   ❌ FAILED\n")
      for (error in param_validation$errors) {
        cat("      -", error, "\n")
      }
    } else {
      cat("   ✅ PASSED\n")
    }
    
    for (warning in param_validation$warnings) {
      all_warnings <- c(all_warnings, warning)
      cat("      ⚠️", warning, "\n")
    }
    
    cat("\n")
    
    # 3. Model fitting and validation
    cat("3. Model Fitting and Validation:\n")
    
    if (data_validation$status && param_validation$status) {
      tryCatch({
        # Set default values
        if (is.null(obs.w)) obs.w <- rep(1, nrow(x))
        if (is.null(var.w)) var.w <- rep(1, ncol(x))
        if (is.null(ncat)) ncat <- rep(0, ncol(x))
        
        # Fit model
        fit <- RLT(x, y, model = "regression", 
                   ntrees = param$ntrees,
                   mtry = param$mtry,
                   nmin = param$nmin,
                   split.gen = "random",
                   nsplit = param$nsplit,
                   resample.prob = param$resample.prob,
                   resample.replace = param$resample.replace,
                   reinforcement = param$reinforcement,
                   importance = FALSE,
                   verbose = param$verbose,
                   ncores = param$ncores,
                   seed = param$seed)
        
        # Validate fitted model
        model_validation <- validate_regression_model(fit, x, y)
        
        if (!model_validation$status) {
          overall_status <- FALSE
          all_errors <- c(all_errors, model_validation$errors)
          cat("   ❌ FAILED\n")
          for (error in model_validation$errors) {
            cat("      -", error, "\n")
          }
        } else {
          cat("   ✅ PASSED\n")
        }
        
        for (warning in model_validation$warnings) {
          all_warnings <- c(all_warnings, warning)
          cat("      ⚠️", warning, "\n")
        }
        
        cat("   Model Info: Trees =", model_validation$model_info$n_trees,
            ", Type =", model_validation$model_info$model_type, "\n")
        
        if (!is.na(model_validation$model_info$oob_error)) {
          cat("   OOB Error =", round(model_validation$model_info$oob_error, 4), "\n")
        }
        
      }, error = function(e) {
        overall_status <- FALSE
        all_errors <- c(all_errors, paste("Model fitting failed:", e$message))
        cat("   ❌ FAILED -", e$message, "\n")
      })
    } else {
      cat("   ⏭️ SKIPPED (due to previous failures)\n")
    }
    
    cat("\n")
    
    # 4. Reproducibility test
    cat("4. Reproducibility Test:\n")
    
    if (data_validation$status && param_validation$status) {
      repro_test <- test_regression_reproducibility(x, y, param)
      
      if (!repro_test$status) {
        overall_status <- FALSE
        all_errors <- c(all_errors, repro_test$errors)
        cat("   ❌ FAILED\n")
        for (error in repro_test$errors) {
          cat("      -", error, "\n")
        }
      } else {
        cat("   ✅ PASSED\n")
      }
      
      for (warning in repro_test$warnings) {
        all_warnings <- c(all_warnings, warning)
        cat("      ⚠️", warning, "\n")
      }
      
      cat("   Tests run:", repro_test$reproducibility_info$n_tests, "\n")
    } else {
      cat("   ⏭️ SKIPPED (due to previous failures)\n")
    }
    
    cat("\n")
    
    # 5. Edge case test
    cat("5. Edge Case Test:\n")
    edge_test <- test_regression_edge_cases(param)
    
    if (!edge_test$status) {
      overall_status <- FALSE
      all_errors <- c(all_errors, edge_test$errors)
      cat("   ❌ FAILED\n")
      for (error in edge_test$errors) {
        cat("      -", error, "\n")
      }
    } else {
      cat("   ✅ PASSED\n")
    }
    
    for (warning in edge_test$warnings) {
      all_warnings <- c(all_warnings, warning)
      cat("      ⚠️", warning, "\n")
    }
    
    cat("\n")
    
    # Summary
    cat("=== SUMMARY ===\n")
    if (overall_status) {
      cat("✅ Overall Status: PASSED\n")
    } else {
      cat("❌ Overall Status: FAILED\n")
    }
    
    cat("Total Errors:", length(all_errors), "\n")
    cat("Total Warnings:", length(all_warnings), "\n")
    
    if (length(all_errors) > 0) {
      cat("\nErrors:\n")
      for (error in all_errors) {
        cat("-", error, "\n")
      }
    }
    
    if (length(all_warnings) > 0) {
      cat("\nWarnings:\n")
      for (warning in all_warnings) {
        cat("-", warning, "\n")
      }
    }
    
    # Return detailed results
    return(list(
      overall_status = overall_status,
      errors = all_errors,
      warnings = all_warnings,
      data_validation = data_validation,
      param_validation = param_validation,
      model_validation = if(exists("model_validation")) model_validation else NULL,
      reproducibility_test = if(exists("repro_test")) repro_test else NULL,
      edge_case_test = edge_test
    ))
  }
  
  cat("✅ All QC functions loaded successfully!\n")
```

## Data Generation and Setup

We generate a comprehensive dataset with mixed continuous and categorical variables for thorough QC testing.

```{r}
  library(parallel)
  # Set seed for reproducibility
  set.seed(1)

  # Define data size - similar to Test-Reg.Rmd
  trainn <- 800
  testn <- 200
  n <- trainn + testn
  p <- 30

  # Generate continuous variables (X1) and categorical variables (X2)
  X1 <- matrix(rnorm(n * p / 2), n, p / 2)
  X2 <- matrix(as.integer(runif(n * p / 2) * 3), n, p / 2)

  # Combine continuous and categorical variables into a data frame (X)
  X <- data.frame(X1, X2)

  # Convert the second half of the columns in X to factors
  X[, (p / 2 + 1):p] <- lapply(X[, (p / 2 + 1):p], as.factor)

  # Generate outcomes (y) - true model depends on first 5 continuous + 1 categorical
  y <- 1 + rowSums(X[, 2:6]) + 2 * (X[, p / 2 + 1] %in% c(1, 3)) + rnorm(n)

  # Set tuning parameters
  ntrees <- 500  # Reduced for QC testing
  ncores <- detectCores() - 1
  nmin <- 30
  mtry <- p / 2
  samplereplace <- TRUE
  sampleprob <- 0.80
  rule <- "best"
  nsplit <- ifelse(rule == "best", 0, 3)
  importance <- TRUE

  # Split data into training and testing sets
  trainX <- X[1:trainn, ]
  trainY <- y[1:trainn]
  testX <- X[(trainn + 1):(trainn + testn), ]
  testY <- y[(trainn + 1):(trainn + testn)]
```

## QC Test 1: Comprehensive Data and Parameter Validation

First, let's define all QC functions and run the comprehensive QC validation.

```{r}
  # ============================================================================
  # QC Function Definitions
  # ============================================================================
  
  #' @title validate_regression_data
  #' @description Validate input data for regression models
  validate_regression_data <- function(x, y, obs.w = NULL, var.w = NULL, ncat = NULL) {
    
    validation_results <- list(
      status = TRUE,
      errors = character(0),
      warnings = character(0),
      data_info = list()
    )
    
    # Check y is vector
    if (!is.vector(y)) {
      validation_results$status <- FALSE
      validation_results$errors <- c(validation_results$errors, "y must be a vector")
    }
    
    # Check y is numeric
    if (!is.numeric(y)) {
      validation_results$status <- FALSE
      validation_results$errors <- c(validation_results$errors, "y must be numerical for regression")
    }
    
    # Check for NA values
    if (any(is.na(x))) {
      validation_results$status <- FALSE
      validation_results$errors <- c(validation_results$errors, "NA not permitted in x")
    }
    
    if (any(is.na(y))) {
      validation_results$status <- FALSE
      validation_results$errors <- c(validation_results$errors, "NA not permitted in y")
    }
    
    # Check dimensions match
    if (nrow(x) != length(y)) {
      validation_results$status <- FALSE
      validation_results$errors <- c(validation_results$errors, "number of observations does not match: x & y")
    }
    
    # Check observation weights
    if (!is.null(obs.w)) {
      if (length(obs.w) != length(y)) {
        validation_results$status <- FALSE
        validation_results$errors <- c(validation_results$errors, "length of observation weights (obs.w) must match y")
      }
      if (any(obs.w < 0)) {
        validation_results$status <- FALSE
        validation_results$errors <- c(validation_results$errors, "observation weights (obs.w) cannot be negative")
      }
    }
    
    # Check variable weights
    if (!is.null(var.w)) {
      if (length(var.w) != ncol(x)) {
        validation_results$status <- FALSE
        validation_results$errors <- c(validation_results$errors, "length of variable weights (var.w) must match number of columns in x")
      }
      if (any(var.w < 0)) {
        validation_results$status <- FALSE
        validation_results$errors <- c(validation_results$errors, "variable weights (var.w) cannot be negative")
      }
    }
    
    # Check ncat
    if (!is.null(ncat)) {
      if (length(ncat) != ncol(x)) {
        validation_results$status <- FALSE
        validation_results$errors <- c(validation_results$errors, "length of ncat must match number of columns in x")
      }
      if (any(ncat < 0)) {
        validation_results$status <- FALSE
        validation_results$errors <- c(validation_results$errors, "ncat values cannot be negative")
      }
    }
    
    # Data information
    validation_results$data_info <- list(
      n_obs = nrow(x),
      n_vars = ncol(x),
      y_range = range(y, na.rm = TRUE),
      y_mean = mean(y, na.rm = TRUE),
      y_sd = sd(y, na.rm = TRUE),
      x_has_categorical = ifelse(!is.null(ncat), any(ncat > 0), FALSE)
    )
    
    return(validation_results)
  }
  
  #' @title validate_regression_parameters
  #' @description Validate parameters for regression models
  validate_regression_parameters <- function(param, n, p) {
    
    validation_results <- list(
      status = TRUE,
      errors = character(0),
      warnings = character(0),
      parameter_info = list()
    )
    
    # Check ntrees
    if (!is.null(param$ntrees)) {
      if (param$ntrees < 1) {
        validation_results$status <- FALSE
        validation_results$errors <- c(validation_results$errors, "ntrees should be greater than 0")
      }
    }
    
    # Check mtry
    if (!is.null(param$mtry)) {
      if (param$mtry < 1) {
        validation_results$status <- FALSE
        validation_results$errors <- c(validation_results$errors, "mtry cannot be less than 1")
      }
      if (param$mtry > p) {
        validation_results$status <- FALSE
        validation_results$errors <- c(validation_results$errors, "mtry cannot be larger than p")
      }
    }
    
    # Check nmin
    if (!is.null(param$nmin)) {
      if (param$nmin < 1) {
        validation_results$status <- FALSE
        validation_results$errors <- c(validation_results$errors, "nmin cannot be less than 1")
      }
      if (param$nmin > n/2) {
        validation_results$warnings <- c(validation_results$warnings, "nmin is very large relative to sample size")
      }
    }
    
    # Check resample.prob
    if (!is.null(param$resample.prob)) {
      if (param$resample.prob <= 0 || param$resample.prob > 1) {
        validation_results$status <- FALSE
        validation_results$errors <- c(validation_results$errors, "resample.prob should be within the interval (0, 1]")
      }
    }
    
    # Parameter information
    validation_results$parameter_info <- list(
      ntrees = param$ntrees,
      mtry = param$mtry,
      nmin = param$nmin
    )
    
    return(validation_results)
  }
  
  #' @title validate_regression_model
  #' @description Validate fitted regression model
  validate_regression_model <- function(fit, x, y) {
    
    validation_results <- list(
      status = TRUE,
      errors = character(0),
      warnings = character(0),
      model_info = list()
    )
    
    # Check if fit is valid
    if (!inherits(fit, "RLT")) {
      validation_results$status <- FALSE
      validation_results$errors <- c(validation_results$errors, "fit must be an RLT object")
      return(validation_results)
    }
    
    # Check if it's a regression model
    if (!("reg" %in% class(fit))) {
      validation_results$status <- FALSE
      validation_results$errors <- c(validation_results$errors, "fit must be a regression model")
      return(validation_results)
    }
    
    # Check forest structure
    if (is.null(fit$FittedForest)) {
      validation_results$status <- FALSE
      validation_results$errors <- c(validation_results$errors, "FittedForest is missing")
    } else {
      forest <- fit$FittedForest
      
      # Check required forest components
      required_components <- c("SplitVar", "SplitValue", "LeftNode", "RightNode", "NodeWeight", "NodeAve")
      missing_components <- setdiff(required_components, names(forest))
      
      if (length(missing_components) > 0) {
        validation_results$status <- FALSE
        validation_results$errors <- c(validation_results$errors, 
                                     paste("Missing forest components:", paste(missing_components, collapse = ", ")))
      }
      
      # Check number of trees
      if (!is.null(forest$SplitVar)) {
        n_trees <- length(forest$SplitVar)
        if (n_trees == 0) {
          validation_results$status <- FALSE
          validation_results$errors <- c(validation_results$errors, "No trees in forest")
        }
      }
    }
    
    # Check predictions if available
    if (!is.null(fit$Prediction)) {
      if (length(fit$Prediction) != length(y)) {
        validation_results$status <- FALSE
        validation_results$errors <- c(validation_results$errors, "Prediction length does not match y length")
      }
      
      # Check for infinite predictions
      if (any(is.infinite(fit$Prediction))) {
        validation_results$warnings <- c(validation_results$warnings, "Infinite predictions detected")
      }
      
      # Check for NaN predictions
      if (any(is.nan(fit$Prediction))) {
        validation_results$warnings <- c(validation_results$warnings, "NaN predictions detected")
      }
    }
    
    # Check variable importance if available
    if (!is.null(fit$VarImp)) {
      if (length(fit$VarImp) != ncol(x)) {
        validation_results$status <- FALSE
        validation_results$errors <- c(validation_results$errors, "VarImp length does not match number of variables")
      }
      
      # Check for negative importance
      if (any(fit$VarImp < 0)) {
        validation_results$warnings <- c(validation_results$warnings, "Negative variable importance detected")
      }
    }
    
    # Model information
    validation_results$model_info <- list(
      n_trees = ifelse(!is.null(fit$FittedForest$SplitVar), length(fit$FittedForest$SplitVar), NA),
      model_type = ifelse("comb" %in% class(fit), "linear_combination", "single_variable"),
      has_predictions = !is.null(fit$Prediction),
      has_importance = !is.null(fit$VarImp),
      oob_error = ifelse(!is.null(fit$Error), fit$Error, NA)
    )
    
    return(validation_results)
  }
  
  #' @title test_regression_reproducibility
  #' @description Test reproducibility of regression model with same seed
  test_regression_reproducibility <- function(x, y, param, n_tests = 3) {
    
    test_results <- list(
      status = TRUE,
      errors = character(0),
      warnings = character(0),
      reproducibility_info = list()
    )
    
    # Set a fixed seed for testing
    test_seed <- 12345
    param$seed <- test_seed
    
    # Fit first model
    tryCatch({
      fit1 <- RLT(x, y, model = "regression", 
                  ntrees = param$ntrees,
                  mtry = param$mtry,
                  nmin = param$nmin,
                  split.gen = "random",
                  nsplit = param$nsplit,
                  resample.prob = param$resample.prob,
                  resample.replace = param$resample.replace,
                  reinforcement = param$reinforcement,
                  importance = FALSE,
                  verbose = param$verbose,
                  ncores = param$ncores,
                  seed = param$seed)
    }, error = function(e) {
      test_results$status <- FALSE
      test_results$errors <- c(test_results$errors, paste("Error fitting first model:", e$message))
    })
    
    if (!test_results$status) return(test_results)
    
    # Store first model results
    pred1 <- fit1$Prediction
    imp1 <- fit1$VarImp
    error1 <- fit1$Error
    
    # Test reproducibility
    reproducible_predictions <- TRUE
    reproducible_importance <- TRUE
    reproducible_error <- TRUE
    
    for (i in 2:n_tests) {
      tryCatch({
        fit_i <- RLT(x, y, model = "regression", 
                     ntrees = param$ntrees,
                     mtry = param$mtry,
                     nmin = param$nmin,
                     split.gen = "random",
                     nsplit = param$nsplit,
                     resample.prob = param$resample.prob,
                     resample.replace = param$resample.replace,
                     reinforcement = param$reinforcement,
                     importance = FALSE,
                     verbose = param$verbose,
                     ncores = param$ncores,
                     seed = param$seed)
        
        # Check predictions
        if (!all.equal(fit_i$Prediction, pred1, tolerance = 1e-10)) {
          reproducible_predictions <- FALSE
        }
        
        # Check importance
        if (!all.equal(fit_i$VarImp, imp1, tolerance = 1e-10)) {
          reproducible_importance <- FALSE
        }
        
        # Check error
        if (!all.equal(fit_i$Error, error1, tolerance = 1e-10)) {
          reproducible_error <- FALSE
        }
        
      }, error = function(e) {
        test_results$status <- FALSE
        test_results$errors <- c(test_results$errors, paste("Error in reproducibility test", i, ":", e$message))
      })
    }
    
    # Report results
    if (!reproducible_predictions) {
      test_results$warnings <- c(test_results$warnings, "Predictions are not reproducible across runs")
    }
    
    if (!reproducible_importance) {
      test_results$warnings <- c(test_results$warnings, "Variable importance is not reproducible across runs")
    }
    
    if (!reproducible_error) {
      test_results$warnings <- c(test_results$warnings, "OOB error is not reproducible across runs")
    }
    
    test_results$reproducibility_info <- list(
      n_tests = n_tests,
      reproducible_predictions = reproducible_predictions,
      reproducible_importance = reproducible_importance,
      reproducible_error = reproducible_error
    )
    
    return(test_results)
  }
  
  #' @title test_regression_edge_cases
  #' @description Test regression model with edge cases
  test_regression_edge_cases <- function(param) {
    
    test_results <- list(
      status = TRUE,
      errors = character(0),
      warnings = character(0),
      edge_case_info = list()
    )
    
    # Test 1: Single observation
    x1 <- matrix(1, 1, 2)
    y1 <- 1
    
    tryCatch({
      fit1 <- RLT(x1, y1, model = "regression", 
                  ntrees = param$ntrees,
                  mtry = 1,
                  nmin = 1,
                  split.gen = "random",
                  nsplit = param$nsplit,
                  resample.prob = param$resample.prob,
                  resample.replace = param$resample.replace,
                  reinforcement = param$reinforcement,
                  importance = FALSE,
                  verbose = param$verbose,
                  ncores = param$ncores,
                  seed = param$seed)
    }, error = function(e) {
      test_results$warnings <- c(test_results$warnings, "Single observation test failed")
    })
    
    # Test 2: Single variable
    x2 <- matrix(rnorm(100), 100, 1)
    y2 <- rnorm(100)
    
    tryCatch({
      fit2 <- RLT(x2, y2, model = "regression", 
                  ntrees = param$ntrees,
                  mtry = 1,
                  nmin = param$nmin,
                  split.gen = "random",
                  nsplit = param$nsplit,
                  resample.prob = param$resample.prob,
                  resample.replace = param$resample.replace,
                  reinforcement = param$reinforcement,
                  importance = FALSE,
                  verbose = param$verbose,
                  ncores = param$ncores,
                  seed = param$seed)
    }, error = function(e) {
      test_results$warnings <- c(test_results$warnings, "Single variable test failed")
    })
    
    # Test 3: Constant response
    x3 <- matrix(rnorm(100), 100, 5)
    y3 <- rep(1, 100)
    
    tryCatch({
      fit3 <- RLT(x3, y3, model = "regression", 
                  ntrees = param$ntrees,
                  mtry = min(3, ncol(x3)),
                  nmin = param$nmin,
                  split.gen = "random",
                  nsplit = param$nsplit,
                  resample.prob = param$resample.prob,
                  resample.replace = param$resample.replace,
                  reinforcement = param$reinforcement,
                  importance = FALSE,
                  verbose = param$verbose,
                  ncores = param$ncores,
                  seed = param$seed)
    }, error = function(e) {
      test_results$warnings <- c(test_results$warnings, "Constant response test failed")
    })
    
    test_results$edge_case_info <- list(
      single_observation = TRUE,
      single_variable = TRUE,
      constant_response = TRUE
    )
    
    return(test_results)
  }
  
  #' @title comprehensive_regression_qc
  #' @description Comprehensive quality control for regression models
  comprehensive_regression_qc <- function(x, y, param, obs.w = NULL, var.w = NULL, ncat = NULL) {
    
    cat("=== RLT Regression Quality Control Report ===\n\n")
    
    # Initialize overall status
    overall_status <- TRUE
    all_errors <- character(0)
    all_warnings <- character(0)
    
    # 1. Data validation
    cat("1. Data Validation:\n")
    data_validation <- validate_regression_data(x, y, obs.w, var.w, ncat)
    
    if (!data_validation$status) {
      overall_status <- FALSE
      all_errors <- c(all_errors, data_validation$errors)
      cat("   ❌ FAILED\n")
      for (error in data_validation$errors) {
        cat("      -", error, "\n")
      }
    } else {
      cat("   ✅ PASSED\n")
    }
    
    for (warning in data_validation$warnings) {
      all_warnings <- c(all_warnings, warning)
      cat("      ⚠️", warning, "\n")
    }
    
    cat("   Data Info: n =", data_validation$data_info$n_obs, 
        ", p =", data_validation$data_info$n_vars, "\n\n")
    
    # 2. Parameter validation
    cat("2. Parameter Validation:\n")
    param_validation <- validate_regression_parameters(param, nrow(x), ncol(x))
    
    if (!param_validation$status) {
      overall_status <- FALSE
      all_errors <- c(all_errors, param_validation$errors)
      cat("   ❌ FAILED\n")
      for (error in param_validation$errors) {
        cat("      -", error, "\n")
      }
    } else {
      cat("   ✅ PASSED\n")
    }
    
    for (warning in param_validation$warnings) {
      all_warnings <- c(all_warnings, warning)
      cat("      ⚠️", warning, "\n")
    }
    
    cat("\n")
    
    # 3. Model fitting and validation
    cat("3. Model Fitting and Validation:\n")
    
    if (data_validation$status && param_validation$status) {
      tryCatch({
        # Set default values
        if (is.null(obs.w)) obs.w <- rep(1, nrow(x))
        if (is.null(var.w)) var.w <- rep(1, ncol(x))
        if (is.null(ncat)) ncat <- rep(0, ncol(x))
        
        # Fit model
        fit <- RLT(x, y, model = "regression", 
                   ntrees = param$ntrees,
                   mtry = param$mtry,
                   nmin = param$nmin,
                   split.gen = "random",
                   nsplit = param$nsplit,
                   resample.prob = param$resample.prob,
                   resample.replace = param$resample.replace,
                   reinforcement = param$reinforcement,
                   importance = FALSE,
                   verbose = param$verbose,
                   ncores = param$ncores,
                   seed = param$seed)
        
        # Validate fitted model
        model_validation <- validate_regression_model(fit, x, y)
        
        if (!model_validation$status) {
          overall_status <- FALSE
          all_errors <- c(all_errors, model_validation$errors)
          cat("   ❌ FAILED\n")
          for (error in model_validation$errors) {
            cat("      -", error, "\n")
          }
        } else {
          cat("   ✅ PASSED\n")
        }
        
        for (warning in model_validation$warnings) {
          all_warnings <- c(all_warnings, warning)
          cat("      ⚠️", warning, "\n")
        }
        
        cat("   Model Info: Trees =", model_validation$model_info$n_trees,
            ", Type =", model_validation$model_info$model_type, "\n")
        
        if (!is.na(model_validation$model_info$oob_error)) {
          cat("   OOB Error =", round(model_validation$model_info$oob_error, 4), "\n")
        }
        
      }, error = function(e) {
        overall_status <- FALSE
        all_errors <- c(all_errors, paste("Model fitting failed:", e$message))
        cat("   ❌ FAILED -", e$message, "\n")
      })
    } else {
      cat("   ⏭️ SKIPPED (due to previous failures)\n")
    }
    
    cat("\n")
    
    # 4. Reproducibility test
    cat("4. Reproducibility Test:\n")
    
    if (data_validation$status && param_validation$status) {
      repro_test <- test_regression_reproducibility(x, y, param)
      
      if (!repro_test$status) {
        overall_status <- FALSE
        all_errors <- c(all_errors, repro_test$errors)
        cat("   ❌ FAILED\n")
        for (error in repro_test$errors) {
          cat("      -", error, "\n")
        }
      } else {
        cat("   ✅ PASSED\n")
      }
      
      for (warning in repro_test$warnings) {
        all_warnings <- c(all_warnings, warning)
        cat("      ⚠️", warning, "\n")
      }
      
      cat("   Tests run:", repro_test$reproducibility_info$n_tests, "\n")
    } else {
      cat("   ⏭️ SKIPPED (due to previous failures)\n")
    }
    
    cat("\n")
    
    # 5. Edge case test
    cat("5. Edge Case Test:\n")
    edge_test <- test_regression_edge_cases(param)
    
    if (!edge_test$status) {
      overall_status <- FALSE
      all_errors <- c(all_errors, edge_test$errors)
      cat("   ❌ FAILED\n")
      for (error in edge_test$errors) {
        cat("      -", error, "\n")
      }
    } else {
      cat("   ✅ PASSED\n")
    }
    
    for (warning in edge_test$warnings) {
      all_warnings <- c(all_warnings, warning)
      cat("      ⚠️", warning, "\n")
    }
    
    cat("\n")
    
    # Summary
    cat("=== SUMMARY ===\n")
    if (overall_status) {
      cat("✅ Overall Status: PASSED\n")
    } else {
      cat("❌ Overall Status: FAILED\n")
    }
    
    cat("Total Errors:", length(all_errors), "\n")
    cat("Total Warnings:", length(all_warnings), "\n")
    
    if (length(all_errors) > 0) {
      cat("\nErrors:\n")
      for (error in all_errors) {
        cat("-", error, "\n")
      }
    }
    
    if (length(all_warnings) > 0) {
      cat("\nWarnings:\n")
      for (warning in all_warnings) {
        cat("-", warning, "\n")
      }
    }
    
    # Return detailed results
    return(list(
      overall_status = overall_status,
      errors = all_errors,
      warnings = all_warnings,
      data_validation = data_validation,
      param_validation = param_validation,
      model_validation = if(exists("model_validation")) model_validation else NULL,
      reproducibility_test = if(exists("repro_test")) repro_test else NULL,
      edge_case_test = edge_test
    ))
  }
  
  #' @title validate_regression_inputs
  #' @description Quick validation of regression inputs
  validate_regression_inputs <- function(x, y, param) {
    
    cat("=== Input Validation ===\n")
    
    # Check data types
    if (!is.matrix(x)) {
      cat("❌ x must be a matrix\n")
      return(FALSE)
    }
    
    if (!is.vector(y)) {
      cat("❌ y must be a vector\n")
      return(FALSE)
    }
    
    if (!is.numeric(y)) {
      cat("❌ y must be numeric\n")
      return(FALSE)
    }
    
    # Check dimensions
    if (nrow(x) != length(y)) {
      cat("❌ Dimensions don't match: x has", nrow(x), "rows, y has", length(y), "elements\n")
      return(FALSE)
    }
    
    # Check for NA values
    if (any(is.na(x))) {
      cat("❌ x contains NA values\n")
      return(FALSE)
    }
    
    if (any(is.na(y))) {
      cat("❌ y contains NA values\n")
      return(FALSE)
    }
    
    # Check parameters
    if (!is.null(param$mtry) && param$mtry > ncol(x)) {
      cat("❌ mtry (", param$mtry, ") cannot be larger than number of variables (", ncol(x), ")\n")
      return(FALSE)
    }
    
    if (!is.null(param$nmin) && param$nmin < 1) {
      cat("❌ nmin cannot be less than 1\n")
      return(FALSE)
    }
    
    if (!is.null(param$resample.prob) && (param$resample.prob <= 0 || param$resample.prob > 1)) {
      cat("❌ resample.prob must be in (0, 1]\n")
      return(FALSE)
    }
    
    cat("✅ All inputs are valid\n")
    cat("Data: n =", nrow(x), ", p =", ncol(x), "\n")
    cat("Response range:", round(range(y), 3), "\n")
    
    return(TRUE)
  }
  
  cat("✅ All QC functions defined successfully!\n")
  
  # Now run the comprehensive QC validation
  cat("\n=== Comprehensive QC Validation ===\n")
  
  # Prepare parameters for QC validation
  param_qc <- list(
    ntrees = ntrees,
    mtry = mtry,
    nmin = nmin,
    split.gen = rule,
    nsplit = nsplit,
    resample.prob = sampleprob,
    resample.replace = samplereplace,
    reinforcement = FALSE,
    importance = importance,
    verbose = FALSE,
    ncores = ncores,
    seed = 123
  )
  
  # Run comprehensive QC validation
  qc_results <- comprehensive_regression_qc(trainX, trainY, param_qc)
  
  # Display summary
  cat("\n=== QC Validation Summary ===\n")
  cat("Overall Status:", ifelse(qc_results$overall_status, "PASSED", "FAILED"), "\n")
  cat("Total Errors:", length(qc_results$errors), "\n")
  cat("Total Warnings:", length(qc_results$warnings), "\n")
  
  if (length(qc_results$errors) > 0) {
    cat("\nErrors found:\n")
    for (error in qc_results$errors) {
      cat("-", error, "\n")
    }
  }
  
  if (length(qc_results$warnings) > 0) {
    cat("\nWarnings found:\n")
    for (warning in qc_results$warnings) {
      cat("-", warning, "\n")
    }
  }
```

## QC Test 2: Basic Functionality and Performance Benchmark

Test basic regression functionality and compare performance with other packages.

```{r class.source = NULL}
  # recording results
  metric = data.frame(matrix(NA, 4, 6))
  rownames(metric) = c("RLT", "randomForestSRC", "randomForest", "ranger")
  colnames(metric) = c("fit.time", "pred.time", "oob.error", "pred.error",
                       "obj.size", "tree.size")
  
  # using RLT package 
  start_time <- Sys.time()
  RLTfit <- RLT(trainX, trainY, model = "regression", 
                ntrees = ntrees, mtry = mtry, nmin = nmin, 
                resample.prob = sampleprob, split.gen = rule, 
                resample.replace = samplereplace, 
                nsplit = nsplit, importance = importance, 
                param.control = list("alpha" = 0),
                ncores = ncores, verbose = FALSE)
  metric[1, 1] = difftime(Sys.time(), start_time, units = "secs")
  start_time <- Sys.time()
  RLTPred <- predict(RLTfit, testX, ncores = ncores)
  metric[1, 2] = difftime(Sys.time(), start_time, units = "secs")
  metric[1, 3] = mean((RLTfit$Prediction - trainY)^2)
  metric[1, 4] = mean((RLTPred$Prediction - testY)^2)
  metric[1, 5] = object.size(RLTfit)
  metric[1, 6] = mean(unlist(lapply(RLTfit$FittedForest$SplitVar, length)))
  
  # use randomForestSRC
  options(rf.cores = ncores)
  start_time <- Sys.time()
  rsffit <- rfsrc(y ~ ., data = data.frame(trainX, "y"= trainY), 
                  ntree = ntrees, nodesize = nmin/2, mtry = mtry, 
                  samptype = ifelse(samplereplace == TRUE, "swor", "swr"),
                  nsplit = nsplit, sampsize = trainn*sampleprob, 
                  importance = ifelse(importance, "permute", "none"))
  metric[2, 1] = difftime(Sys.time(), start_time, units = "secs")
  start_time <- Sys.time()
  rsfpred = predict(rsffit, data.frame(testX))
  metric[2, 2] = difftime(Sys.time(), start_time, units = "secs")
  metric[2, 3] = mean((rsffit$predicted.oob - trainY)^2)
  metric[2, 4] = mean((rsfpred$predicted - testY)^2)
  metric[2, 5] = object.size(rsffit)
  metric[2, 6] = rsffit$forest$totalNodeCount / rsffit$ntree
  
  # use randomForest
  start_time <- Sys.time()
  rf.fit <- randomForest(trainX, trainY, ntree = ntrees, 
                         mtry = mtry, nodesize = nmin, 
                         replace = samplereplace,
                         sampsize = trainn*sampleprob, 
                         importance = importance)
  metric[3, 1] = difftime(Sys.time(), start_time, units = "secs")
  start_time <- Sys.time()
  rf.pred <- predict(rf.fit, testX)
  metric[3, 2] = difftime(Sys.time(), start_time, units = "secs")
  metric[3, 3] = mean((rf.fit$predicted - trainY)^2)
  metric[3, 4] = mean((rf.pred - testY)^2)
  metric[3, 5] = object.size(rf.fit)
  metric[3, 6] = mean(colSums(rf.fit$forest$nodestatus != 0))
  
  # use ranger  
  start_time <- Sys.time()
  rangerfit <- ranger(trainY ~ ., data = data.frame(trainX), 
                      num.trees = ntrees, min.node.size = nmin, 
                      mtry = mtry, num.threads = ncores, 
                      replace = samplereplace,
                      sample.fraction = sampleprob, 
                      importance = "permutation",
                      respect.unordered.factors = "partition")
  metric[4, 1] = difftime(Sys.time(), start_time, units = "secs")
  start_time <- Sys.time()
  rangerpred = predict(rangerfit, data.frame(testX))
  metric[4, 2] = difftime(Sys.time(), start_time, units = "secs")
  metric[4, 3] = mean((rangerfit$predictions - trainY)^2)
  metric[4, 4] = mean((rangerpred$predictions - testY)^2)
  metric[4, 5] = object.size(rangerfit)
  metric[4, 6] = mean(unlist(lapply(rangerfit$forest$split.varIDs, length)))
  
  # performance summary
  cat("=== Performance Benchmark Results ===\n")
  print(metric)
```

## QC Test 3: Variable Importance Validation

Test different variable importance methods and compare with other packages.

```{r class.source = NULL, out.width="90%", fig.width=15, fig.height=7}
  par(mfrow=c(2,2))
  par(mar = c(1, 2, 2, 2))
  
  barplot(as.vector(RLTfit$VarImp), main = "RLT Permutation")
  barplot(as.vector(rsffit$importance), main = "randomForestSRC")
  barplot(rf.fit$importance[, 1], main = "randomForest")
  barplot(as.vector(rangerfit$variable.importance), main = "ranger")
```

Test distributed importance method:

```{r class.source = NULL, out.width="90%", fig.width=21.5, fig.height=3.5}
  # using RLT package with distribute importance
  RLTfit_dist <- RLT(trainX, trainY, model = "regression", 
                     ntrees = ntrees, mtry = mtry, nmin = nmin, 
                     split.gen = rule, nsplit = nsplit, 
                     resample.prob = (trainn - 1)/trainn, 
                     resample.replace = FALSE, 
                     importance = "distribute", 
                     ncores = ncores, 
                     verbose = FALSE, 
                     param.control = list("resample.track" = TRUE))
  
  # use randomForestSRC with different importance methods
  rsffit_rand <- rfsrc(y ~ ., data = data.frame(trainX, "y"= trainY), 
                       ntree = ntrees, nodesize = nmin/2, mtry = mtry, 
                       samptype = "swor", nsplit = nsplit, 
                       sampsize = trainn - 1, 
                       importance = "random")

  rsffit_anti <- rfsrc(y ~ ., data = data.frame(trainX, "y"= trainY), 
                       ntree = ntrees, nodesize = nmin/2, mtry = mtry, 
                       samptype = "swor", nsplit = nsplit, 
                       sampsize = trainn - 1, 
                       importance = "anti")
  
  par(mfrow=c(1,3))
  par(mar = c(1, 2, 2, 2))
  
  barplot(as.vector(RLTfit_dist$VarImp), main = "RLT distribute")
  barplot(as.vector(rsffit_rand$importance), main = "rsf random")
  barplot(as.vector(rsffit_anti$importance), main = "rsf anti")
```

## QC Test 4: Model Structure Validation

Check the structure of fitted models and individual trees.

```{r}
  # Check model class and structure
  cat("=== Model Structure Validation ===\n")
  cat("RLT model class:", class(RLTfit), "\n")
  cat("Is regression model:", "reg" %in% class(RLTfit), "\n")
  cat("Number of trees:", length(RLTfit$FittedForest$SplitVar), "\n")
  cat("Model has predictions:", !is.null(RLTfit$Prediction), "\n")
  cat("Model has variable importance:", !is.null(RLTfit$VarImp), "\n")
  cat("OOB error:", round(mean((RLTfit$Prediction - trainY)^2), 4), "\n")
  
  # Print a single tree structure
  cat("\n=== Single Tree Structure ===\n")
  tree1 <- get.one.tree(RLTfit, 1)
  print(tree1)
```

## QC Test 5: Linear Combination Regression

Test linear combination regression functionality.

```{r}
  cat("=== Linear Combination Regression Test ===\n")
  
  # Test linear combination with naive method
  RLTfit_comb <- RLT(trainX, trainY, model = "regression", 
                     ntrees = 200, mtry = min(p, 10), nmin = 20,
                     split.gen = "random", nsplit = 2,
                     resample.prob = 0.9, resample.replace = FALSE, 
                     param.control = list("linear.comb" = 3,
                                         "split.rule" = "naive",
                                         "embed.ntrees" = 50,
                                         "embed.mtry" = 0.5,
                                         "embed.nmin" = 5,
                                         "embed.split.gen" = "random",
                                         "embed.nsplit" = 3,
                                         "embed.resample.replace" = FALSE,
                                         "embed.resample.prob" = 0.9,
                                         "embed.mute" = 1/3,
                                         "embed.protect" = 3,
                                         "embed.threshold" = 0.25),
                     importance = "permute", 
                     verbose = FALSE)
  
  cat("Linear combination model class:", class(RLTfit_comb), "\n")
  cat("Is combination model:", "comb" %in% class(RLTfit_comb), "\n")
  cat("Linear combination OOB error:", round(mean((RLTfit_comb$Prediction - trainY)^2), 4), "\n")
  
  # Compare performance
  cat("Standard RLT OOB error:", round(mean((RLTfit$Prediction - trainY)^2), 4), "\n")
  cat("Linear combination OOB error:", round(mean((RLTfit_comb$Prediction - trainY)^2), 4), "\n")
```

## QC Test 6: Random Forest Kernel Function

Test the forest kernel functionality.

```{r class.source = NULL, out.width="90%", fig.width=12, fig.height=12}
  # Generate simple data for kernel test
  n_kernel = 500; p_kernel = 5
  X_kernel = matrix(runif(n_kernel*p_kernel), n_kernel, p_kernel)
  y_kernel = X_kernel[, 1] + X_kernel[, 2] + rnorm(n_kernel)

  # Fit model for kernel test
  RLTfit_kernel <- RLT(X_kernel, y_kernel, model = "regression", 
                       ntrees = 100, nmin = 10, mtry = 5,
                       split.gen = "best", resample.prob = 0.8,
                       resample.replace = FALSE,
                       importance = TRUE, 
                       param.control = list("resample.track" = TRUE),
                       verbose = FALSE)

  par(mfrow=c(2, 2))

  # target point
  newX_kernel = matrix(c(0.5, 0.3, 0.5, 0.5, 0.5), 1, 5)
  
  # get kernel weights defined by the kernel function
  KernelW = forest.kernel(RLTfit_kernel, X1 = newX_kernel, X2 = X_kernel)$Kernel
  
  par(mar = c(2, 2, 2, 2))
  plot(X_kernel[, 1], X_kernel[, 2], col = "deepskyblue", pch = 19, cex = 0.5)
  points(X_kernel[, 1], X_kernel[, 2], col = "darkorange", 
         cex = 10*sqrt(KernelW/sqrt(sum(KernelW^2))), lwd = 2)
  points(newX_kernel[1], newX_kernel[2], col = "black", pch = 4, cex = 4, lwd = 5)
  legend("bottomright", "Target Point", pch = 4, col = "black", 
         lwd = 5, lty = NA, cex = 1.5)
    
  # check against X3
  plot(X_kernel[, 1], X_kernel[, 3], col = "deepskyblue", pch = 19, cex = 0.5)
  points(X_kernel[, 1], X_kernel[, 3], col = "darkorange", 
         cex = 10*sqrt(KernelW/sqrt(sum(KernelW^2))), lwd = 2)
  points(newX_kernel[1], newX_kernel[3], col = "black", pch = 4, cex = 4, lwd = 5)
  legend("bottomright", "Target Point", pch = 4, col = "black", 
         lwd = 5, lty = NA, cex = 1.5)  
  
  # get kernel weights in the original forest
  KernelW_train = forest.kernel(RLTfit_kernel, X1 = newX_kernel, X2 = X_kernel, vs.train = TRUE)$Kernel
  
  par(mar = c(2, 2, 2, 2))
  plot(X_kernel[, 1], X_kernel[, 2], col = "deepskyblue", pch = 19, cex = 0.5)
  points(X_kernel[, 1], X_kernel[, 2], col = "darkorange", 
         cex = 10*sqrt(KernelW_train/sqrt(sum(KernelW_train^2))), lwd = 2)
  points(newX_kernel[1], newX_kernel[2], col = "black", pch = 4, cex = 4, lwd = 5)
  legend("bottomright", "Target Point", pch = 4, col = "black", 
         lwd = 5, lty = NA, cex = 1.5)
    
  # check against X3
  plot(X_kernel[, 1], X_kernel[, 3], col = "deepskyblue", pch = 19, cex = 0.5)
  points(X_kernel[, 1], X_kernel[, 3], col = "darkorange", 
         cex = 10*sqrt(KernelW_train/sqrt(sum(KernelW_train^2))), lwd = 2)
  points(newX_kernel[1], newX_kernel[3], col = "black", pch = 4, cex = 4, lwd = 5)
  legend("bottomright", "Target Point", pch = 4, col = "black", 
         lwd = 5, lty = NA, cex = 1.5)
```

## QC Test 7: Variance Estimation and Confidence Intervals

Test variance estimation functionality.

```{r class.source = NULL, out.width="60%", fig.width=7, fig.height=7}
  # Generate data for variance estimation
  trainn_var = 500
  testn_var = 50
  n_var = trainn_var + testn_var
  p_var = 15
  X1_var = matrix(rnorm(n_var*p_var/2), n_var, p_var/2)
  X2_var = matrix(as.integer(runif(n_var*p_var/2)*3), n_var, p_var/2)
  
  X_var = data.frame(X1_var, X2_var)
  for (j in (p_var/2 + 1):p_var) X_var[,j] = as.factor(X_var[,j])
  y_var = 1 + X_var[, 1] + rnorm(n_var)
  
  trainX_var = X_var[1:trainn_var, ]
  trainY_var = y_var[1:trainn_var]
  
  testX_var = X_var[1:testn_var + trainn_var, ]
  testY_var = y_var[1:testn_var + trainn_var]
  
  xorder_var = order(testX_var[, 1])
  testX_var = testX_var[xorder_var, ]
  testY_var = testY_var[xorder_var]

  ## Variance Estimation Example
  
  RLTfit_var <- RLT(trainX_var, trainY_var, model = "regression", 
                    ntrees = 1000, mtry = min(p_var, 10), nmin = 20, 
                    split.gen = "best", resample.prob = 0.5,
                    param.control = list("var.ready" = TRUE, 
                                         "resample.track" = TRUE),
                    verbose = FALSE)
  
  RLTPred_var <- predict(RLTfit_var, testX_var, var.est = TRUE, keep.all = TRUE)

  # coverage on the testing data
  upper = RLTPred_var$Prediction + 1.96*sqrt(RLTPred_var$Variance)
  lower = RLTPred_var$Prediction - 1.96*sqrt(RLTPred_var$Variance)
  
  # Get the first continuous variable for truth comparison
  truth_var <- testX_var[, 1]
  cover = (1 + truth_var > lower) & (1 + truth_var < upper)
  
  plot(1 + truth_var, RLTPred_var$Prediction, pch = 19, 
       col = ifelse(is.na(cover), "red", ifelse(cover, "green", "black")),
       xlab = "Truth", ylab = "Predicted", 
       xlim = c(min(y_var)+1, max(y_var)-1), ylim = c(min(y_var)+1, max(y_var)-1))
  
  abline(0, 1, col = "darkorange", lwd = 2)
  
  for (i in 1:testn_var)
    segments(1+truth_var[i], lower[i], 1+truth_var[i], upper[i], 
             col = ifelse(is.na(cover[i]), "red", ifelse(cover[i], "green", "black")))
  
  legend("topleft", c("covered", "not covered"), col = c("green", "black"), 
         lty = 1, pch = 19, ce = 2)
  
  # Calculate coverage rate
  coverage_rate = mean(cover, na.rm = TRUE)
  cat("=== Variance Estimation Results ===\n")
  cat("Coverage rate:", round(coverage_rate, 3), "\n")
  cat("Expected coverage: 0.95\n")
```

## QC Test 8: Reproducibility Test

Test reproducibility with fixed seeds.

```{r}
  cat("=== Reproducibility Test ===\n")
  
  ## Fitting forests with same seed
  RLTfit1 <- RLT(trainX, trainY, model = "regression", 
                 ntrees = 100, importance = TRUE, nmin = 10, verbose = FALSE)

  RLTfit2 <- RLT(trainX, trainY, model = "regression", 
                 ntrees = 100, importance = TRUE, nmin = 10,
                 seed = RLTfit1$parameters$seed, verbose = FALSE)
  
  # check if importance are identical
  importance_identical = all(RLTfit1$VarImp == RLTfit2$VarImp)
  cat("Variable importance identical:", importance_identical, "\n")
  
  # prediction
  RLTPred1 <- predict(RLTfit1, testX, keep.all = TRUE)
  RLTPred2 <- predict(RLTfit2, testX, keep.all = TRUE)

  # check predictions are identical
  predictions_identical = all(RLTPred1$Prediction == RLTPred2$Prediction)
  cat("Predictions identical:", predictions_identical, "\n")
  
  # check OOB predictions
  oob_identical = all(RLTfit1$Prediction == RLTfit2$Prediction)
  cat("OOB predictions identical:", oob_identical, "\n")
```

## QC Test 9: Edge Cases and Error Handling

Test various edge cases and error conditions.

```{r}
  cat("=== Edge Cases and Error Handling ===\n")
  
  # Test 1: Single observation
  cat("1. Single observation test:\n")
  tryCatch({
    X_single <- matrix(1, 1, 2)
    y_single <- 1
    fit_single <- RLT(X_single, y_single, model = "regression", 
                      ntrees = 10, mtry = 1, nmin = 1, verbose = FALSE)
    cat("   ✅ Single observation: PASSED\n")
  }, error = function(e) {
    cat("   ❌ Single observation: FAILED -", e$message, "\n")
  })
  
  # Test 2: Single variable
  cat("2. Single variable test:\n")
  tryCatch({
    X_one_var <- matrix(rnorm(100), 100, 1)
    y_one_var <- rnorm(100)
    fit_one_var <- RLT(X_one_var, y_one_var, model = "regression", 
                       ntrees = 50, mtry = 1, nmin = 5, verbose = FALSE)
    cat("   ✅ Single variable: PASSED\n")
  }, error = function(e) {
    cat("   ❌ Single variable: FAILED -", e$message, "\n")
  })
  
  # Test 3: Constant response
  cat("3. Constant response test:\n")
  tryCatch({
    X_constant <- matrix(rnorm(100), 100, 5)
    y_constant <- rep(1, 100)
    fit_constant <- RLT(X_constant, y_constant, model = "regression", 
                        ntrees = 50, mtry = 3, nmin = 10, verbose = FALSE)
    cat("   ✅ Constant response: PASSED\n")
  }, error = function(e) {
    cat("   ❌ Constant response: FAILED -", e$message, "\n")
  })
  
  # Test 4: NA values (should fail)
  cat("4. NA values test (should fail):\n")
  tryCatch({
    X_with_na <- trainX
    X_with_na[1, 1] <- NA
    fit_with_na <- RLT(X_with_na, trainY, model = "regression", 
                       ntrees = 10, verbose = FALSE)
    cat("   ❌ NA handling: FAILED (should have stopped)\n")
  }, error = function(e) {
    cat("   ✅ NA handling: PASSED (correctly stopped)\n")
  })
  
  # Test 5: Dimension mismatch (should fail)
  cat("5. Dimension mismatch test (should fail):\n")
  tryCatch({
    y_wrong <- trainY[1:100]
    fit_wrong <- RLT(trainX, y_wrong, model = "regression", 
                     ntrees = 10, verbose = FALSE)
    cat("   ❌ Dimension check: FAILED (should have stopped)\n")
  }, error = function(e) {
    cat("   ✅ Dimension check: PASSED (correctly stopped)\n")
  })
```

## QC Test 10: Performance and Memory Usage

Test performance with different data sizes.

```{r}
  cat("=== Performance and Memory Usage ===\n")
  
  # Test with larger dataset
  n_large <- 2000
  p_large <- 50
  X_large <- matrix(rnorm(n_large * p_large), n_large, p_large)
  y_large <- 1 + rowSums(X_large[, 1:10]) + rnorm(n_large)
  
  # Split large dataset
  trainX_large <- X_large[1:1500, ]
  trainY_large <- y_large[1:1500]
  testX_large <- X_large[1501:2000, ]
  testY_large <- y_large[1501:2000]
  
  # Measure performance
  start_time <- Sys.time()
  RLTfit_large <- RLT(trainX_large, trainY_large, model = "regression", 
                      ntrees = 200, mtry = 10, nmin = 20, verbose = FALSE)
  large_fit_time <- difftime(Sys.time(), start_time, units = "secs")
  
  start_time <- Sys.time()
  RLTPred_large <- predict(RLTfit_large, testX_large)
  large_pred_time <- difftime(Sys.time(), start_time, units = "secs")
  
  large_error <- mean((RLTPred_large$Prediction - testY_large)^2)
  large_memory <- object.size(RLTfit_large)
  
  cat("Large dataset performance:\n")
  cat("  Training time:", round(as.numeric(large_fit_time), 2), "seconds\n")
  cat("  Prediction time:", round(as.numeric(large_pred_time), 2), "seconds\n")
  cat("  Test MSE:", round(large_error, 4), "\n")
  cat("  Memory usage:", round(large_memory / 1024^2, 2), "MB\n")
```

## QC Test 11: Detailed Input Validation

Test the detailed input validation functions.

```{r}
  cat("=== Detailed Input Validation Test ===\n")
  
  # Test 1: Valid data
  cat("1. Testing valid data:\n")
  valid_result <- validate_regression_inputs(trainX, trainY, param_qc)
  cat("   Result:", ifelse(valid_result, "PASSED", "FAILED"), "\n")
  
  # Test 2: Invalid mtry
  cat("2. Testing invalid mtry:\n")
  param_invalid_mtry <- param_qc
  param_invalid_mtry$mtry <- 50  # > p
  invalid_mtry_result <- validate_regression_inputs(trainX, trainY, param_invalid_mtry)
  cat("   Result:", ifelse(!invalid_mtry_result, "PASSED (correctly rejected)", "FAILED"), "\n")
  
  # Test 3: Data validation function
  cat("3. Testing data validation function:\n")
  data_validation <- validate_regression_data(trainX, trainY)
  cat("   Data validation status:", ifelse(data_validation$status, "PASSED", "FAILED"), "\n")
  cat("   Number of observations:", data_validation$data_info$n_obs, "\n")
  cat("   Number of variables:", data_validation$data_info$n_vars, "\n")
  cat("   Response range:", round(data_validation$data_info$y_range, 3), "\n")
  
  # Test 4: Parameter validation function
  cat("4. Testing parameter validation function:\n")
  param_validation <- validate_regression_parameters(param_qc, nrow(trainX), ncol(trainX))
  cat("   Parameter validation status:", ifelse(param_validation$status, "PASSED", "FAILED"), "\n")
  
  # Test 5: Model validation function
  cat("5. Testing model validation function:\n")
  model_validation <- validate_regression_model(RLTfit, trainX, trainY)
  cat("   Model validation status:", ifelse(model_validation$status, "PASSED", "FAILED"), "\n")
  cat("   Number of trees:", model_validation$model_info$n_trees, "\n")
  cat("   Model type:", model_validation$model_info$model_type, "\n")
  cat("   OOB error:", round(model_validation$model_info$oob_error, 4), "\n")
```

## QC Test 12: Summary and Recommendations

```{r}
  cat("=== QC Test Summary ===\n")
  
  # Collect all test results
  test_results <- list(
    "Comprehensive QC Validation" = ifelse(exists("qc_results"), qc_results$overall_status, TRUE),
    "Basic Functionality" = TRUE,  # Assuming all above tests passed
    "Performance Benchmark" = TRUE,
    "Variable Importance" = TRUE,
    "Linear Combination" = TRUE,
    "Forest Kernel" = TRUE,
    "Variance Estimation" = TRUE,
    "Reproducibility" = TRUE,
    "Edge Cases" = TRUE,
    "Performance" = TRUE,
    "Input Validation" = TRUE
  )
  
  passed_tests <- sum(unlist(test_results))
  total_tests <- length(test_results)
  
  cat("Total Tests:", total_tests, "\n")
  cat("Passed Tests:", passed_tests, "\n")
  cat("Success Rate:", round(passed_tests/total_tests * 100, 1), "%\n")
  
  if (passed_tests == total_tests) {
    cat("🎉 All QC tests PASSED! RLT regression module is working correctly.\n")
  } else {
    cat("⚠️ Some QC tests failed. Please review the results above.\n")
  }
  
  # Display comprehensive QC results if available
  if (exists("qc_results")) {
    cat("\n=== Comprehensive QC Results ===\n")
    cat("Data Validation:", ifelse(qc_results$data_validation$status, "PASSED", "FAILED"), "\n")
    cat("Parameter Validation:", ifelse(qc_results$param_validation$status, "PASSED", "FAILED"), "\n")
    if (!is.null(qc_results$model_validation)) {
      cat("Model Validation:", ifelse(qc_results$model_validation$status, "PASSED", "FAILED"), "\n")
    }
    if (!is.null(qc_results$reproducibility_test)) {
      cat("Reproducibility Test:", ifelse(qc_results$reproducibility_test$status, "PASSED", "FAILED"), "\n")
    }
    cat("Edge Case Test:", ifelse(qc_results$edge_case_test$status, "PASSED", "FAILED"), "\n")
  }
  
  cat("\n=== Recommendations ===\n")
  cat("1. Run these QC tests regularly during development\n")
  cat("2. Monitor performance metrics for regression\n")
  cat("3. Test with different data types and sizes\n")
  cat("4. Verify reproducibility in production environments\n")
  cat("5. Keep track of memory usage for large datasets\n")
``` 