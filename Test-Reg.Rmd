---
title: "RLT Package Regression Test"
author: "Ruoqing Zhu"
date: "Last Updated: `r format(Sys.time(), '%B %d, %Y')`"
output:
  html_document:
    code_folding: hide
    df_print: paged
    toc: yes
    toc_float:
      collapsed: true
      smooth_scroll: true
    toc_depth: '2'
  pdf_document:
    toc: yes
    toc_depth: 2
---

```{r set-options, echo=FALSE, cache=FALSE}
  options(width = 1000)
  knitr::opts_chunk$set(fig.width=9, fig.height=7, out.width = "75%", fig.align = 'center')
  knitr::opts_chunk$set(class.source = "fold-show")
  knitr::opts_chunk$set(collapse=TRUE)
```

## Install and Load Package

Install and load the GitHub version of the RLT package. Do not use the CRAN version. 

```{r}
  # install.packages("devtools")
  # devtools::install_github("teazrq/RLT")
  library(RLT)
```

Load other packages used in this guide.

```{r message=FALSE}
  library(randomForest)
  library(randomForestSRC)
  library(ranger)
  library(parallel)
```

## Benchmark Against Existing Packages

We generate a dataset with 1000 observations and 400 variables, with 200 continuous variables and 200 categorical ones with three categories.

```{r}
  set.seed(1)
  
  trainn = 1000; testn = 1000; n = trainn + testn; p = 400
  X1 = matrix(rnorm(n*p/2), n, p/2) # continuous variables
  X2 = matrix(as.integer(runif(n*p/2)*3), n, p/2) # factors
  
  # generate outcomes
  X = data.frame(X1, X2)
  for (j in (p/2 + 1):p) X[,j] = as.factor(X[,j])
  y = 1 + rowSums(X[, 1:5]) + 2 * (X[, p/2+1] %in% c(1, 3)) + rnorm(n)
  
  # tuning parameters
  ntrees = 100
  ncores = detectCores() - 1
  nmin = 60
  mtry = p/2
  sampleprob = 0.85
  rule = "best"
  nsplit = ifelse(rule == "best", 0, 3)
  importance = TRUE 
  
  # split into training and testing data
  trainX = X[1:trainn, ]
  trainY = y[1:trainn]
  testX = X[1:testn + trainn, ]
  testY = y[1:testn + trainn]
```


```{r class.source = NULL}
  # recording results
  metric = data.frame(matrix(NA, 4, 5))
  rownames(metric) = c("RLT", "randomForestSRC", "randomForest", "ranger")
  colnames(metric) = c("fit.time", "pred.time", "pred.error", 
                       "obj.size", "tree.size")
  
  # using RLT package 
  start_time <- Sys.time()
  RLTfit <- RLT(trainX, trainY, ntrees = ntrees, ncores = ncores, 
                nmin = nmin, mtry = mtry, nsplit = nsplit,
                split.gen = rule, resample.prob = sampleprob,
                importance = importance, param.control = list("alpha" = 0),
                verbose = TRUE)
  metric[1, 1] = difftime(Sys.time(), start_time, units = "secs")
  start_time <- Sys.time()
  RLTPred <- predict(RLTfit, testX, ncores = ncores)
  metric[1, 2] = difftime(Sys.time(), start_time, units = "secs")
  metric[1, 3] = mean((RLTPred$Prediction - testY)^2)
  metric[1, 4] = object.size(RLTfit)
  metric[1, 5] = mean(unlist(lapply(RLTfit$FittedForest$SplitVar, length)))
  
  
  # use randomForestSRC
  options(rf.cores = ncores)
  start_time <- Sys.time()
  rsffit <- rfsrc(y ~ ., data = data.frame(trainX, "y"= trainY), 
                  ntree = ntrees, nodesize = nmin/2, mtry = mtry, 
                  nsplit = nsplit, sampsize = trainn*sampleprob, 
                  importance = ifelse(importance, "permute", "none"))
  metric[2, 1] = difftime(Sys.time(), start_time, units = "secs")
  start_time <- Sys.time()
  rsfpred = predict(rsffit, data.frame(testX))
  metric[2, 2] = difftime(Sys.time(), start_time, units = "secs")
  metric[2, 3] = mean((rsfpred$predicted - testY)^2)
  metric[2, 4] = object.size(rsffit)
  metric[2, 5] = rsffit$forest$totalNodeCount / rsffit$ntree
  
  
  # use randomForest
  start_time <- Sys.time()
  rf.fit <- randomForest(trainX, trainY, ntree = ntrees, 
                         mtry = mtry, nodesize = nmin, 
                         sampsize = trainn*sampleprob, 
                         importance = importance)
  metric[3, 1] = difftime(Sys.time(), start_time, units = "secs")
  start_time <- Sys.time()
  rf.pred <- predict(rf.fit, testX)
  metric[3, 2] = difftime(Sys.time(), start_time, units = "secs")
  metric[3, 3] = mean((rf.pred - testY)^2)
  metric[3, 4] = object.size(rf.fit)
  metric[3, 5] = mean(colSums(rf.fit$forest$nodestatus != 0))
  
  
  # use ranger  
  start_time <- Sys.time()
  rangerfit <- ranger(trainY ~ ., data = data.frame(trainX), 
                      num.trees = ntrees, min.node.size = nmin, 
                      mtry = mtry, num.threads = ncores, 
                      sample.fraction = sampleprob, 
                      importance = "permutation",
                      respect.unordered.factors = "partition")
  metric[4, 1] = difftime(Sys.time(), start_time, units = "secs")
  start_time <- Sys.time()
  rangerpred = predict(rangerfit, data.frame(testX))
  metric[4, 2] = difftime(Sys.time(), start_time, units = "secs")
  metric[4, 3] = mean((rangerpred$predictions - testY)^2)
  metric[4, 4] = object.size(rangerfit)
  metric[4, 5] = mean(unlist(lapply(rangerfit$forest$split.varIDs, length)))
  
  # performance summary
  metric
```

## Variable Importance Check

```{r class.source = NULL, out.width="90%", fig.width=15, fig.height=7}
  par(mfrow=c(2,2))
  par(mar = c(1, 2, 2, 2))
  
  barplot(as.vector(RLTfit$VarImp), main = "RLT")
  barplot(as.vector(rsffit$importance), main = "rsf")
  barplot(rf.fit$importance[, 1], main = "rf")
  barplot(as.vector(rangerfit$variable.importance), main = "ranger")
```

## Print a Single Tree

You can use the `get.one.tree()` function to peek into a single tree.  

```{r}
  get.one.tree(RLTfit, 1)
```

## Random Forest Kernel

Let's generate a new dataset with 5 continuous variables. The true model depends on just the first two variables. 

```{r class.source = NULL, out.width="90%", fig.width=16, fig.height=8}
  # generate data
  n = 1000; p = 5
  X = matrix(runif(n*p), n, p)
  y = X[, 1] + X[, 2] + rnorm(n)

  # fit model
  RLTfit <- RLT(X, y, ntrees = 500, nmin = 4, mtry = 5,
                split.gen = "best", resample.prob = 0.8,
                resample.replace = FALSE,
                resample.track = TRUE, importance = TRUE)

  par(mfrow=c(1, 2))

  # target point
  newX = matrix(c(0.5, 0.3, 0.5, 0.5, 0.5), 
                1, 5)
  
  KernelW = forest.kernel(RLTfit, X1 = newX, X2 = X, vs.train = TRUE)$Kernel
  
  par(mar = c(2, 2, 2, 2))
  plot(X[, 1], X[, 2], col = "deepskyblue", pch = 19, cex = 0.5)
  points(X[, 1], X[, 2], col = "darkorange", cex = 10*sqrt(KernelW/sqrt(sum(KernelW^2))), lwd = 2)
  points(newX[1], newX[2], col = "black", pch = 4, cex = 4, lwd = 5)
  legend("bottomright", "Target Point", pch = 4, col = "black", 
         lwd = 5, lty = NA, cex = 1.5)
    
  # check against X3
  plot(X[, 1], X[, 3], col = "deepskyblue", pch = 19, cex = 0.5)
  points(X[, 1], X[, 3], col = "darkorange", cex = 10*sqrt(KernelW/sqrt(sum(KernelW^2))), lwd = 2)
  points(newX[1], newX[3], col = "black", pch = 4, cex = 4, lwd = 5)
  legend("bottomright", "Target Point", pch = 4, col = "black", 
         lwd = 5, lty = NA, cex = 1.5)
```

## Variance Estimation for Regression

We generate a set of data with 20 variables, and the true model depends only on the first one. We further estimate the variance of the estimator and check if the confidence interval covers the truth. Note that since this model is very simple, the truth is almost the same as the expected value of a random forest prediction. However, in general case, a random forest could be biased and the coverage rate of truth may suffer. 

```{r class.source = NULL, out.width="60%", fig.width=7, fig.height=7}
  trainn = 1000
  testn = 100
  n = trainn + testn
  p = 20
  X1 = matrix(rnorm(n*p/2), n, p/2)
  X2 = matrix(as.integer(runif(n*p/2)*3), n, p/2)
  
  X = data.frame(X1, X2)
  for (j in (p/2 + 1):p) X[,j] = as.factor(X[,j])
  y = 1 + X[, 1] + rnorm(n)
  
  trainX = X[1:trainn, ]
  trainY = y[1:trainn]
  
  testX = X[1:testn + trainn, ]
  testY = y[1:testn + trainn]
  
  xorder = order(testX[, 1])
  testX = testX[xorder, ]
  testY = testY[xorder]
  
  ## Variance Estimation Example
  
  RLTfit <- RLT(trainX, trainY, ntrees = 20000, nmin = 8, 
                mtry = p, split.gen = "random", nsplit = 3, resample.prob = 0.5, 
                resample.replace = FALSE, var.ready = TRUE, resample.track = TRUE)
  
  RLTPred <- predict(RLTfit, testX, var.est = TRUE, keep.all = TRUE)

  # coverage on the testing data
  upper = RLTPred$Prediction + 1.96*sqrt(RLTPred$Variance)
  lower = RLTPred$Prediction - 1.96*sqrt(RLTPred$Variance)
  cover = (1 + testX$X1 > lower) & (1 + testX$X1 < upper)
  
  plot(1 + testX$X1, RLTPred$Prediction, pch = 19, 
       col = ifelse(is.na(cover), "red", ifelse(cover, "green", "black")),
       xlab = "Truth", ylab = "Predicted")
  
  abline(0, 1, col = "darkorange", lwd = 2)
  
  for (i in 1:testn)
    segments(1+testX$X1[i], lower[i], 1+testX$X1[i], upper[i], 
             col = ifelse(is.na(cover[i]), "red", ifelse(cover[i], "green", "black")))
```

