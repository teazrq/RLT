% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/RLT.r
\name{RLT}
\alias{RLT}
\title{\if{html}{\out{<div class="sourceCode">}}\preformatted{            Reinforcement Learning Trees
}\if{html}{\out{</div>}}}
\usage{
RLT(
  x,
  y,
  censor = NULL,
  model = NULL,
  ntrees = if (reinforcement) 100 else 500,
  mtry = max(1, as.integer(ncol(x)/3)),
  nmin = max(1, as.integer(log(nrow(x)))),
  split.gen = "random",
  nsplit = 1,
  resample.replace = TRUE,
  resample.prob = if (resample.replace) 1 else 0.8,
  resample.preset = NULL,
  obs.w = NULL,
  var.w = NULL,
  importance = FALSE,
  reinforcement = FALSE,
  param.control = list(),
  ncores = 0,
  verbose = 0,
  seed = NULL,
  ...
)
}
\arguments{
\item{x}{A \code{matrix} or \code{data.frame} of features. If \code{x} is a data.frame,
then all factors are treated as categorical variables.}

\item{y}{Response variable. a \code{numeric}/\code{factor} vector.}

\item{censor}{Censoring indicator if survival model is used.}

\item{model}{The model type: \code{"regression"}, \code{"classification"},
\code{"quantile"}, \code{"survival"} or \code{"graph"}.}

\item{ntrees}{Number of trees, \code{ntrees = 100} if reinforcement is
used and \code{ntrees = 1000} otherwise.}

\item{mtry}{Number of randomly selected variables used at each
internal node.}

\item{nmin}{Terminal node size. Splitting will stop when the internal
node size is less equal to \code{nmin}.}

\item{split.gen}{How the cutting points are generated: \code{"random"},
\code{"rank"} or \code{"best"}. If minimum child node size is
enforced (\code{alpha} $> 0$), then \code{"rank"} and \code{"best"}
should be used.}

\item{nsplit}{Number of random cutting points to compare for each
variable at an internal node.}

\item{resample.replace}{Whether the in-bag samples are obtained with
replacement.}

\item{resample.prob}{Proportion of in-bag samples.}

\item{resample.preset}{A pre-specified matrix for in-bag data indicator/count
matrix. It must be an \eqn{n \times} \code{ntrees}
matrix with integer entries. Positive number indicates
the number of copies of that observation (row) in the corresponding
tree (column); zero indicates out-of-bag; negative values
indicates not being used in either. Extremely large counts are not
recommended. The sum of each column should not exceed \eqn{n}.}

\item{obs.w}{Observation weights. The weights will be used for calculating
the splitting scores, but not for sampling observations. This is
An experimental feature currently only implemented for regression.}

\item{var.w}{Variable weights. If this is supplied, the default is to
perform weighted sampling of \code{mtry} variables. For
other usage, see the details of \code{split.rule} in
\code{param.control}.}

\item{importance}{Whether to calculate variable importance measures. The calculation
follows Breiman's original permutation strategy.}

\item{reinforcement}{Should reinforcement splitting rule be used. Default
is \code{"FALSE"}, i.e., regular random forests with marginal
search of splitting variable. When it is activated, an
embedded model is fitted to find the best splitting variable
or a linear combination of them, if \code{linear.comb} $> 1$.
They can also be specified in \code{param.control}.}

\item{param.control}{A list of additional parameters. This can be used to
specify other features in a random forest or set embedded
model parameters for reinforcement splitting rules.
Using \code{reinforcement = TRUE} will automatically
generate some default tuning. They are not necessarily optimized.
\itemize{
\item \code{embed.ntrees}: number of trees in the embedded model
\item \code{embed.resample.prob}: proportion of samples (of the internal node) in the embedded model
\item \code{embed.mtry}: number or proportion of variables
\item \code{embed.split.gen} random cutting point search method (\code{"random"}, \code{"rank"} or \code{"best"})
\item \code{embed.nsplit} number of random cutting points.
}

\if{html}{\out{<div class="sourceCode">}}\preformatted{                   \code{linear.comb} is a separate feature that can be activated 
                   with or without using reinforcement. It creates linear combination of 
                   features as the splitting rule. Currently only available for regression and classification. 
                   \\itemize\{
                   \\item In reinforcement mode, a linear combination is created using the top continuous 
                   variables from the embedded model. If a categorical variable is the best, then a regular 
                   split will be used. The splitting point will be searched based on \code{split.rule} of the
                   model. 
                   \\item In non-reinforcement mode, a marginal screening is performed and the top features 
                   are used to construct the linear combination. This is an experimental feature. 
                   \}
                   
                   \code{split.rule} is used to specify the criteria used to compare different splittings.
                   Here are the available choices. The first one is the default:
                   \\itemize\{
                   \\item Regression: `"var"` (variance reduction); `"pca"` and `"sir"` can be used for linear combination splits
                   \\item Classification: `"gini"`
                   \\item Survival: `"logrank"`, `"suplogrank"`, `"coxgrad"`.
                   \\item Quantile: `"ks"` (Kolmogorov-Smirnov test)
                   \\item Graph: `"spectral"` (spectral embedding)
                   \}
                   
                   \code{resample.track} indicates whether to keep track of the observations used in each tree.
                   
                   \code{var.ready} this is a feature to calculate variance of the random forest prediction
                   Currently only available for regression (Xu, Zhu & Shao, 2023) and survival models (Formentini, Liang & Zhu, 2023). 
                   Specifying \code{var.ready = TRUE} has the following effect:
                   \\itemize\{
                   \\item \code{resample.preset} is constructed automatically
                   \\item \code{resample.replace} is set to `FALSE`
                   \\item \code{resample.prob} is set to \eqn{n / 2}
                   \\item \code{resample.track} is set to `TRUE`
                   \}
                   
                   It is recommended to use a very large \code{ntrees}, e.g, 10000 or larger. 
                   For \code{resample.prob} greater than \eqn{n / 2}, one should consider the approach in Xu, Zhu & Shao (2023).
                   
                   \code{alpha} force a minimum proportion of samples in each child node.
                   
                   \code{failcount} specifies the unique number of failure time points used in survival model. 
                   By default, all failure time points will be used. A smaller number may speed up the computation. 
                   The time points will be chosen uniformly on the quantiles of failure times.
}\if{html}{\out{</div>}}}

\item{ncores}{Number of cores. Default is 0 (using all available cores).}

\item{verbose}{Whether info should be printed.}

\item{seed}{Random seed number to replicate a previously fitted forest.
Internally, the \verb{xoshiro256++} generator is used. If not specified,
a seed will be generated automatically.}

\item{...}{Additional arguments.}
}
\value{
A \code{RLT} fitted object, constructed as a list consisting
\itemize{
\item{FittedForest}{Fitted tree structures}
\item{VarImp}{Variable importance measures, if \code{importance = TRUE}}
\item{Prediction}{In-bag prediction values}
\item{OOBPrediction}{Out-of-bag prediction values}
\item{resample.preset}{An \code{n} \eqn{\times} \code{ntrees} matrix that indicates
which observations are used in each tree. Provided if
\code{resample.preset} was supplied, \code{resample.track = TRUE},
or \code{var.ready = TRUE}}
}

For classification forests, these items are further provided
\itemize{
\item{NClass}{The number of classes}
}

For survival forests, these items are further provided
\itemize{
\item{NFail}{The number of observed failure times}
\item{VarImpCov}{if \code{VI.var=TRUE}, estimated covariance matrix for the variable importance}
\item{cindex_tree}{Out-of-bag c-index for each tree}
\item{cindex}{Out-of-bag c-index for the forest}
\item{timepoints}{ordered observed failure times}
}
}
\description{
\if{html}{\out{<div class="sourceCode">}}\preformatted{      Fit models for regression, classification and 
                   survival analysis using reinforced splitting rules.
                   The model reduces to regular random forests if 
                   reinforcement is turned off.
}\if{html}{\out{</div>}}

To activate embedded model for splitting variable selection, use \code{reinforcement = TRUE}.
To specify parameters of embedded models, see definition of \code{param.control}.
}
\references{
\itemize{
\item Zhu, R., Zeng, D., & Kosorok, M. R. (2015) "Reinforcement Learning Trees." Journal of the American Statistical Association. 110(512), 1770-1784.
\item Xu, T., Zhu, R., & Shao, X. (2023) "On Variance Estimation of Random Forests with Infinite-Order U-statistics." arXiv preprint arXiv:2202.09008.
\item Formentini, S. E., Wei L., & Zhu, R. (2022) "Confidence Band Estimation for Survival Random Forests." arXiv preprint arXiv:2204.12038.
}

\donttest{}
}
