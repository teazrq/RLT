---
title: "RLT Package Reinforcement Learning Trees and Linear Combinaation Split Test"
author: "Ruoqing Zhu"
date: "Last Updated: `r format(Sys.time(), '%B %d, %Y')`"
output:
  html_document:
    code_folding: hide
    df_print: paged
    toc: yes
    toc_float:
      collapsed: true
      smooth_scroll: true
    toc_depth: '2'
  pdf_document:
    toc: yes
    toc_depth: 2
---

```{r set-options, echo=FALSE, cache=FALSE}
  options(width = 1000)
  knitr::opts_chunk$set(fig.width=9, fig.height=7, out.width = "75%", fig.align = 'center')
  knitr::opts_chunk$set(class.source = "fold-show")
  knitr::opts_chunk$set(collapse=TRUE)
```

## Install and Load Package

Install and load the GitHub version of the RLT package. Do not use the CRAN version. 

```{r}
  # install.packages("devtools")
  # devtools::install_github("teazrq/RLT")
  library(RLT)
```

## Single Variable Embedded Splitting

When `reinforcement` is enabled, an embedded random forest model and the corresponding variable importance measure will be used to search for the best splitting rule. There will be a default setting of parameters for the embedded model, however you can still tune them individually. 

```{r}
  set.seed(2)
  
  n = 1000
  p = 10
  X = matrix(rnorm(n*p), n, p)
  y = 1 + X[, 1] + X[, 3] + X[, 9] + rnorm(n)
  
  testX = matrix(rnorm(n*p), n, p)
  testy = 1 + testX[, 1] + testX[, 3] + testX[, 9] + rnorm(n)
  
  start_time <- Sys.time()
  
  RLTfit <- RLT(X, y, model = "regression", 
                ntrees = 100, ncores = 1, nmin = 10,
                split.gen = "random", nsplit = 1,
                resample.prob = 0.85, resample.replace = FALSE,
                reinforcement = TRUE, importance = "distribute",
                param.control = list("embed.ntrees" = 50,
                                     "embed.mtry" = 1/2,
                                     "embed.nmin" = 5),
                verbose = TRUE)
  
  difftime(Sys.time(), start_time, units = "secs")
  
    # oob error
  mean((RLTfit$Prediction - y)^2, na.rm = TRUE)
  
  # prediction error
  pred = predict(RLTfit, testX)
  mean((pred$Prediction - testy)^2)
  
```

```{r fig.width= 8, fig.height=5}
  # sparse variable importance
  barplot(as.vector(RLTfit$VarImp), main = "RLT")
  
  # check one tree
  get.one.tree(RLTfit, 1)
```

Check seed match

```{r}
  RLTrep <- RLT(X, y, model = "regression", 
                ntrees = 100, ncores = 1, nmin = 10,
                split.gen = "random", nsplit = 1,
                resample.prob = 0.85, resample.replace = FALSE,
                reinforcement = TRUE, importance = "distribute",
                param.control = list("embed.ntrees" = 50,
                                     "embed.mtry" = 1/2,
                                     "embed.nmin" = 5),
                verbose = TRUE, seed = RLTfit$parameters$seed)

  all(RLTfit$VarImp == RLTrep$VarImp)
```

## Linear Combination Split 

We can also use a linear combination of variables as the splitting rule, i.e., 

$$ \mathbf{1}( \boldsymbol \beta^T \mathbf{x} > c ) $$
The search of top variables is the same embedded random forest, however, $\boldsymbol \beta$ is determined using other criteria such as the sliced inverse regression (`"sir"`) and PCA (`"pca"`). When a categorical variable is encountered (random best at internal node), the algorithm switch to the default single variable split. 

```{r}
  # set.seed(1)
  library(MASS)
  ntrain = 300
  ntest = 500
  n = ntrain + ntest
  p = 10
  S = matrix(0.3, p, p)
  diag(S) = 1

  X1 = mvrnorm(n, mu = rep(0, p), Sigma = S)
  X2 <- matrix(as.integer(runif(n * p) * 5), n, p)

  # Combine continuous and categorical variables into a data frame (X)
  X <- data.frame(X1, X2)

  # Convert the second half of the columns in X to factors
  X[, (p + 1):(2*p)] <- lapply(X[, (p+1):(2*p)], as.factor)
  
  xlink <- function(x) 1 + x[, 1] + x[, 3] + x[, ncol(x)] %in% c(0,2,4)
  
  # outcome
  y = xlink(X) + rnorm(n)
  w = runif(ntrain)
  
  xtrain = X[1:ntrain, ]
  ytrain = y[1:ntrain]
  xtest = X[-(1:ntrain), ]
  ytest = y[-(1:ntrain)]

  start_time <- Sys.time()
  RLTfit <- RLT(xtrain, ytrain, model = "regression", obs.w = w,
                ntrees = 100, ncores = 1, nmin = 10, mtry = 10,
                split.gen = "random", nsplit = 2,
                resample.prob = 0.9, resample.replace = FALSE, 
                param.control = list("linear.comb" = 3,
                                     "split.rule" = "sir",
                                     "embed.ntrees" = 50,
                                     "embed.mtry" = 0.5,
                                     "embed.nmin" = 5,
                                     "embed.split.gen" = "random",
                                     "embed.nsplit" = 3,
                                     "embed.resample.replace" = FALSE,
                                     "embed.resample.prob" = 0.9,
                                     "embed.mute" = 1/3,
                                     "embed.protect" = 3,
                                     "embed.threshold" = 0.25),
                importance = "permute", 
                verbose = TRUE)
  difftime(Sys.time(), start_time, units = "secs")
  
  # oob prediction and error
  plot(RLTfit$Prediction, ytrain)
  mean( (RLTfit$Prediction - ytrain)^2 , na.rm = TRUE)
  
  # testing data error
  mean((predict(RLTfit, xtest)$Prediction - ytest)^2)
```

```{r}
  start_time <- Sys.time()
  RLTvi2 <- RLT(xtrain, ytrain, model = "regression", obs.w = w,
                ntrees = 100, ncores = 1, nmin = 10, mtry = 10,
                split.gen = "random", nsplit = 2,
                resample.prob = 0.9, resample.replace = FALSE, 
                param.control = list("linear.comb" = 3,
                                     "split.rule" = "sir",
                                     "embed.ntrees" = 50,
                                     "embed.mtry" = 0.5,
                                     "embed.nmin" = 5,
                                     "embed.split.gen" = "random",
                                     "embed.nsplit" = 3,
                                     "embed.resample.replace" = FALSE,
                                     "embed.resample.prob" = 0.9,
                                     "embed.mute" = 1/3,
                                     "embed.protect" = 3,
                                     "embed.threshold" = 0.25),
                importance = "distribute", 
                verbose = TRUE)
  difftime(Sys.time(), start_time, units = "secs")
```


```{r class.source = NULL, out.width="90%", fig.width=15, fig.height=7}
  par(mfrow=c(1,2))
  par(mar = c(1, 2, 2, 2))
  
  # sparse variable importance
  barplot(as.vector(RLTfit$VarImp), main = "RLT")
  barplot(as.vector(RLTvi2$VarImp), main = "RLT")
  
  # check one tree
  get.one.tree(RLTfit, 1)
```

## Linear combination kernel

The linear combination split leads to non-rectangular kernels. 

```{r class.source = NULL, out.width="90%", fig.width=12, fig.height=12}
  # generate data
  n = 500; p = 5
  X = matrix(runif(n*p), n, p)
  y = X[, 1] + X[, 3] + rnorm(n)

  # fit model
  RLTfit <- RLT(X, y, model = "regression",
                ntrees = 100, ncores = 1, nmin = 10, mtry = 5,
                split.gen = "random", nsplit = 3,
                resample.prob = 0.9, resample.replace = FALSE,
                param.control = list("embed.ntrees" = 50,
                                     "linear.comb" = 2,
                                     "embed.nmin" = 2,
                                     "split.rule" = "sir"),
                importance = TRUE,
                verbose = TRUE)

  # target point
  newX = matrix(c(0.5, 0.5, 0.5, 0.5, 0.5), 
                1, 5)
  
  # get kernel weights defined by the kernel function
  KernelW = forest.kernel(RLTfit, X1 = newX, X2 = X)$Kernel
  
  par(mar = c(2, 2, 2, 2))
  plot(X[, 1], X[, 3], col = "deepskyblue", pch = 19, cex = 0.5)
  points(X[, 1], X[, 3], col = "darkorange", 
         cex = 10*sqrt(KernelW/sqrt(sum(KernelW^2))), lwd = 2)
  points(newX[1], newX[3], col = "black", pch = 4, cex = 4, lwd = 5)
  legend("topright", "Target Point", pch = 4, col = "black", 
         lwd = 5, lty = NA, cex = 1.5)
```


